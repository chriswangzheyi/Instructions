#02.Spark 操作Hudi

## 整合Hudi的spark包

	cd /home/zheyi/hudi-0.10.0/packaging/hudi-spark-bundle/target	
	scp hudi-spark3-bundle_2.12-0.10.0.jar zheyi@hudi2:/home/zheyi/spark/jars
	scp hudi-spark3-bundle_2.12-0.10.0.jar zheyi@hudi3:/home/zheyi/spark/jars
	scp hudi-spark3-bundle_2.12-0.10.0.jar zheyi@hudi4:/home/zheyi/spark/jars

重启spark集群	

## 添加权限

	sudo groupadd supergroup
	sudo usermod -a -G supergroup zheyi
	hdfs dfsadmin -refreshUserToGroupsMappings

	
## Spark 命令行 Demo

### 准备数据

	hdfs dfs -mkdir /data
	hdfs dfs -mkdir /hudi_dw

测试数据	
	cd /home/zheyi
	vim user.txt

	1,黄渤,29,M,BJ
	2,韩雪,44,M,BJ
	3,杨紫,31,M,SH

上传文件：

	hdfs dfs -put user.txt /data/

### 用Spark读取HDFS数据

进入spark shell

	cd /home/zheyi/spark/bin
	sh spark-shell  \
	 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
    --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'

输入：

	import org.apache.hudi.QuickstartUtils._
	import scala.collection.JavaConversions._
	import org.apache.spark.sql.SaveMode._
	import org.apache.hudi.DataSourceReadOptions._
	import org.apache.hudi.DataSourceWriteOptions._
	import org.apache.hudi.config.HoodieWriteConfig._
	import org.apache.spark.sql.{DataFrame, SparkSession}
	import org.apache.spark.sql.types.{DataTypes, StructType}
	
	val schema = new StructType().add("id", DataTypes.StringType).add("name", DataTypes.StringType).add("age", DataTypes.IntegerType).add("gender", DataTypes.StringType).add("city", DataTypes.StringType)
	
	//创建提交时间
	val commitTime = System.currentTimeMillis().toString 
	//生成提交时间
	val df: DataFrame = spark.read.schema(schema).csv("/data/user.txt").withColumn("ts" , lit(commitTime))


将DF数据写入到hudi:

	df.write.format("hudi").
	  options(getQuickstartWriteConfigs).
	  option(PRECOMBINE_FIELD.key(), "ts").
	  option(RECORDKEY_FIELD.key(), "id").
	  option(PARTITIONPATH_FIELD.key(), "city").
	  option(TBL_NAME.key(), "test01").
	  mode(Overwrite).
	  save("hdfs://hudi1:9000/hudi_dw/hudi_test01")

读取数据展示数据:

	val resDF = spark.read.format("hudi").load("hdfs://hudi1:9000/hudi_dw/hudi_test01")
	//展示数据
	resDF.show
	// 使用sparkSQL的方式查询数据 
	resDF.createOrReplaceTempView("tb_test01")
	spark.sql("select * from tb_test01").show

 
 显示：
 
	 --+----------------------+--------------------+---+----+---+------+-------------+----+
	|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|name|age|gender|           ts|city|
	+-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+------+-------------+----+
	|  20221231111333687|20221231111333687...|                 1|                    BJ|645c6d10-c56e-46a...|  1|黄渤| 29|     M|1672456400628|  BJ|
	|  20221231111333687|20221231111333687...|                 2|                    BJ|645c6d10-c56e-46a...|  2|韩雪| 44|     M|1672456400628|  BJ|
	|  20221231111333687|20221231111333687...|                 3|                    SH|4ee792de-777c-470...|  3|杨紫| 31|     M|1672456400628|  SH|
	+-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+------+-------------+----+
	
hdfs 底层结构

	hdfs dfs -ls /hudi_dw/hudi_test01	
	drwxr-xr-x   - zheyi supergroup          0 2022-12-31 11:13 /hudi_dw/hudi_test01/.hoodie
	drwxr-xr-x   - zheyi supergroup          0 2022-12-31 11:13 /hudi_dw/hudi_test01/BJ
	drwxr-xr-x   - zheyi supergroup          0 2022-12-31 11:13 /hudi_dw/hudi_test01/SH
	

## Spark sql Demo

	sh spark-sql --packages org.apache.hudi:hudi-spark3-bundle_2.12:0.9.0,org.apache.spark:spark-avro_2.12:3.0.1 \
	--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
	--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'

### 查询

	-- 创建表
	drop table if exists tb_test02 ;
	create table if not exists tb_test02 (
	  id int, 
	  name string, 
	  age int ,
	  gender string ,
	  city string
	) using hudi
	location 'hdfs://hudi1:9000/hudi_dw/hudi_test02'
	options (
	  type = 'cow',
	  primaryKey = 'id'
	);
	-- 向表中插入数据
	insert into tb_test02 select 1,'zss',23,'M','SH' ;
	-- 查询数据 
	select * from  tb_test02  ;
	-- 删除数据 
	delete from tb_test02 where id = 1; 
