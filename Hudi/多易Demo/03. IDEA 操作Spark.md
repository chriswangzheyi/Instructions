#03. IDEA 操作Spark


## 准备数据

	vi teacher.csv

插入：

	1,涛哥,40,M
	2,星哥,37,M
	3,老娜,29,F

导入hdfs

	hdfs dfs -put teacher.csv /data



## 代码

### 写hdfs

	package com.wzy
	
	import org.apache.hudi.DataSourceWriteOptions
	import org.apache.hudi.QuickstartUtils.getQuickstartWriteConfigs
	import org.apache.hudi.config.HoodieWriteConfig
	import org.apache.spark.sql.functions.lit
	import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}
	import org.apache.spark.sql.types.{DataTypes, StructType}
	
	object WriteData {
	
	  def main(args: Array[String]): Unit = {
	
	    val spark = SparkSession.builder()
	      .appName("write data to hdfs by hudi")
	      .master("spark://192.168.2.123:7077")
	      .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
	      .getOrCreate()
	    val sc = spark.sparkContext
	    import spark.implicits._
	
	    val schema = new StructType()
	      .add("id", DataTypes.StringType)
	      .add("name", DataTypes.StringType)
	      .add("age", DataTypes.IntegerType)
	      .add("gender", DataTypes.StringType)
	
	    val commitTime = System.currentTimeMillis().toString //生成提交时间
	
	    val df: DataFrame = spark.read.schema(schema).csv("/data/teacher.csv")
	      .withColumn("ts" , lit(commitTime))
	
	    df.write.format("hudi")
	      .options(getQuickstartWriteConfigs)
	      .option(DataSourceWriteOptions.PRECOMBINE_FIELD.key(), "ts") // 提交时间
	      .option(DataSourceWriteOptions.RECORDKEY_FIELD.key(), "id") // 主键
	      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD.key(), "gender")// 分区字段
	      .option(HoodieWriteConfig.TBL_NAME.key, "tb_teacher")// 表名
	      .mode(SaveMode.Append)
	      .save("hdfs://192.168.2.121:9000/hudi_wd/tbls/tb_teacher")
	
	    spark.close()
	  }
	}



### 读hdfs

	package com.wzy
	
	import org.apache.spark.sql.{DataFrame, SparkSession}
	import org.apache.spark.sql.types.{DataTypes, StructType}
	
	object ReadData {
	
	  def main(args: Array[String]): Unit = {
	
	
	    val spark = SparkSession.builder()
	      .appName("read data for hudi")
	      .master("spark://192.168.2.123:7077")
	      .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
	      //本地测试运行需要加这一句话，部署在生产环境则删除
	      //.config("spark.jars", "/Users/zheyiwang/IdeaProjects/hudi_test/target/hudi_test-1.0-SNAPSHOT-jar-with-dependencies.jar")
	      .getOrCreate()
	    val sc = spark.sparkContext
	    import spark.implicits._
	
	    val schema = new StructType()
	      .add("id", DataTypes.StringType)
	      .add("name", DataTypes.StringType)
	      .add("age", DataTypes.IntegerType)
	      .add("gender", DataTypes.StringType)
	
	    val commitTime = System.currentTimeMillis().toString //生成提交时间
	
	    val df: DataFrame = spark.read.format("hudi")
	      .load("hdfs://192.168.2.121:9000/hudi_wd/tbls/tb_teacher")
	    df.show()
	
	  }
	}



### 运行程序

#### 写

	sh spark-submit  --class com.wzy.WriteData --master spark://192.168.2.123:7077 /home/zheyi/hudi_test-1.0-SNAPSHOT-jar-with-dependencies.jar
	
#### 读
	
	sh spark-submit  --class com.wzy.ReadData --master spark://192.168.2.123:7077 /home/zheyi/hudi_test-1.0-SNAPSHOT-jar-with-dependencies.jar
	
打印

	+-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+-------------+------+
	|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|name|age|           ts|gender|
	+-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+-------------+------+
	|  20230108233537463|20230108233537463...|                 1|                     M|f8737921-d900-4f9...|  1|涛哥| 40|1673192129901|     M|
	|  20230108233537463|20230108233537463...|                 2|                     M|f8737921-d900-4f9...|  2|星哥| 37|1673192129901|     M|
	|  20230108233537463|20230108233537463...|                 3|                     F|bc136072-d0c3-44e...|  3|老娜| 29|1673192129901|     F|
	+-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+-------------+------+
	

### pom

	<?xml version="1.0" encoding="UTF-8"?>
	<project xmlns="http://maven.apache.org/POM/4.0.0"
	         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	    <modelVersion>4.0.0</modelVersion>
	
	    <groupId>org.example</groupId>
	    <artifactId>hudi_test</artifactId>
	    <version>1.0-SNAPSHOT</version>
	
	    <properties>
	        <maven.compiler.source>8</maven.compiler.source>
	        <maven.compiler.target>8</maven.compiler.target>
	    </properties>
	    <dependencies>
	        <dependency>
	            <groupId>com.alibaba</groupId>
	            <artifactId>fastjson</artifactId>
	            <version>1.2.75</version>
	        </dependency>
	        <dependency>
	            <groupId>org.scala-lang</groupId>
	            <artifactId>scala-library</artifactId>
	            <version>2.12.10</version>
	        </dependency>
	        <dependency>
	            <groupId>org.apache.hudi</groupId>
	            <artifactId>hudi-spark-bundle_2.12</artifactId>
	            <version>0.9.0</version>
	        </dependency>
	        <!-- https://mvnrepository.com/artifact/joda-time/joda-time -->
	        <dependency>
	            <groupId>joda-time</groupId>
	            <artifactId>joda-time</artifactId>
	            <version>2.9.9</version>
	        </dependency>
	
	        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
	          <dependency>
	              <groupId>org.apache.spark</groupId>
	              <artifactId>spark-core_2.12</artifactId>
	              <version>3.0.0</version>
	          </dependency>
	
	        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming -->
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-streaming_2.12</artifactId>
	            <version>3.0.0</version>
	        </dependency>
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-sql_2.12</artifactId>
	            <version>3.0.0</version>
	        </dependency>
	        <dependency>
	            <groupId>com.wzy</groupId>
	            <artifactId>hudi_test</artifactId>
	            <version>12</version>
	            <scope>system</scope>
	            <systemPath>${project.basedir}/lib/hudi-spark3-bundle_2.12-0.10.0.jar</systemPath>
	        </dependency>
	
	
	    </dependencies>
	    <!-- 依赖下载国内镜像库 -->
	    <repositories>
	        <repository>
	            <id>nexus-aliyun</id>
	            <name>Nexus aliyun</name>
	            <layout>default</layout>
	            <url>http://maven.aliyun.com/nexus/content/groups/public</url>
	            <snapshots>
	                <enabled>false</enabled>
	                <updatePolicy>never</updatePolicy>
	            </snapshots>
	            <releases>
	                <enabled>true</enabled>
	                <updatePolicy>never</updatePolicy>
	            </releases>
	        </repository>
	    </repositories>
	
	    <!-- maven插件下载国内镜像库 -->
	    <pluginRepositories>
	        <pluginRepository>
	            <id>ali-plugin</id>
	            <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
	            <snapshots>
	                <enabled>false</enabled>
	                <updatePolicy>never</updatePolicy>
	            </snapshots>
	            <releases>
	                <enabled>true</enabled>
	                <updatePolicy>never</updatePolicy>
	            </releases>
	        </pluginRepository>
	    </pluginRepositories>
	    <build>
	        <plugins>
	            <!-- 指定编译java的插件 -->
	            <plugin>
	                <groupId>org.apache.maven.plugins</groupId>
	                <artifactId>maven-compiler-plugin</artifactId>
	                <version>3.5.1</version>
	                <configuration>
	                    <source>1.8</source>
	                    <target>1.8</target>
	                </configuration>
	            </plugin>
	
	            <!-- 指定编译scala的插件 -->
	            <plugin>
	                <groupId>net.alchim31.maven</groupId>
	                <artifactId>scala-maven-plugin</artifactId>
	                <version>3.2.2</version>
	                <executions>
	                    <execution>
	                        <goals>
	                            <goal>compile</goal>
	                            <goal>testCompile</goal>
	                        </goals>
	                        <configuration>
	                            <args>
	                                <arg>-dependencyfile</arg>
	                                <arg>${project.build.directory}/.scala_dependencies</arg>
	                            </args>
	                        </configuration>
	                    </execution>
	                </executions>
	            </plugin>
	            <plugin>
	                <groupId>org.apache.maven.plugins</groupId>
	                <artifactId>maven-compiler-plugin</artifactId>
	                <configuration>
	                    <source>1.8</source>
	                    <target>1.8</target>
	                </configuration>
	            </plugin>
	            <plugin>
	                <artifactId>maven-assembly-plugin</artifactId>
	                <configuration>
	                    <descriptorRefs>
	                        <descriptorRef>jar-with-dependencies</descriptorRef>
	                    </descriptorRefs>
	                </configuration>
	                <executions>
	                    <execution>
	                        <id>make-assembly</id>
	                        <phase>package</phase>
	                        <goals>
	                            <goal>single</goal>
	                        </goals>
	                    </execution>
	                </executions>
	            </plugin>
	        </plugins>
	    </build>
	
	</project>


## Update 数据

### 准备数据

	vi teacher2.csv

插入：

	1,大涛哥,41,M
	4,杨幂,33,F
	
导入hdfs

	hdfs dfs -put teacher2.csv /data	
	
	
### 提交代码

	sh spark-submit  --class com.wzy.UpdateData --master spark://192.168.2.123:7077 /home/zheyi/hudi_test-1.0-SNAPSHOT-jar-with-dependencies.jar


### 代码

	package com.wzy
	
	import org.apache.hudi.DataSourceWriteOptions
	import org.apache.hudi.QuickstartUtils.getQuickstartWriteConfigs
	import org.apache.hudi.config.HoodieWriteConfig
	import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}
	import org.apache.spark.sql.functions.lit
	import org.apache.spark.sql.types.{DataTypes, StructType}
	
	object UpdateData {
	
	  def main(args: Array[String]): Unit = {
	
	    val spark = SparkSession.builder()
	      .appName("update data")
	      .master("spark://192.168.2.123:7077")
	      .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
	      .getOrCreate()
	    val sc = spark.sparkContext
	
	    val schema = new StructType().
	      add("id", DataTypes.StringType)
	      .add("name", DataTypes.StringType)
	      .add("age", DataTypes.IntegerType)
	      .add("gender", DataTypes.StringType)
	    val commitTime = System.currentTimeMillis().toString //生成提交时间
	
	    val df: DataFrame = spark.read.schema(schema)
	      .csv("/data/teacher2.csv")
	      .withColumn("ts" , lit(commitTime))
	
	    df.write.format("hudi")
	      .options(getQuickstartWriteConfigs)
	      .option(DataSourceWriteOptions.PRECOMBINE_FIELD.key(), "ts") // 提交时间
	      .option(DataSourceWriteOptions.RECORDKEY_FIELD.key(), "id") // 主键
	      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD.key(), "gender")// 分区字段
	      .option(HoodieWriteConfig.TBL_NAME.key, "tb_teacher")// 表名
	      .mode(SaveMode.Append)
	      .save("/hudi_wd/tbls/tb_teacher")
	
	    spark.close()
	
	  }
	}

### 现在的数据

	+-------------------+--------------------+------------------+----------------------+--------------------+---+------+---+-------------+------+
	|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|  name|age|           ts|gender|
	+-------------------+--------------------+------------------+----------------------+--------------------+---+------+---+-------------+------+
	|  20230108234013473|20230108234013473...|                 1|                     M|f8737921-d900-4f9...|  1|大涛哥| 41|1673192407108|     M|
	|  20230108233537463|20230108233537463...|                 2|                     M|f8737921-d900-4f9...|  2|  星哥| 37|1673192129901|     M|
	|  20230108233537463|20230108233537463...|                 3|                     F|bc136072-d0c3-44e...|  3|  老娜| 29|1673192129901|     F|
	|  20230108234013473|20230108234013473...|                 4|                     F|bc136072-d0c3-44e...|  4|  杨幂| 33|1673192407108|     F|
	+-------------------+--------------------+------------------+----------------------+--------------------+---+------+---+-------------+------+
	
可以看到下面的内容产生了变化 

* _hoodie_commit_time
* hoodie_commit_seqno
* ts


## 1.6.2.	增量查询之时间点查询


### 提交代码

	sh spark-submit  --class com.wzy.QueryPointInTime --master spark://192.168.2.123:7077 /home/zheyi/hudi_test-1.0-SNAPSHOT-jar-with-dependencies.jar
	
#### 最后一批导入的数据 (val beginTime: String = arr(arr.length - 2))

	+-------------------+--------------------+------------------+----------------------+--------------------+---+------+---+------+-------------+
	|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|  name|age|gender|           ts|
	+-------------------+--------------------+------------------+----------------------+--------------------+---+------+---+------+-------------+
	|  20230108234013473|20230108234013473...|                 1|                     M|f8737921-d900-4f9...|  1|大涛哥| 41|     M|1673192407108|
	|  20230108234013473|20230108234013473...|                 4|                     F|bc136072-d0c3-44e...|  4|  杨幂| 33|     F|1673192407108|
	+-------------------+--------------------+------------------+----------------------+--------------------+---+------+---+------+-------------+
	
