# 01.环境搭建


## 准备服务器

* 192.168.2.121 hudi / hadoop name node /zk
* 192.168.2.122 spark master / hadoop data node /zk
* 192.168.2.123 spark worker / hadoop data node /zk
* 192.168.2.124 zk

### 设置别名

	hostnamectl set-hostname hudi1
	hostnamectl set-hostname hudi2
	hostnamectl set-hostname hudi3
	hostnamectl set-hostname hudi4


## 安装步骤


### 安装java (每一台机器上)

	tar -zxvf jdk-8u341-linux-x64.tar.gz
	mv jdk1.8.0_341/ java
	
	
-修改配置文件

	sudo vim /etc/profile

插入：

	export JAVA_HOME=/home/zheyi/java
	export PATH=$JAVA_HOME/bin:$PATH
	export JAVA_HOME PATH
	CLASSPATH=.:$JAVA_HOME/jre/lib/ext:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar	


### 安装Maven（ hudi1上）

	tar -zxvf  apache-maven-3.8.6-bin.tar.gz
	
修改配置文件

	cd /home/zheyi/apache-maven-3.8.6/conf
	
修改setting文件，

	vim settings.xml

在mirror之间插入：

	<mirrors>
	    <mirror>
	      <id>alimaven</id>
	      <name>aliyun maven</name>
	      <url>http://maven.aliyun.com/nexus/content/groups/public</url>
	      <mirrorOf>central</mirrorOf>
	    </mirror>

    <mirror>
      <id>alimaven</id>
      <mirrorOf>*</mirrorOf>
      <name>阿里云spring插件仓库</name>
      <url>http://maven.aliyun.com/repository/spring-plugin</url>
    </mirror>

    <mirror>
      <id>repo2</id>
      <mirrorOf>central</mirrorOf>
      <name>Mirror from Maven Repo2</name>
      <url>https://repo.spring.io/plugins-release</url>
    </mirror>

    <mirror>
      <id>UK</id>
      <mirrorOf>central</mirrorOf>
      <name>UK Central</name>
      <url>https://uk.maven.org/maven2</url>
    </mirror>

    <mirror>
      <id>jboss-public-repository-group</id>
      <mirrorOf>central</mirrorOf>
      <name>JBoss Public Repository Group</name>
      <url>https://repository.jboss.org/nexus/content/groups/public</url>
    </mirror>

    <mirror>
      <id>CN</id>
      <mirrorOf>central</mirrorOf>
      <name>OSChina Central</name>
      <url>https://maven.oschina.net/content/groups/public</url>
    </mirror>

    <mirror>
      <id>google-maven-central</id>
      <mirrorOf>central</mirrorOf>
      <name>GCS Maven Central mirror Asia Pacific</name>
      <url>https://maven-central-asia.storage-download.googleapis.com/maven2</url>
    </mirror>

    <mirror>
      <id>confluent</id>
      <mirrorOf>confluent</mirrorOf>
      <name>confluent maven</name>
      <url>http://packages.confluent.io/maven/</url>
    </mirror>

    <mirror>
      <id>nexus-aliyun</id>
      <mirrorOf>central</mirrorOf>
      <name>Nexus aliyun</name>
      <url>http://maven.aliyun.com/nexus/content/groups/public</url>
    </mirror>
    <mirror>
      <id>maven-default-http-blocker</id>
      <mirrorOf>external:http:*</mirrorOf>
      <name>Pseudo repository to mirror external repositories initially using HTTP.</name>
      <url>http://0.0.0.0/</url>
      <blocked>true</blocked>
    </mirror>
    </mirrors>


####增加repo

	<localRepository>/home/zheyi/repo</localRepository>

准备好repo存储路径

	mkdir /home/zheyi/repo

	
#### 修改配置文件

	sudo vim /etc/profile
	
插入

	export  MAVEN_HOME=/home/zheyi/apache-maven-3.8.6
	export PATH=$MAVEN_HOME/bin:$PATH

测试 

	mvn  -version
	


## 构建Hudi （ hudi1上）
	
### 解压

	tar -zxvf hudi-0.10.0.src.tgz

### Hudi项目构建

	cd /home/zheyi/hudi-0.10.0
	mvn clean install -DskipTests -DskipITs -Dcheckstyle.skip=true -Drat.skip=true -Dhadoop.version=3.1.3 -Dscala-2.12 -Dspark3 -Pflink-bundle-shade-hive33


	[INFO] Reactor Summary for Hudi 0.10.0:
	[INFO] 
	[INFO] Hudi ............................................... SUCCESS [  2.553 s]
	[INFO] hudi-common ........................................ SUCCESS [ 13.145 s]
	[INFO] hudi-aws ........................................... SUCCESS [  2.458 s]
	[INFO] hudi-timeline-service .............................. SUCCESS [  1.850 s]
	[INFO] hudi-client ........................................ SUCCESS [  0.024 s]
	[INFO] hudi-client-common ................................. SUCCESS [  9.587 s]
	[INFO] hudi-hadoop-mr ..................................... SUCCESS [  4.467 s]
	[INFO] hudi-spark-client .................................. SUCCESS [ 20.533 s]
	[INFO] hudi-sync-common ................................... SUCCESS [  1.184 s]
	[INFO] hudi-hive-sync ..................................... SUCCESS [  4.261 s]
	[INFO] hudi-spark-datasource .............................. SUCCESS [  0.014 s]
	[INFO] hudi-spark-common_2.12 ............................. SUCCESS [ 12.035 s]
	[INFO] hudi-spark3_2.12 ................................... SUCCESS [  8.549 s]
	[INFO] hudi-spark_2.12 .................................... SUCCESS [ 38.785 s]
	[INFO] hudi-utilities_2.12 ................................ SUCCESS [  6.536 s]
	[INFO] hudi-utilities-bundle_2.12 ......................... SUCCESS [ 14.409 s]
	[INFO] hudi-cli ........................................... SUCCESS [ 13.177 s]
	[INFO] hudi-java-client ................................... SUCCESS [  2.573 s]
	[INFO] hudi-flink-client .................................. SUCCESS [  7.770 s]
	[INFO] hudi-spark2_2.12 ................................... SUCCESS [ 13.000 s]
	[INFO] hudi-dla-sync ...................................... SUCCESS [  1.552 s]
	[INFO] hudi-sync .......................................... SUCCESS [  0.013 s]
	[INFO] hudi-hadoop-mr-bundle .............................. SUCCESS [  5.564 s]
	[INFO] hudi-hive-sync-bundle .............................. SUCCESS [  1.656 s]
	[INFO] hudi-spark3-bundle_2.12 ............................ SUCCESS [ 11.815 s]
	[INFO] hudi-presto-bundle ................................. SUCCESS [ 38.301 s]
	[INFO] hudi-timeline-server-bundle ........................ SUCCESS [ 10.857 s]
	[INFO] hudi-hadoop-docker ................................. SUCCESS [  1.884 s]
	[INFO] hudi-hadoop-base-docker ............................ SUCCESS [ 46.250 s]
	[INFO] hudi-hadoop-namenode-docker ........................ SUCCESS [  1.206 s]
	[INFO] hudi-hadoop-datanode-docker ........................ SUCCESS [  1.241 s]
	[INFO] hudi-hadoop-history-docker ......................... SUCCESS [  1.218 s]
	[INFO] hudi-hadoop-hive-docker ............................ SUCCESS [  8.821 s]
	[INFO] hudi-hadoop-sparkbase-docker ....................... SUCCESS [  1.730 s]
	[INFO] hudi-hadoop-sparkmaster-docker ..................... SUCCESS [  1.138 s]
	[INFO] hudi-hadoop-sparkworker-docker ..................... SUCCESS [  1.108 s]
	[INFO] hudi-hadoop-sparkadhoc-docker ...................... SUCCESS [  1.155 s]
	[INFO] hudi-hadoop-presto-docker .......................... SUCCESS [  1.182 s]
	[INFO] hudi-integ-test .................................... SUCCESS [ 46.860 s]
	[INFO] hudi-integ-test-bundle ............................. SUCCESS [03:58 min]
	[INFO] hudi-examples ...................................... SUCCESS [  6.709 s]
	[INFO] hudi-flink_2.12 .................................... SUCCESS [03:19 min]
	[INFO] hudi-kafka-connect ................................. SUCCESS [ 24.907 s]
	[INFO] hudi-flink-bundle_2.12 ............................. SUCCESS [03:31 min]
	[INFO] hudi-kafka-connect-bundle .......................... SUCCESS [ 27.455 s]
	


## 免密操作

### 修改host (所有机器)

sudo vim /etc/hosts

	192.168.2.121 hudi1
	192.168.2.122 hudi2
	192.168.2.123 hudi3
	192.168.2.124 hudi4

### 创建免密 （所有机器）

	ssh-keygen -t rsa 
	
	#在对应机器上执行
	ssh-copy-id hudi1
	
	#在对应机器上执行
	ssh-copy-id hudi2
	
	#在对应机器上执行
	ssh-copy-id hudi3
	
	#在对应机器上执行
	ssh-copy-id hudi4

分别输入yes 和 密码

### 机器之间互相免密

	ssh-copy-id hudi1
	ssh-copy-id hudi2
	ssh-copy-id hudi3
	ssh-copy-id hudi4




## 安装Zookeeper

在Hudi2上：

	tar -zxf apache-zookeeper-3.8.0-bin.tar.gz
	mv apache-zookeeper-3.8.0-bin zookeeper
	
####修改配置：

	cd /home/zheyi/zookeeper/conf
	cp zoo_sample.cfg zoo.cfg
	vi zoo.cfg
	
插入：

	dataDir=/home/zheyi/zookeeper/data
	server.1=hudi2:2888:3888
	server.2=hudi3:2888:3888
	server.3=hudi4:2888:3888

注意：2888 原子广播端口，3888 选举端口，zookeeper 有几个节点，就配置几个 server。

	
####创建目录
	mkdir /home/zheyi/zookeeper/data

#### 复制
	scp -r /home/zheyi/zookeeper zheyi@hudi3:/home/zheyi
	scp -r /home/zheyi/zookeeper zheyi@hudi4:/home/zheyi

#### 创建id
在hudi2、hudi3、hudi4中分别执行：

	echo "1" >> /home/zheyi/zookeeper/data/myid
	echo "2" >> /home/zheyi/zookeeper/data/myid
	echo "3" >> /home/zheyi/zookeeper/data/myid


#### 关闭防火墙 （所有机器）

	systemctl stop firewalld
	systemctl disable firewalld.service

#### 启动命令

在Hudi2，Hudi3，Hudi4中

	cd /home/zheyi/zookeeper/bin
	sh zkServer.sh start


## 安装Hadoop

#### 解压程序
在Hudi1上:

	tar -zxf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop
	


####配置core-site.xml文件

	vi /home/zheyi/hadoop/etc/hadoop/core-site.xml

插入

	<configuration>
	    <!--指定zookeeper地址-->
	    <property>
	        <name>ha.zookeeper.quorum</name>
	        <value>hudi2:2181,hudi3:2181,hudi4:2181</value>
	    </property>
	    <!--指定Hadoop临时目录-->
	    <property>
	        <name>hadoop.temp.dir</name>
	        <value>/home/zheyi/hadoop/tmp</value>
	    </property>
	    <!--指定Namenode访问端口-->
	    <property>
	        <name>fs.defaultFS</name>
	        <value>hdfs://hudi1:9000</value>
	    </property>
	</configuration>


#### hadoop-env.sh

	vi hadoop-env.sh
	
插入

	export JAVA_HOME=/home/zheyi/java

#### hdfs-site.xml

	vi hdfs-site.xml

插入：

	<configuration>
	
	<!--指定设备备份数量-->
	    <property>
	        <name>dfs.replication</name>
	        <value>3</value>
	    </property>
	
	<!--指定高可用集群的名字空间-->
	    <property>
	        <name>dfs.nameservices</name>
	        <value>mycluster</value>
	    </property>
	
	<!-- 指定NameNode节点的名字空间 -->
	    <property>
	      <name>dfs.ha.namenodes.mycluster</name>
	      <value>nn1,nn2</value>
	    </property>
	
	<!--指定nn1,nn2的RPC地址-->
	    <property>
	      <name>dfs.namenode.rpc-address.mycluster.nn1</name>
	      <value>hudi1:9000</value>
	    </property>
	    <property>
	      <name>dfs.namenode.rpc-address.mycluster.nn2</name>
	      <value>hudi2:9000</value>
	    </property>
	
	<!-- 指定nn1、nn2的http地址 -->
	        <property>
	        <name>dfs.namenode.http-address.mycluster.nn1</name>
	        <value>hudi1:50070</value>
	    </property>
	    <property>
	      <name>dfs.namenode.http-address.mycluster.nn2</name>
	      <value>hudi2:50070</value>
	    </property>
	
	<!--解释：hadoop 守护进程一般同时运行RPC 和HTTP两个服务器，RPC服务器支持守护进程间的通信，HTTP服务器则提供与用户交互的Web页面。-->
	
	
	<!--设置共享edits的存放地址，将共享edits文件存放在哎QJournal集群中的QJCluster目录下-->
	    <property>
	      <name>dfs.namenode.shared.edits.dir</name>
	      <value>qjournal://hudi2:8485;hudi3:8485;hudi4:8485/QJCluster</value>
	    </property>
	
	
	<!--指定JournalNode集群在对NameNode的目录进行共享时，自己存储数据的磁盘路径-->
	
	    <property>
	      <name>dfs.journalnode.edits.dir</name>
	      <value>/home/zheyi/hadoop/QJEditsData</value>
	    </property>
	
	
	<!--指定是否启动自动故障恢复，即当NameNode出故障时，是否自动切换到另一台NameNode-->
	
	    <property>
	       <name>dfs.ha.automatic-failover.enabled</name>
	       <value>true</value>
	     </property>
	
	
	<!--指定cluster1出故障时，哪个实现类负责执行故障切换-->
	
	    <property>
	      <name>dfs.client.failover.proxy.provider.mycluster</name>
	      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	    </property>
	
	
	<!--一旦需要NameNode切换，使用ssh方式进行操作-->
	
	    <property>
	      <name>dfs.ha.fencing.methods</name>
	      <value>sshfence</value>
	    </property>
	
	
	<!--如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置-->
	
	    <property>
	      <name>dfs.ha.fencing.ssh.private-key-files</name>
	      <value>/home/zheyi/.ssh/id_rsa</value>
	    </property>
	
	 <!--关闭权限校验-->   
	    <property>
	        <name>dfs.permissions</name>
	        <value>false</value>
	     </property>    
	
	</configuration>



#### yarn-site.xml

	<configuration>
	
	<!-- Site specific YARN configuration properties -->
	 <property>
	        <name>yarn.nodemanager.aux-services</name>
	        <value>mapreduce_shuffle</value>
	        <description>NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序</description>
	    </property>
	
	<property>
	    <name>yarn.resourcemanager.hostname</name>
	    <value>hudi1</value>
	    <description>titan1</description>
	</property> 
	
	<property>
	    <name>yarn.scheduler.maximum-allocation-mb</name>
	    <value>2048</value>
	</property>
	
	<property>
	    <name>yarn.scheduler.minimum-allocation-mb</name>
	    <value>2048</value>
	</property>
	
	<property>
	    <name>yarn.nodemanager.vmem-pmem-ratio</name>
	    <value>2.1</value>
	</property>
	
	<property>
	    <name>mapred.child.java.opts</name>
	    <value>-Xmx2048m</value>
	</property>
	
	
	</configuration>


#### mapred-site.xml

	vi mapred-site.xml

插入：

	<configuration>
	   <property>
	        <name>mapreduce.framework.name</name>
	        <value>yarn</value>
	    </property>
	    <property>
	      <name>yarn.app.mapreduce.am.env</name>
	      <value>HADOOP_MAPRED_HOME=/home/zheyi/hadoop</value>
	    </property>
	    <property>
	      <name>mapreduce.map.env</name>
	      <value>HADOOP_MAPRED_HOME=/home/zheyi/hadoop</value>
	    </property>
	    <property>
	      <name>mapreduce.reduce.env</name>
	      <value>HADOOP_MAPRED_HOME=/home/zheyi/hadoop</value>
	    </property>
	</configuration>


#### slaves

	vi workers 
	删除localhost

插入

	hudi2
	hudi3
	hudi4


#### yarn-env.sh
	vi yarn-env.sh

插入
	
	export JAVA_HOME=/home/zheyi/java


#### 复制hadoop 到其他服务器

	scp -r /home/zheyi/hadoop zheyi@hudi2:/home/zheyi
	scp -r /home/zheyi/hadoop zheyi@hudi3:/home/zheyi
	scp -r /home/zheyi/hadoop zheyi@hudi4:/home/zheyi



####环境变量(所有服务器上增加)

	sudo vim /etc/profile

插入：

	#zk
	export ZK_HOME=/home/zheyi/zookeeper
	export PATH=$PATH:$ZK_HOME/bin
	
	#hadoop
	export HADOOP_HOME=/home/zheyi/hadoop
	export PATH=$PATH:$HADOOP_HOME/bin
	export PATH=$PATH:$HADOOP_HOME/sbin
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export HADOOP_YARN_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
	export JAVA_LIBRARY_PATH=$HADDOP_HOME/lib/native:$JAVA_LIBRARY_PATH
	
让配置生效

	source /etc/profile
	
#### 启动Hadoop集群

在hudi2, hudi3, hudi4中启动journalnode

	cd /home/zheyi/hadoop/sbin
	hadoop-daemon.sh start journalnode	
	
在hudi1中格式化Namenode

	hdfs namenode -format
	
显示下列信息表示成功：

	common.Storage: Storage directory /tmp/hadoop-zheyi/dfs/name has been successfully formatted.
	
复制hudi1 下的 tmp文件到其他服务器

	scp -r /tmp/hadoop-zheyi zheyi@hudi2:/tmp/hadoop-zheyi
	scp -r /tmp/hadoop-zheyi zheyi@hudi3:/tmp/hadoop-zheyi
	scp -r /tmp/hadoop-zheyi zheyi@hudi4:/tmp/hadoop-zheyi


#####启动hadoop

	start-all.sh
	
##### 如果格式化失败需要重置zk

	hdfs  zkfc -formatZK

#### 切换节点

1、将 NN2 切换为 Standby 备用节点

	hdfs haadmin -transitionToStandby --forcemanual nn2
	
2、将 NN1 切换为 Active 备用节点

	hdfs haadmin -transitionToActive --forcemanual nn1

#### 查看UI界面

	http://192.168.3.121:50070/dfshealth.html#tab-overview

	
## 安装Spark

在Hudi2上：

	tar -zxvf  spark-3.1.2-bin-hadoop3.2.tgz 
	mv spark-3.1.2-bin-hadoop3.2 spark
	 

修改配置文件

	sudo vim /etc/profile
	
插入：

	export SPARK_HOME=/home/zheyi/spark
	export PATH=$SPARK_HOME/sbin

	
####修改配置文件：

	cd /home/zheyi/spark/conf
	cp spark-env.sh.template spark-env.sh
	vi spark-env.sh

插入：

	#配置java环境变量
	export JAVA_HOME=/home/zheyi/java/
	
	#指定master的IP
	export SPARK_MASTER_HOST=192.168.2.121
	
	#指定Master的端口
	export SPARK_MASTER_PORT=7077
	
#### slaves

	cd /home/zheyi/spark/conf
	vi slave

插入：

	192.168.2.121
	
	
#### 关闭防火墙

	systemctl stop firewalld.service
	systemctl disable firewalld.service
	
	
#### 启动

	cd /home/zheyi/spark/sbin
	sh start-all.sh 

#### 查看

	http://192.168.2.121:8080/


### 安装Hadoop
	




