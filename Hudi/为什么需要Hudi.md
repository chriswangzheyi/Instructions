# 为什么需要Hudi

https://blog.csdn.net/qq_22473611/article/details/121339523

## 痛点

传统的离线数仓，通常数据是T+1的，不能满足对当日数据分析的需求。
而流式计算一般是基于窗口，并且窗口逻辑相对比较固定。

现有一类特殊的需求，业务分析比较熟悉现有事务数据库的数据结构，并且希望有很多即席分析，这些分析包含当日比较实时的数据。惯常他们是基于Mysql从库，直接通过Sql做相应的分析计算。但很多时候会遇到如下障碍

* 数据量较大、分析逻辑较为复杂时，Mysql从库耗时较长
* 一些跨库的分析无法实现

## hudi的特点

Apache Hudi 在基于 HDFS 数据存储之上，提供了两种流原语：

1. 插入更新
1. 增量拉取

 一般来说，我们会将大量数据存储到HDFS，新数据增量写入，而旧数据鲜有改动，特别是在经过数据清洗，放入数据仓库的场景。而且在数据仓库如 hive中，对于update的支持非常有限，计算昂贵。另一方面，若是有仅对某段时间内新增数据进行分析的场景，则hive、presto、hbase等也未提供原生方式，而是需要根据时间戳进行过滤分析。


 在此需求下，Hudi可以提供这两种需求的实现。第一个是对record级别的更新，另一个是仅对增量数据的查询。且Hudi提供了对Hive、presto、Spark的支持，可以直接使用这些组件对Hudi管理的数据进行查询。

## 使用Hudi的优点

* 使用Bloomfilter机制+二次查找，可快速确定记录是更新还是新增
* 更新范围小，是文件级别，不是表级别
* 文件大小与hdfs的Blocksize保持一致
* 数据文件使用parquet格式，充分利用列存的优势（dremal论文实现）
* 提供了可扩展的大数据更新框架
* 并发度由spark控制

（时间关系没有复制完毕，请稍后补充）