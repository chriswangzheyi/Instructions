{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf2ef29-9862-47e3-816e-565321ed5817",
   "metadata": {},
   "source": [
    "# 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3cc1d-dff9-439e-8679-316b8bc5cd21",
   "metadata": {},
   "source": [
    "函数所有偏导数构成的向量就叫做梯度。\n",
    "\n",
    "梯度是导数在多维空间的扩展。\n",
    "\n",
    "导数告诉你函数在某一点变化的快慢（斜率）。\n",
    "梯度告诉你函数在一个多维空间中，往哪个方向变化得最快。\n",
    "\n",
    "![](Images/3.png)\n",
    "\n",
    "\n",
    "梯度向量的方向即为函数值增长最快的方向。\n",
    "\n",
    "模型就是通过不断地减小损失函数值的方式来进行学习的。让损失函数最小化，通常就要采用梯度下降的方式，即：每一次给模型的权重进行更新的时候，都要按照梯度的反方向进行。\n",
    "为什么呢？因为梯度向量的方向即为函数值增长最快的方向，反方向则是减小最快的方向\n",
    "\n",
    "![](Images/4.png)\n",
    "\n",
    "\n",
    "\n",
    "![](Images/5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e2932-49db-44c1-97de-8d74a089e2a9",
   "metadata": {},
   "source": [
    "# 链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0908b-8475-47c6-9f83-e33c8b74e22f",
   "metadata": {},
   "source": [
    "链式法则：“两个函数组合起来的复合函数，导数等于里面函数代入外函数值的导数，乘以里面函数之导数。”\n",
    "\n",
    "\n",
    "![](Images/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56489594-9f6d-455f-8366-92124644a41f",
   "metadata": {},
   "source": [
    "# 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322c21c-2e2f-44ee-b3e7-c3a7bbee0114",
   "metadata": {},
   "source": [
    "反向传播算法（Backpropagation）是目前训练神经网络最常用且最有效的算法。模型就是通过反向传播的方式来不断更新自身的参数，从而实现了“学习”知识的过程。\n",
    "\n",
    "前向传播：数据从输入层经过隐藏层最后输出，其过程和之前讲过的前馈网络基本一致。\n",
    "\n",
    "计算误差并传播：计算模型输出结果和真实结果之间的误差，并将这种误差通过某种方式反向传播，即从输出层向隐藏层传递并最后到达输入层。\n",
    "\n",
    "迭代：在反向传播的过程中，根据误差不断地调整模型的参数值，并不断地迭代前面两个步骤，直到达到模型结束训练的条件。\n",
    "\n",
    "\n",
    "![](Images/7.png)\n",
    "\n",
    "\n",
    "\n",
    "![](Images/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf2865-f40c-4e23-81cd-64547600d5cb",
   "metadata": {},
   "source": [
    "# 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72013a2-17b2-4ecc-850a-d778c63ca3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=1.0000, y=25.0000, L=225.0000\n",
      "解析梯度 dL/dx = 600.0000\n",
      "数值梯度 dL/dx ≈ 600.0000\n"
     ]
    }
   ],
   "source": [
    "# 目标：L = (y - 10)^2，其中 y = (2x + 3)^2\n",
    "# 用链式法则算 dL/dx，并和数值微分对比\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def forward(x):\n",
    "    y = (2*x + 3)**2\n",
    "    L = (y - 10)**2\n",
    "    return L, y\n",
    "\n",
    "def analytical_grad(x):\n",
    "    # dL/dy = 2(y-10)\n",
    "    y = (2*x + 3)**2\n",
    "    dL_dy = 2*(y - 10)\n",
    "    # dy/dx = 4(2x+3)\n",
    "    dy_dx = 4*(2*x + 3)\n",
    "    # 链式法则：dL/dx = dL/dy * dy/dx\n",
    "    return dL_dy * dy_dx\n",
    "\n",
    "def numerical_grad(x, eps=1e-6):   \n",
    "    L1, _ = forward(x + eps)  #计算当x增加一点时，损失函数L的值。\n",
    "    L2, _ = forward(x - eps)  #计算当x减少一点时，损失函数L的值。\n",
    "    return (L1 - L2) / (2*eps) #用“中心差分公式”近似导数\n",
    "\n",
    "x = 1.0\n",
    "L, y = forward(x)\n",
    "g_ana = analytical_grad(x)\n",
    "g_num = numerical_grad(x)\n",
    "\n",
    "print(f\"x={x:.4f}, y={y:.4f}, L={L:.4f}\")\n",
    "print(f\"解析梯度 dL/dx = {g_ana:.4f}\")\n",
    "print(f\"数值梯度 dL/dx ≈ {g_num:.4f}\")\n",
    "\n",
    "#解析梯度（Analytical Gradient）：用数学公式（链式法则）精确算出来的。\n",
    "#数值梯度（Numerical Gradient）：靠“试探”——让参数稍微动一下，看损失变多少。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda0ea0-022c-414d-8710-8d299d9fdd92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
