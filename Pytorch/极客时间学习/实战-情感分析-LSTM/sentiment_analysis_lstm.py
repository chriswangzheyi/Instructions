# -*- coding: utf-8 -*-
"""
åŸºäº LSTM çš„æƒ…æ„Ÿåˆ†æå®Œæ•´å®ç°
ä½¿ç”¨ IMDB æ•°æ®é›†è¿›è¡ŒäºŒåˆ†ç±»ä»»åŠ¡ï¼ˆæ­£é¢/è´Ÿé¢æƒ…æ„Ÿï¼‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import time
import os
import urllib.request
import tarfile


# ==================== 0. æ•°æ®é›†ä¸‹è½½å’ŒåŠ è½½ ====================

def download_imdb(data_dir='./data'):
    """ä¸‹è½½ IMDB æ•°æ®é›†"""
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    
    url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'
    tar_path = os.path.join(data_dir, 'aclImdb_v1.tar.gz')
    extract_path = os.path.join(data_dir, 'aclImdb')
    
    if os.path.exists(extract_path):
        print("æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        return extract_path
    
    print(f"æ­£åœ¨ä¸‹è½½ IMDB æ•°æ®é›†...")
    try:
        urllib.request.urlretrieve(url, tar_path)
        print("ä¸‹è½½å®Œæˆï¼Œæ­£åœ¨è§£å‹...")
        with tarfile.open(tar_path, 'r:gz') as tar:
            tar.extractall(data_dir)
        os.remove(tar_path)
        print("è§£å‹å®Œæˆï¼")
        return extract_path
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥: {e}")
        print("ä½¿ç”¨ç¤ºä¾‹æ•°æ®ä»£æ›¿...")
        return None


class IMDBDataset(Dataset):
    """è‡ªå®šä¹‰ IMDB æ•°æ®é›†"""
    
    def __init__(self, data_dir, split='train', use_sample=False):
        self.data = []
        
        if use_sample or data_dir is None:
            # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
            if split == 'train':
                self.data = [
                    ("This movie is great! I love it.", "pos"),
                    ("Terrible film, waste of time.", "neg"),
                    ("Amazing performance by the actors.", "pos"),
                    ("Boring and predictable plot.", "neg"),
                    ("Excellent cinematography and story.", "pos"),
                    ("Very disappointing, would not recommend.", "neg"),
                    ("A masterpiece! Must watch.", "pos"),
                    ("Poor acting and bad script.", "neg"),
                    ("Fantastic movie, highly recommended!", "pos"),
                    ("Awful experience, total disaster.", "neg"),
                ] * 250  # 2500 samples
            else:
                self.data = [
                    ("Good movie, enjoyed it.", "pos"),
                    ("Not good at all.", "neg"),
                    ("Best film I've seen.", "pos"),
                    ("Completely terrible.", "neg"),
                ] * 125  # 500 samples
        else:
            # ä»æ–‡ä»¶åŠ è½½
            for label in ['pos', 'neg']:
                dir_path = os.path.join(data_dir, split, label)
                if os.path.exists(dir_path):
                    for filename in os.listdir(dir_path):
                        if filename.endswith('.txt'):
                            with open(os.path.join(dir_path, filename), 'r', encoding='utf-8') as f:
                                text = f.read()
                                self.data.append((text, label))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]


def simple_tokenizer(text):
    """ç®€å•çš„åˆ†è¯å™¨"""
    return text.lower().replace('.', ' .').replace('!', ' !').replace('?', ' ?').replace(',', ' ,').split()


# ==================== 1. æ•°æ®é¢„å¤„ç† ====================

def yield_tokens(data_iter, tokenizer): #ä»æ•°æ®é›†ä¸­æå–æ‰€æœ‰çš„è¯ï¼ˆtokenï¼‰ï¼Œç”¨äºæ„å»ºè¯æ±‡è¡¨
    """ä»æ•°æ®è¿­ä»£å™¨ä¸­æå– token"""
    for label, text in data_iter:  #data_iter: IMDB æ•°æ®é›†çš„è¿­ä»£å™¨ï¼Œæ¯æ¬¡è¿”å› (label, text) å¯¹
        yield tokenizer(text) #yield: Python ç”Ÿæˆå™¨å…³é”®å­—ï¼Œæ¯æ¬¡è¿”å›ä¸€ä¸ªç»“æœï¼Œä½†ä¸ä¼šç»“æŸå‡½æ•°

#è¿™ä¸ªå‡½æ•°å°±åƒå»ºç«‹ä¸€ä¸ªè¯å…¸ç´¢å¼•ï¼ŒæŠŠæ‰€æœ‰é‡è¦çš„è¯ï¼ˆå‡ºç°é¢‘ç‡ â‰¥ 5ï¼‰éƒ½ç¼–å·ï¼Œæ–¹ä¾¿åç»­æ¨¡å‹å¤„ç†ã€‚å°±åƒç»™æ¯ä¸ªå­¦ç”Ÿåˆ†é…å­¦å·ä¸€æ ·ã€‚
def build_vocabulary(train_data, tokenizer, min_freq=5):   #min_freq=5: æœ€å°è¯é¢‘ï¼Œåªä¿ç•™å‡ºç°æ¬¡æ•° â‰¥ 5 çš„è¯ã€‚å‡ºç° â‰¥ 5 æ¬¡çš„è¯ â†’ åŠ å…¥è¯æ±‡è¡¨
    """æ„å»ºè¯æ±‡è¡¨"""
    word_count = {}
    
    # ç»Ÿè®¡è¯é¢‘
    for text, _ in train_data:
        tokens = tokenizer(text)
        for token in tokens:
            word_count[token] = word_count.get(token, 0) + 1
    
    # æ„å»ºè¯æ±‡è¡¨
    vocab = {'<unk>': 0, '<pad>': 1}
    idx = 2
    for word, count in word_count.items():
        if count >= min_freq:
            vocab[word] = idx
            idx += 1
    
    # åˆ›å»ºé»˜è®¤å€¼å­—å…¸ï¼ˆè¿”å› <unk> ç´¢å¼•ï¼‰
    class VocabDict(dict):
        def __missing__(self, key):
            return self['<unk>']
    
    return VocabDict(vocab)

#å°†ä¸€æ‰¹åŸå§‹æ–‡æœ¬æ•°æ®è½¬æ¢æˆæ¨¡å‹å¯ä»¥å¤„ç†çš„å¼ é‡æ ¼å¼ï¼Œå¹¶ä¸”å°†æ–‡æœ¬åºåˆ—é•¿åº¦ç»Ÿä¸€ï¼Œæ–¹ä¾¿åç»­çš„è®­ç»ƒã€‚
def collate_batch(batch, tokenizer, vocab, device):
    """
    å°†ä¸€ä¸ª batch çš„æ•°æ®å¤„ç†æˆæ¨¡å‹éœ€è¦çš„æ ¼å¼
    è¿”å›ï¼šæ–‡æœ¬çš„ç´¢å¼•åºåˆ—ã€åºåˆ—é•¿åº¦ã€æ ‡ç­¾
    """
    label_list, text_list, length_list = [], [], []   
    
    for label, text in batch:
        # æ ‡ç­¾å¤„ç†ï¼špos -> 1, neg -> 0
        label_list.append(1 if label == 'pos' else 0)
        
        # æ–‡æœ¬å¤„ç†ï¼šåˆ†è¯ -> è½¬ç´¢å¼•
        tokens = tokenizer(text)
        text_indices = [vocab[token] for token in tokens]
        text_list.append(text_indices)
        length_list.append(len(text_indices))
        """
        # sä¸Šé¢çš„ä»£ç ä¸¾ä¸ªä¾‹å­ï¼šå‡è®¾ batch æœ‰ 3 æ¡æ•°æ®
        batch = [
            ('pos', "This movie is great"),
            ('neg', "Bad film"),
            ('pos', "I love it")
        ]
        # å¤„ç†åï¼š
        label_list = [1, 0, 1]  # pos->1, neg->0

        text_list = [
            [2, 4, 3, 5],      # "This movie is great"
            [10, 6],           # "Bad film"
            [8, 7, 9]          # "I love it"
        ]
        length_list = [4, 2, 3]  # æ¯ä¸ªå¥å­çš„é•¿åº¦
        """
    
    # è½¬æ¢ä¸ºå¼ é‡
    label_list = torch.tensor(label_list, dtype=torch.long)
    length_list = torch.tensor(length_list, dtype=torch.long)
    
    # å¯¹æ–‡æœ¬è¿›è¡Œå¡«å……ï¼Œä½¿ä¸€ä¸ª batch å†…çš„åºåˆ—é•¿åº¦ç›¸åŒ
    # æŒ‰é•¿åº¦é™åºæ’åˆ—ï¼ˆpack_padded_sequence è¦æ±‚ï¼‰
    length_list, sorted_idx = length_list.sort(descending=True)
    label_list = label_list[sorted_idx]
    text_list = [text_list[i] for i in sorted_idx]
    """ 
    ä¸¾ä¸ªä¾‹å­ æ’åºå‰ï¼š
    ä½ç½®0: text=[2,4,3,5]    label=1  length=4  â† æœ€é•¿
    ä½ç½®1: text=[10,6]       label=0  length=2  â† æœ€çŸ­  
    ä½ç½®2: text=[8,7,9]      label=1  length=3  â† ä¸­ç­‰
    æ’åºåï¼š
    ä½ç½®0: text=[2,4,3,5]    label=1  length=4  â† æœ€é•¿
    ä½ç½®1: text=[8,7,9]      label=1  length=3  â† ä¸­ç­‰
    ä½ç½®2: text=[10,6]       label=0  length=2  â† æœ€çŸ­
    """

    # å°†ä¸åŒé•¿åº¦çš„åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼Œç„¶åè½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    padded_text_list = []
    max_len = length_list[0].item()
    for text in text_list:
        padded = text + [vocab['<pad>']] * (max_len - len(text))
        padded_text_list.append(padded)
    
    text_list = torch.tensor(padded_text_list, dtype=torch.long)
    """
    ä¸¾ä¾‹ å¡«å……å‰ï¼š
    text_list[0] = [2, 4, 3, 5]         â† é•¿åº¦ 4
    text_list[1] = [8, 7, 9]            â† é•¿åº¦ 3 (çŸ­1ä¸ª)
    text_list[2] = [10, 6]              â† é•¿åº¦ 2 (çŸ­2ä¸ª)
    âŒ æ— æ³•ç»„æˆçŸ©å½¢å¼ é‡ï¼
    å¡«å……åï¼š                       #è¯´æ˜ï¼š<pad> è¿™ä¸ªç‰¹æ®Šæ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•æ˜¯1
    padded_text_list[0] = [2, 4, 3, 5]      â† é•¿åº¦ 4
    padded_text_list[1] = [8, 7, 9, 1]      â† é•¿åº¦ 4 (å¡«å……1ä¸ª)
    padded_text_list[2] = [10, 6, 1, 1]     â† é•¿åº¦ 4 (å¡«å……2ä¸ª)
    âœ… å¯ä»¥ç»„æˆçŸ©å½¢å¼ é‡ [3, 4]ï¼
    """
    return text_list.to(device), length_list.to(device), label_list.to(device)


# ==================== 2. æ¨¡å‹å®šä¹‰ ====================

class LSTM(nn.Module):
    """
    LSTM æƒ…æ„Ÿåˆ†ææ¨¡å‹
    ä½¿ç”¨åŒå‘ LSTM + Dropout + å…¨è¿æ¥å±‚
    """
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, 
                 n_layers, bidirectional, dropout_rate, pad_idx):
        super(LSTM, self).__init__()
        
        # Embedding å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)
        
        # LSTM å±‚
        self.lstm = nn.LSTM(
            embedding_dim,                                  # è¯åµŒå…¥ç»´åº¦ï¼ˆå¦‚ 300ï¼‰
            hidden_dim,                                     # LSTM éšè—å±‚ç»´åº¦ï¼ˆå¦‚ 300ï¼‰
            num_layers=n_layers,                            # LSTM å±‚æ•°ï¼ˆå¦‚ 2ï¼‰
            bidirectional=bidirectional,                    # æ˜¯å¦ä½¿ç”¨åŒå‘ LSTMï¼ˆå¦‚ Trueï¼‰
            dropout=dropout_rate if n_layers > 1 else 0,    # Dropout æ¯”ç‡ï¼ˆå¦‚ 0.5ï¼‰
            batch_first=True                                # æ˜¯å¦ä½¿ç”¨ batch_firstï¼ˆå¦‚ Trueï¼‰
        )
        
        # Dropout å±‚
        self.dropout = nn.Dropout(dropout_rate)
        
        # å…¨è¿æ¥å±‚
        # å¦‚æœæ˜¯åŒå‘ LSTMï¼Œhidden_dim è¦ä¹˜ä»¥ 2
        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim
        self.fc = nn.Linear(fc_input_dim, output_dim)
    
    def forward(self, text, text_lengths):
        """
        å‰å‘ä¼ æ’­
        text: [batch_size, seq_len]
        text_lengths: [batch_size]
        """
        # Embedding
        # embedded: [batch_size, seq_len, embedding_dim]
        embedded = self.dropout(self.embedding(text))  #ä¸ºä»€ä¹ˆåœ¨ Embedding ååŠ  Dropoutï¼Ÿé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
        
        # æ‰“åŒ…åºåˆ—ï¼ˆæé«˜æ•ˆç‡ï¼Œé¿å…å¯¹ padding éƒ¨åˆ†è¿›è¡Œè®¡ç®—ï¼‰
        packed_embedded = pack_padded_sequence(
            embedded,              # è¾“å…¥ï¼šå¡«å……åçš„è¯å‘é‡
            text_lengths.cpu(),    # è¾“å…¥ï¼šæ¯ä¸ªåºåˆ—çš„é•¿åº¦
            batch_first=True,      # è¾“å…¥ï¼šæ˜¯å¦ä½¿ç”¨ batch_first
            enforce_sorted=True    # è¾“å…¥ï¼šæ˜¯å¦å¯¹åºåˆ—è¿›è¡Œæ’åº
        )
        
        # LSTM
        # packed_output: æ‰“åŒ…çš„è¾“å‡º
        # hidden: [num_layers * num_directions, batch_size, hidden_dim]
        # cell: [num_layers * num_directions, batch_size, hidden_dim]
        packed_output, (hidden, cell) = self.lstm(packed_embedded)    #packed_output - æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼Œhidden - æœ€åçš„éšè—çŠ¶æ€ï¼Œcell - ç»†èƒçŠ¶æ€
        
        # è§£åŒ…ï¼ˆæœ¬ä¾‹ä¸­ä¸ä½¿ç”¨ outputï¼Œç›´æ¥ä½¿ç”¨ hiddenï¼‰
        # output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)
        
        # å¤„ç†åŒå‘ LSTM çš„éšè—çŠ¶æ€
        # å–æœ€åä¸€å±‚çš„æ­£å‘å’Œåå‘çš„éšè—çŠ¶æ€è¿›è¡Œæ‹¼æ¥
        if self.lstm.bidirectional:
            # hidden[-2]: æœ€åä¸€å±‚æ­£å‘çš„éšè—çŠ¶æ€
            # hidden[-1]: æœ€åä¸€å±‚åå‘çš„éšè—çŠ¶æ€
            hidden = self.dropout(torch.cat([hidden[-2], hidden[-1]], dim=-1))
        else:
            # hidden[-1]: æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
            hidden = self.dropout(hidden[-1])
        
        # å…¨è¿æ¥å±‚
        # output: [batch_size, output_dim]
        output = self.fc(hidden)
        
        return output


# ==================== 3. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ====================

def train_epoch(model, dataloader, optimizer, criterion, device):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    epoch_loss = 0
    epoch_acc = 0
    
    for text, text_lengths, labels in dataloader:
        # å‰å‘ä¼ æ’­
        predictions = model(text, text_lengths)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(predictions, labels)
        
        # è®¡ç®—å‡†ç¡®ç‡
        predicted_labels = predictions.argmax(dim=1)
        acc = (predicted_labels == labels).float().mean()
        '''
        predictions:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  2.3   -1.5  â”‚ â†’ argmax â†’ 0 (2.3 > -1.5)
        â”‚ -0.8    1.9  â”‚ â†’ argmax â†’ 1 (1.9 > -0.8)
        â”‚  1.2    0.3  â”‚ â†’ argmax â†’ 0 (1.2 > 0.3)
        â”‚  0.5    0.7  â”‚ â†’ argmax â†’ 1 (0.7 > 0.5)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“ dim=1
        predicted_labels = [0, 1, 0, 1]
        '''
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()
    
    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)


def evaluate(model, dataloader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    epoch_loss = 0
    epoch_acc = 0
    
    with torch.no_grad():
        for text, text_lengths, labels in dataloader:
            # å‰å‘ä¼ æ’­
            predictions = model(text, text_lengths)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(predictions, labels)
            
            # è®¡ç®—å‡†ç¡®ç‡
            predicted_labels = predictions.argmax(dim=1)
            acc = (predicted_labels == labels).float().mean()
            
            epoch_loss += loss.item()
            epoch_acc += acc.item()
    
    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)


def epoch_time(start_time, end_time):
    """è®¡ç®—æ—¶é—´"""
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs


# ==================== 4. é¢„æµ‹å‡½æ•° ====================

def predict_sentiment(model, tokenizer, vocab, sentence, device):
    """
    é¢„æµ‹å•ä¸ªå¥å­çš„æƒ…æ„Ÿ
    """
    model.eval()  #è¯„ä¼°æ¨¡å¼ï¼šDropout å…³é—­ï¼šä¸å†éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼ŒBatchNorm ä½¿ç”¨è®­ç»ƒæ—¶çš„ç»Ÿè®¡é‡ï¼ˆå¦‚æœæœ‰ï¼‰
    
    # åˆ†è¯å¹¶è½¬æ¢ä¸ºç´¢å¼•
    tokens = tokenizer(sentence)  
    indices = [vocab[token] for token in tokens]
    length = torch.tensor([len(indices)], dtype=torch.long)
    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦
    
    # è½¬ç§»åˆ°è®¾å¤‡
    tensor = tensor.to(device)
    length = length.to(device)
    
    # é¢„æµ‹
    with torch.no_grad():    #ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼šèŠ‚çœå†…å­˜ï¼ŒåŠ é€Ÿè®¡ç®—ï¼Œé¢„æµ‹æ—¶ä¸éœ€è¦åå‘ä¼ æ’­
        prediction = model(tensor, length)
        probabilities = torch.softmax(prediction, dim=1)  # è½¬æ¢ä¸ºæ¦‚ç‡
        predicted_class = prediction.argmax(dim=1).item()
    
    # è¿”å›ç»“æœ
    sentiment = 'pos' if predicted_class == 1 else 'neg'
    confidence = probabilities[0][predicted_class].item()
    
    return sentiment, confidence


# ==================== 5. ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    
    print("=" * 60)
    print("åŸºäº LSTM çš„æƒ…æ„Ÿåˆ†æå®æˆ˜")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    SEED = 1234
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # ==================== æ•°æ®åŠ è½½ ====================
    print("\n" + "=" * 60)
    print("1. åŠ è½½æ•°æ®é›†...")
    print("=" * 60)
    
    # è·å–åˆ†è¯å™¨
    tokenizer = simple_tokenizer
    
    # ä¸‹è½½/åŠ è½½ IMDB æ•°æ®é›†
    print("å‡†å¤‡ IMDB æ•°æ®é›†...")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½ï¼ˆå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ USE_SAMPLE_DATA=1 ç›´æ¥ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼‰
    import os as os_module
    force_sample = os_module.environ.get('USE_SAMPLE_DATA', '0') == '1'
    
    if force_sample:
        print("ğŸ“ ä½¿ç”¨ç¤ºä¾‹æ•°æ®æ¨¡å¼ï¼ˆè®¾ç½®äº† USE_SAMPLE_DATA=1ï¼‰")
        data_dir = None
        use_sample = True
    else:
        data_dir = download_imdb('./data')
        use_sample = (data_dir is None)
    
    if use_sample:
        print("âš ï¸ ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆ2500 è®­ç»ƒæ ·æœ¬ï¼Œ500 æµ‹è¯•æ ·æœ¬ï¼‰")
        print("æç¤ºï¼šè¦ä½¿ç”¨å®Œæ•´æ•°æ®é›†ï¼Œè¯·ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆçº¦84MBï¼‰")
    else:
        print("âœ“ ä½¿ç”¨å®Œæ•´ IMDB æ•°æ®é›†ï¼ˆ25000 è®­ç»ƒæ ·æœ¬ï¼Œ25000 æµ‹è¯•æ ·æœ¬ï¼‰")
    
    # åŠ è½½æ•°æ®é›†
    train_dataset = IMDBDataset(data_dir, split='train', use_sample=use_sample)
    test_dataset = IMDBDataset(data_dir, split='test', use_sample=use_sample)
    
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
    
    # æ„å»ºè¯æ±‡è¡¨
    print("\næ„å»ºè¯æ±‡è¡¨...")
    vocab = build_vocabulary(train_dataset, tokenizer, min_freq=2 if use_sample else 5)
    vocab_size = len(vocab)
    print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    
    # ==================== åˆ›å»ºæ•°æ®åŠ è½½å™¨ ====================
    print("\n" + "=" * 60)
    print("2. åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    print("=" * 60)
    
    BATCH_SIZE = 64
    
    # åˆ›å»º DataLoader
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        collate_fn=lambda batch: collate_batch(batch, tokenizer, vocab, device)
    )
    
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        collate_fn=lambda batch: collate_batch(batch, tokenizer, vocab, device)
    )
    
    print(f"è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_dataloader)}")
    print(f"æµ‹è¯•é›†æ‰¹æ¬¡æ•°: {len(test_dataloader)}")
    
    # ==================== æ¨¡å‹åˆå§‹åŒ– ====================
    print("\n" + "=" * 60)
    print("3. åˆå§‹åŒ–æ¨¡å‹...")
    print("=" * 60)
    
    # æ¨¡å‹è¶…å‚æ•°
    EMBEDDING_DIM = 300
    HIDDEN_DIM = 300
    OUTPUT_DIM = 2
    N_LAYERS = 2
    BIDIRECTIONAL = True
    DROPOUT_RATE = 0.5
    PAD_IDX = vocab['<pad>']
    
    # åˆ›å»ºæ¨¡å‹
    model = LSTM(
        vocab_size=vocab_size,
        embedding_dim=EMBEDDING_DIM,
        hidden_dim=HIDDEN_DIM,
        output_dim=OUTPUT_DIM,
        n_layers=N_LAYERS,
        bidirectional=BIDIRECTIONAL,
        dropout_rate=DROPOUT_RATE,
        pad_idx=PAD_IDX
    )
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    model = model.to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {count_parameters(model):,}")
    print(f"\næ¨¡å‹ç»“æ„:\n{model}")
    
    # ==================== è®­ç»ƒè®¾ç½® ====================
    print("\n" + "=" * 60)
    print("4. è®­ç»ƒè®¾ç½®...")
    print("=" * 60)
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    
    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    criterion = criterion.to(device)
    
    # è®­ç»ƒè½®æ•°
    N_EPOCHS = 5
    
    print(f"ä¼˜åŒ–å™¨: Adam (lr=1e-3)")
    print(f"æŸå¤±å‡½æ•°: CrossEntropyLoss")
    print(f"è®­ç»ƒè½®æ•°: {N_EPOCHS}")
    
    # ==================== è®­ç»ƒæ¨¡å‹ ====================
    print("\n" + "=" * 60)
    print("5. å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)
    
    best_valid_loss = float('inf')
    
    for epoch in range(N_EPOCHS):
        start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, train_dataloader, optimizer, criterion, device)
        
        # éªŒè¯ï¼ˆä½¿ç”¨æµ‹è¯•é›†ï¼‰
        valid_loss, valid_acc = evaluate(model, test_dataloader, criterion, device)
        
        end_time = time.time()
        epoch_mins, epoch_secs = epoch_time(start_time, end_time)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), './lstm_best.pt')
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        print(f'\nEpoch: {epoch+1:02} | æ—¶é—´: {epoch_mins}m {epoch_secs}s')
        print(f'\tè®­ç»ƒæŸå¤±: {train_loss:.3f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc*100:.2f}%')
        print(f'\téªŒè¯æŸå¤±: {valid_loss:.3f} | éªŒè¯å‡†ç¡®ç‡: {valid_acc*100:.2f}%')
    
    # ==================== æœ€ç»ˆè¯„ä¼° ====================
    print("\n" + "=" * 60)
    print("6. æœ€ç»ˆè¯„ä¼°...")
    print("=" * 60)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('./lstm_best.pt'))
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_loss, test_acc = evaluate(model, test_dataloader, criterion, device)
    
    print(f'\næµ‹è¯•é›†æŸå¤±: {test_loss:.3f}')
    print(f'æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc*100:.2f}%')
    
    # ==================== é¢„æµ‹ç¤ºä¾‹ ====================
    print("\n" + "=" * 60)
    print("7. é¢„æµ‹ç¤ºä¾‹...")
    print("=" * 60)
    
    # æµ‹è¯•ä¸€äº›å¥å­
    test_sentences = [
        "This film is terrible!",
        "This film is great!",
        "I love this movie, it's amazing!",
        "I hate this movie, it's boring.",
        "The acting was superb and the plot was engaging.",
        "Waste of time and money."
    ]
    
    print()
    for sentence in test_sentences:
        sentiment, confidence = predict_sentiment(model, tokenizer, vocab, sentence, device)
        print(f"å¥å­: {sentence}")
        print(f"é¢„æµ‹: {sentiment} (ç½®ä¿¡åº¦: {confidence:.4f})")
        print()
    
    print("=" * 60)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)


if __name__ == '__main__':
    main()

