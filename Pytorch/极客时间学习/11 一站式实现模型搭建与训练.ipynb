{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22784515-e107-4546-ab79-ac0b96477260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "912c81f2-5420-49c4-b94d-c7a7b595e26c",
   "metadata": {},
   "source": [
    "å…ˆéšæœºç”Ÿæˆè®­ç»ƒé›† X ä¸å¯¹åº”çš„æ ‡ç­¾ Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd58e3a5-4bd4-42f3-81fc-5b58baf3f46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10bc00690>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "w = 2\n",
    "b = 3\n",
    "xlim = [-10, 10]\n",
    "x_train = np.random.randint(low=xlim[0], high=xlim[1], size=30)\n",
    "\n",
    "y_train = [w * x + b + random.randint(0,2) for x in x_train]\n",
    "\n",
    "plt.plot(x_train, y_train, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394ce3d2-dc5d-4202-a269-81520d7fe40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LinearModel(nn.Module):  #å¿…é¡»ç»§æ‰¿ nn.Module ç±»\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    #é€šå¸¸æ¥è¯´è¦æŠŠæœ‰éœ€è¦å­¦ä¹ çš„å‚æ•°çš„å±‚æ”¾åˆ°æ„é€ å‡½æ•°ä¸­\n",
    "    self.weight = nn.Parameter(torch.randn(1))\n",
    "    self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "  def forward(self, input): #forward()å‰å‘ä¼ æ’­ æ˜¯å¿…é¡»é‡å†™çš„æ–¹æ³•\n",
    "    return (input * self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bab19b5-de98-420f-b511-57732b2ae644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()\n",
    "# å®šä¹‰ä¼˜åŒ–å™¨\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)  #ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰å°±æ˜¯ä¸€ä¸ªæ ¹æ®æ¢¯åº¦ä¿¡æ¯è‡ªåŠ¨æ›´æ–°æ¨¡å‹å‚æ•°çš„ç®—æ³•ã€‚\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32) #æŠŠåŸæœ¬çš„ NumPyæˆ–Pythonæ•°æ®ï¼ˆy_trainï¼‰è½¬æˆ PyTorchå¼ é‡ï¼ˆTensorï¼‰ï¼Œ\n",
    "for _ in range(1000):\n",
    "    input = torch.from_numpy(x_train)\n",
    "    output = model(input)\n",
    "    loss = nn.MSELoss()(output, y_train)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff544f2f-25f9-43b1-a1b8-2a6b2f19ee0b",
   "metadata": {},
   "source": [
    "ä¼˜åŒ–å™¨ï¼š\n",
    "\n",
    "| å‚æ•°å                  | å«ä¹‰                 | ä¸¾ä¾‹è§£é‡Š                         |\n",
    "| -------------------- | ------------------ | ---------------------------- |\n",
    "| `model.parameters()` | å‘Šè¯‰ä¼˜åŒ–å™¨è¦æ›´æ–°å“ªäº›å‚æ•°       | å°±æ˜¯ model çš„ `weight` å’Œ `bias` |\n",
    "| `lr=1e-4`            | å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ | ä¸€æ¬¡èµ°å¤šå¤§æ­¥ï¼Œä¸‹å±±æ­¥å¹…                  |\n",
    "| `momentum=0.9`       | åŠ¨é‡                 | è®©æ¢¯åº¦æ›´æ–°æ›´å¹³æ»‘ï¼Œä¸ä¼šå·¦å³ä¹±æ™ƒ              |\n",
    "| `weight_decay=1e-2`  | æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰        | é˜²æ­¢å‚æ•°å¤ªå¤§å¯¼è‡´è¿‡æ‹Ÿåˆ                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a58681-2b01-44a3-8997-d16c711db2b4",
   "metadata": {},
   "source": [
    "ä¼˜åŒ–å™¨çš„å·¥ä½œæµç¨‹:\n",
    "\n",
    "for _ in range(1000):\n",
    "\n",
    "    output = model(input)         # 1ï¸âƒ£ å‰å‘ä¼ æ’­\n",
    "    \n",
    "    loss = nn.MSELoss()(output, y_train)  # 2ï¸âƒ£ è®¡ç®—æŸå¤±\n",
    "    \n",
    "    model.zero_grad()             # 3ï¸âƒ£ æ¸…ç©ºæ—§æ¢¯åº¦\n",
    "    \n",
    "    loss.backward()               # 4ï¸âƒ£ åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦\n",
    "    \n",
    "    optimizer.step()              # 5ï¸âƒ£ ä¼˜åŒ–å™¨æ›´æ–°å‚æ•° âœ…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f907fce1-5249-4bff-99aa-10f1a4622644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([2.0042], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([3.3131], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "#model.named_parameters() ä¼šéå†æ¨¡å‹ä¸­æ‰€æœ‰å¯è®­ç»ƒå‚æ•°ï¼ˆlearnable parametersï¼‰\n",
    "\n",
    "for parameter in model.named_parameters():    \n",
    "  print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b30b1c-da9b-4d52-a884-13e27b2c68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¸€ä¸ªæ–°çš„ç¥ç»ç½‘ç»œå±‚ï¼šè¾“å…¥ â†’ Conv2d(è¾“å…¥é€šé“â†’3) â†’ Conv2d(3â†’è¾“å‡ºé€šé“) â†’ è¾“å‡º\n",
    "\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "  def __init__(self, input_channels, output_channels):\n",
    "    super().__init__()\n",
    "    self.conv1_1 = nn.Conv2d(in_channels=input_channels, out_channels=3, kernel_size=3, padding='same')\n",
    "    self.conv1_2 = nn.Conv2d(in_channels=3, out_channels=output_channels, kernel_size=2, padding='same')\n",
    "    \n",
    "  def forward(self, input):\n",
    "    x = self.conv1_1(input)\n",
    "    x = self.conv1_2(x)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f99558-1d16-48fb-8063-c910fae6ac69",
   "metadata": {},
   "source": [
    "è§£é‡Šï¼š\n",
    "\n",
    "ç¬¬ä¸€å±‚å·ç§¯ï¼š\n",
    "\n",
    "self.conv1_1 = nn.Conv2d(\n",
    "    in_channels=input_channels,   # è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°ï¼ˆå¦‚RGBæ˜¯3ï¼‰\n",
    "    out_channels=3,               # è¾“å‡ºé€šé“æ•°=3\n",
    "    kernel_size=3,                # å·ç§¯æ ¸å¤§å° 3x3\n",
    "    padding='same'                # ä¿æŒè¾“å…¥è¾“å‡ºå°ºå¯¸ç›¸åŒ\n",
    ")\n",
    "\n",
    "\n",
    "ğŸ‘‰ æ„æ€æ˜¯ï¼š\n",
    "ç¬¬ä¸€ä¸ªå·ç§¯å±‚æ¥æ”¶è¾“å…¥é€šé“æ•°ä¸º input_channelsï¼ˆä¾‹å¦‚å›¾åƒ3é€šé“ï¼‰ï¼Œ\n",
    "è¾“å‡º3ä¸ªé€šé“çš„ç‰¹å¾å›¾ã€‚\n",
    "\n",
    "padding='same' è¡¨ç¤ºè‡ªåŠ¨è¡¥é›¶ï¼Œè®©è¾“å‡ºå°ºå¯¸å’Œè¾“å…¥ä¸€æ ·å¤§ã€‚\n",
    "\n",
    "ç¬¬äºŒå±‚å·ç§¯\n",
    "\n",
    "self.conv1_2 = nn.Conv2d(\n",
    "    in_channels=3,                # ä¸Šä¸€å±‚è¾“å‡ºé€šé“=3\n",
    "    out_channels=output_channels, # è¾“å‡ºé€šé“=ä½ è®¾å®šçš„å€¼\n",
    "    kernel_size=2,\n",
    "    padding='same'\n",
    ")\n",
    "\n",
    "\n",
    "ğŸ‘‰ æ„æ€æ˜¯ï¼š\n",
    "ç¬¬äºŒå±‚å·ç§¯ä»ä¸Šä¸€å±‚çš„3ä¸ªé€šé“è¾“å…¥ï¼Œ\n",
    "è¾“å‡º output_channels ä¸ªé€šé“çš„ç‰¹å¾å›¾ï¼ˆæ¯”å¦‚64ã€128ç­‰ï¼‰ã€‚\n",
    "\n",
    "\n",
    "\n",
    "def forward(self, input):\n",
    "    x = self.conv1_1(input)   # ç¬¬ä¸€æ¬¡å·ç§¯\n",
    "    x = self.conv1_2(x)       # ç¬¬äºŒæ¬¡å·ç§¯\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d982d-f67d-40e2-b203-8099a1df7a8b",
   "metadata": {},
   "source": [
    "# ä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcecc70d-cd8c-4179-b44c-7ac9f4346b9e",
   "metadata": {},
   "source": [
    "## åªä¿å­˜è®­ç»ƒå¥½çš„å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca9c976-ef6c-4eed-8656-db1d5912e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './linear_model.pth')   #ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ¨¡å‹çš„ state_dictï¼Œè€Œç¬¬äºŒä¸ªå‚æ•°è¦ä¿å­˜çš„ä½ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad055600-ab08-495e-83b0-73dab1b8096d",
   "metadata": {},
   "source": [
    "ä»£ç ä¸­çš„ state_dict æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œåœ¨æ¨¡å‹è¢«å®šä¹‰ä¹‹åä¼šè‡ªåŠ¨ç”Ÿæˆï¼Œå­˜å‚¨çš„æ˜¯æ¨¡å‹å¯è®­ç»ƒçš„å‚æ•°ã€‚æˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºçº¿æ€§å›å½’æ¨¡å‹çš„ state_dictï¼Œå¦‚ä¸‹æ‰€ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f977ea-a693-4112-935f-275a03abf869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([2.0042])), ('bias', tensor([3.3131]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441f1ef-7a22-47ce-a41e-890b06865945",
   "metadata": {},
   "source": [
    "åŠ è½½æ¨¡å‹çš„æ–¹å¼å¦‚ä¸‹æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9410db9-cc53-495b-8785-b42400be7af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([2.0042], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([3.3131], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# å…ˆå®šä¹‰ç½‘ç»œç»“æ„\n",
    "linear_model = LinearModel()\n",
    "# åŠ è½½ä¿å­˜çš„å‚æ•°\n",
    "linear_model.load_state_dict(torch.load('./linear_model.pth'))\n",
    "linear_model.eval()\n",
    "for parameter in linear_model.named_parameters():\n",
    "  print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491fac66-4040-4922-8116-e00bc335e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "# ä½¿ç”¨æ–°å†™æ³•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "alexnet = models.alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "# æˆ–è€…ä½¿ç”¨é»˜è®¤æƒé‡\n",
    "# alexnet = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46edc451-93ff-4763-8cc9-3229c904cb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(264)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "im = Image.open('dog.webp')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "input_tensor = transform(im).unsqueeze(0)\n",
    "alexnet(input_tensor).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e8eda7-5a79-4e1f-811b-64930171847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "papillon\n"
     ]
    }
   ],
   "source": [
    "# 157ç±»æ˜¯ä»€ä¹ˆ\n",
    "\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "categories = weights.meta[\"categories\"]\n",
    "\n",
    "print(categories[157])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f0dbc1e-d977-4220-841d-ffbb83fab4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bc7e3-14f1-4e3d-b7f3-b5ded408a19c",
   "metadata": {},
   "source": [
    "# å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4beef-c1ab-4e09-8f3a-3f99f8f9c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1/3 | train_loss=1.0278 acc=0.6336 | val_loss=0.6252 acc=0.7785\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PyTorch å¾®è°ƒï¼ˆFine-tuningï¼‰AlexNet ç®€å•ç¤ºä¾‹ï¼ˆå« Apple MPS åŠ é€Ÿï¼‰\n",
    "--------------------------------------------------\n",
    "åŠŸèƒ½ï¼š\n",
    "1. ä½¿ç”¨ torchvision è‡ªå¸¦çš„ AlexNet é¢„è®­ç»ƒæ¨¡å‹ï¼›\n",
    "2. å†»ç»“ç‰¹å¾æå–å±‚ï¼ˆfeaturesï¼‰ï¼Œåªè®­ç»ƒåˆ†ç±»å™¨å±‚ï¼›\n",
    "3. åœ¨ CIFAR-10 æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼›\n",
    "4. ä¼˜å…ˆä½¿ç”¨ Apple MPSï¼ˆMetalï¼‰ï¼Œå…¶æ¬¡ CUDAï¼Œæœ€å CPUï¼›\n",
    "5. åŠç²¾åº¦ autocastï¼ˆMPS/CUDAï¼‰ï¼Œè‡ªåŠ¨ä¸‹è½½æ•°æ®ä¸æƒé‡ï¼›\n",
    "6. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ã€‚\n",
    "\n",
    "è¿è¡Œç¯å¢ƒï¼ˆå»ºè®®ï¼‰ï¼š\n",
    "pip install torch torchvision\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# ============================================================\n",
    "# 1. ç¯å¢ƒè®¾ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ MPSï¼Œå…¶æ¬¡ CUDAï¼‰\n",
    "# ============================================================\n",
    "use_mps = torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "if use_mps:\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ï¼ˆå¯é€‰ï¼‰macOS ä¸Š DataLoader å»ºè®®ä½¿ç”¨ spawn\n",
    "try:\n",
    "    import torch.multiprocessing as mp\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "# æ˜¯å¦å¯ç”¨ channels_last åŠ é€Ÿï¼ˆå¯¹ CNN æœ‰æ—¶æ›´å¿«ï¼›è‹¥é‡åˆ°ä¸æ”¯æŒçš„ç®—å­å¯è®¾ä¸º Falseï¼‰\n",
    "USE_CHANNELS_LAST = True\n",
    "\n",
    "# ============================================================\n",
    "# 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶å†»ç»“ç‰¹å¾æå–å±‚\n",
    "# ============================================================\n",
    "# ä»å®˜æ–¹åŠ è½½ ImageNet é¢„è®­ç»ƒçš„ AlexNet æƒé‡\n",
    "model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# å†»ç»“ç‰¹å¾æå–å±‚çš„å‚æ•°ï¼ˆå³å·ç§¯éƒ¨åˆ† featuresï¼‰\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚ï¼Œä½¿è¾“å‡ºç±»åˆ«æ•° = CIFAR10 çš„ 10 ç±»\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 10)\n",
    "\n",
    "# æŠŠæ¨¡å‹æ”¾åˆ°ç›¸åº”è®¾å¤‡\n",
    "model = model.to(device)\n",
    "\n",
    "# å¯é€‰ï¼šchannels_last\n",
    "if USE_CHANNELS_LAST and device != \"cpu\":\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "# ============================================================\n",
    "# 3. æ•°æ®å‡†å¤‡ï¼ˆä½¿ç”¨ AlexNet æƒé‡è‡ªå¸¦çš„æ ‡å‡†åŒ–å‚æ•°ï¼‰\n",
    "# ============================================================\n",
    "# ç”±äº AlexNet æ˜¯åœ¨ ImageNet ä¸Šè®­ç»ƒçš„ï¼Œè¾“å…¥å¤§å°åº”ä¸º 224Ã—224\n",
    "# transforms åŒ…æ‹¬ Resizeã€Cropã€Normalize ç­‰ï¼›æƒé‡è‡ªå¸¦çš„ transforms å« ToTensor+Normalize\n",
    "weight_tf = AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(256),           # è°ƒæ•´çŸ­è¾¹åˆ° 256\n",
    "    transforms.CenterCrop(224),       # ä¸­å¿ƒè£å‰ªåˆ° 224Ã—224\n",
    "    transforms.RandAugment(),         # éšæœºæ•°æ®å¢å¼ºï¼ˆå¯åˆ é™¤ä»¥è¿›ä¸€æ­¥æé€Ÿï¼‰\n",
    "    weight_tf,                        # åŒ…å« ToTensor + Normalize\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    weight_tf,\n",
    "])\n",
    "\n",
    "# è‡ªåŠ¨ä¸‹è½½ CIFAR10 æ•°æ®é›†\n",
    "data_root = \"./data\"\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "train_ds = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_tf)\n",
    "test_ds  = datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_tf)\n",
    "\n",
    "# åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆpin_memory å¯¹ CUDA æœ‰æ•ˆï¼ŒMPS/CPU ä¸‹å…³é—­ï¼‰\n",
    "pin = True if device == \"cuda\" else False\n",
    "num_workers = 2  # å¯æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´ï¼ˆ2~4 è¾ƒåˆé€‚ï¼‰\n",
    "batch_size_train = 64   # M ç³»åˆ—ç»Ÿä¸€å†…å­˜æœ‰é™ï¼Œå¦‚ OOM å¯é™åˆ° 32\n",
    "batch_size_test  = 128  # å¯é…Œæƒ…è°ƒæ•´\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size_train, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=pin)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size_test, shuffle=False,\n",
    "                          num_workers=num_workers, pin_memory=pin)\n",
    "\n",
    "# ============================================================\n",
    "# 4. å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n",
    "# ============================================================\n",
    "# åªä¼˜åŒ– requires_grad=True çš„å‚æ•°ï¼ˆåˆ†ç±»å™¨å±‚ï¼‰\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                              lr=2e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ============================================================\n",
    "# 5. AMP / autocast ä¸Šä¸‹æ–‡ï¼ˆMPS/CUDA åŠç²¾åº¦ï¼‰\n",
    "# ============================================================\n",
    "if device == \"mps\":\n",
    "    autocast_ctx = torch.autocast(device_type=\"mps\", dtype=torch.float16)\n",
    "    scaler = None  # MPS æ— éœ€ GradScaler\n",
    "elif device == \"cuda\":\n",
    "    from torch.cuda.amp import autocast as cuda_autocast, GradScaler\n",
    "    autocast_ctx = cuda_autocast(dtype=torch.float16)\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    autocast_ctx = nullcontext()\n",
    "    scaler = None\n",
    "\n",
    "# ============================================================\n",
    "# 5+. è¯„ä¼°å‡½æ•°ï¼ˆè®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡ï¼‰\n",
    "# ============================================================\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # å¯é€‰ channels_lastï¼šä»…åœ¨é CPU ä¸”å¯ç”¨æ—¶è½¬æ¢\n",
    "            if USE_CHANNELS_LAST and device != \"cpu\":\n",
    "                x = x.to(device, memory_format=torch.channels_last, non_blocking=pin)\n",
    "            else:\n",
    "                x = x.to(device, non_blocking=pin)\n",
    "            y = y.to(device, non_blocking=pin)\n",
    "\n",
    "            with autocast_ctx:\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            total_loss += float(loss.item()) * x.size(0)\n",
    "            total_acc  += (logits.argmax(1) == y).float().sum().item()\n",
    "            n += x.size(0)\n",
    "    return total_loss / n, total_acc / n\n",
    "\n",
    "# ============================================================\n",
    "# 6. è®­ç»ƒå¾ªç¯\n",
    "# ============================================================\n",
    "epochs = 3  # ä¸ºäº†å¿«é€Ÿæ¼”ç¤ºåªè·‘ 3 ä¸ª epochï¼Œå¯è‡ªè¡Œè°ƒå¤§\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss, running_acc, total = 0.0, 0.0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # å¯é€‰ channels_lastï¼šä»…åœ¨é CPU ä¸”å¯ç”¨æ—¶è½¬æ¢\n",
    "        if USE_CHANNELS_LAST and device != \"cpu\":\n",
    "            images = images.to(device, memory_format=torch.channels_last, non_blocking=pin)\n",
    "        else:\n",
    "            images = images.to(device, non_blocking=pin)\n",
    "        labels = labels.to(device, non_blocking=pin)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # å‰å‘ + è®¡ç®— lossï¼ˆåŠç²¾åº¦ï¼‰\n",
    "        with autocast_ctx:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°ï¼ˆCUDA ç”¨ GradScalerï¼ŒMPS/CPU ç›´æ¥ backwardï¼‰\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ç»Ÿè®¡è®­ç»ƒå‡†ç¡®ç‡ä¸æŸå¤±\n",
    "        running_loss += float(loss.item()) * images.size(0)\n",
    "        running_acc  += (outputs.argmax(1) == labels).float().sum().item()\n",
    "        total += images.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc  = running_acc / total\n",
    "    val_loss, val_acc = evaluate(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{epochs} | \"\n",
    "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. å¯é€‰ï¼šè§£å†»éƒ¨åˆ†å·ç§¯å±‚è¿›ä¸€æ­¥ç²¾è°ƒ\n",
    "# ============================================================\n",
    "# å¸¸è§ç­–ç•¥ï¼šå…ˆå†»ç»“ç‰¹å¾å±‚ï¼Œç¨³å®šåˆ†ç±»å™¨ï¼›å†è§£å†»åå‡ å±‚ç»§ç»­è®­ç»ƒï¼ˆæ•ˆæœæ›´å¥½ï¼‰\n",
    "# for param in model.features[6:].parameters():\n",
    "#     param.requires_grad = True\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "# å†è·‘è‹¥å¹² epoch ...\n",
    "\n",
    "# ============================================================\n",
    "# 8. ä¿å­˜æ¨¡å‹æƒé‡\n",
    "# ============================================================\n",
    "save_path = \"alexnet_cifar10_finetuned.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acd6f5-86fe-41bf-ba3f-c9eb28579d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
