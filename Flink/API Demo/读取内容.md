# 读取内容

## 从集合中读取

	package com.wzy
	
	import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
	import org.apache.flink.api.scala._
	
	case class  SensorReading(id:String,timestamp:Long,temperature:Double)
	
	
	object SourceTest {
	  def main(args: Array[String]): Unit = {
	
	
	    val env = StreamExecutionEnvironment.getExecutionEnvironment
	
	    //从集合中读取
	    val dataList = List(
	      SensorReading("sensor_1",1547718199,35.8) ,
	      SensorReading("sensor_2",1547718111,31.1),
	      SensorReading("sensor_3",1547718132,34.3),
	      SensorReading("sensor_4",1547718982,38.1),
	      SensorReading("sensor_5",1547718231,37.9)
	    )
	
	    val stream1 = env.fromCollection(dataList)
	    stream1.print()
	
	    env.execute("source test")
	  }
	
	}
	
输出结果：
	
	
	2> SensorReading(sensor_2,1547718111,31.1)
	5> SensorReading(sensor_5,1547718231,37.9)
	3> SensorReading(sensor_3,1547718132,34.3)
	4> SensorReading(sensor_4,1547718982,38.1)
	1> SensorReading(sensor_1,1547718199,35.8)


## 从文件中读取

	package com.wzy
	
	import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
	import org.apache.flink.api.scala._
	
	case class  SensorReading(id:String,timestamp:Long,temperature:Double)
	
	
	object SourceTest {
	  def main(args: Array[String]): Unit = {
	
	
	    val env = StreamExecutionEnvironment.getExecutionEnvironment
	
	    //从文件中读取
	    val inputPath ="/Users/zheyiwang/IdeaProjects/flink_demo/src/main/scala/com/wzy/sensorReading.txt";
	    val stream2 =env.readTextFile(inputPath)
	    stream2.print()
	
	    env.execute("source test")
	  }
	
	}

输出内容：

	8> sensor_1,1547718199,35.8
	10> sensor_2,1547718111,31.1
	2> sensor_4,1547718982,38.1
	5> sensor_5,1547718231,37.9
	12> sensor_3,1547718132,34.3
	

## 从kafka读取

### 增加flink-connector-kafka

	<?xml version="1.0" encoding="UTF-8"?>
	<project xmlns="http://maven.apache.org/POM/4.0.0"
	         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	    <modelVersion>4.0.0</modelVersion>
	
	    <groupId>org.example</groupId>
	    <artifactId>flink_demo</artifactId>
	    <version>1.0-SNAPSHOT</version>
	
	    <dependencies>
	        <dependency>
	            <groupId>org.apache.flink</groupId>
	            <artifactId>flink-core</artifactId>
	            <version>1.11.2</version>
	        </dependency>
	        <dependency>
	            <groupId>org.apache.flink</groupId>
	            <artifactId>flink-scala_2.12</artifactId>
	            <version>1.11.2</version>
	        </dependency>
	        <dependency>
	            <groupId>org.apache.flink</groupId>
	            <artifactId>flink-streaming-scala_2.12</artifactId>
	            <version>1.11.2</version>
	        </dependency>
	        <dependency>
	            <groupId>org.apache.flink</groupId>
	            <artifactId>flink-clients_2.12</artifactId>
	            <version>1.11.2</version>
	        </dependency>
	        <dependency>
	            <groupId>org.apache.flink</groupId>
	            <artifactId>flink-connector-kafka_2.12</artifactId>
	            <version>1.11.2</version>
	        </dependency>
	
	
	    </dependencies>
	</project>
	
	
### 代码

	package com.wzy
	
	import java.util.Properties
	
	import org.apache.flink.api.common.serialization.SimpleStringSchema
	import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
	import org.apache.flink.api.scala._
	import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
	
	
	object SourceTest {
	  def main(args: Array[String]): Unit = {
	
	    val env = StreamExecutionEnvironment.getExecutionEnvironment
	
	    val properties = new Properties()
	    properties.setProperty("bootstrap.servers", "http://47.112.142.231:9092")
	    properties.setProperty("group.id", "consumer-group")
	    properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
	    properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
	    properties.setProperty("auto.offset.reset", "latest")
	
	    val inputStream = env.addSource(new FlinkKafkaConsumer[String]("sensor", new SimpleStringSchema(), properties))
	    inputStream.print()
	
	    env.execute("11")
	  }
	
	}

	
### 往Kafka中添加topiic

	bash-4.4# ./kafka-console-producer.sh --broker-list localhost:9092 --topic sensor
	>111
	>test
	
### flink收到消息
	9> 111
	7> test