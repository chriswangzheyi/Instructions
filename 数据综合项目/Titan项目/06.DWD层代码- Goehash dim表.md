#06 DWD层代码- Goehash dim表

## 环境准备

在titan3上安装spark。操作步骤建《spark环境搭建》

### 版本信息

* spark-3.1.3-bin-hadoop3.2.tgz* scala-2.12.17.tgz


### 规划

master: titan3
worker：titan2，titan3, titan4

### 建hive库

	create database dim;
	
	
	

## dw_etl

维度建模

![](Images/9.png)

### ReferenceGpsDic

	package com.wzy
	
	import ch.hsr.geohash.GeoHash
	import ch.hsr.geohash.GeoHash
	import org.apache.spark.sql.SparkSession
	
	import java.util.Properties
	
	object ReferenceGpsDic {
	
	  def main(args: Array[String]): Unit = {
	
	    System.setProperty("HADOOP_USER_NAME", "root")
	
	    val spark = SparkSession.builder()
	      .appName("gpsToGeoHash")
	      .config("spark.sql.shuffle.partions","1")
	      .master("spark://192.168.2.113:7077")
	      .enableHiveSupport()
	      //本地测试运行需要加这一句话，部署在生产环境则删除
	      .config("spark.jars","/Users/zheyiwang/IdeaProjects/dw_etl/target/dw_etl-1.0-SNAPSHOT-jar-with-dependencies.jar")
	      .getOrCreate()
	
	    // 建立mysql连接
	    val pros = new Properties();
	    pros.setProperty("user","root");
	    pros.setProperty("password","1qa2ws#ED");
	    val df = spark.read.jdbc("jdbc:mysql://192.168.2.111:3306/realtimedw","t_md_areas",pros);
	
	    df.createTempView("df");
	
	    val gpsToGoeHash = (lng:Double,lat:Double)=>{
	      GeoHash.geoHashStringWithCharacterPrecision(lat,lng,6)
	    }
	
	    spark.udf.register("geohash",gpsToGoeHash)
	
	    //选取里面的3、4级行政单位并找到所属的上级行政单位，把GPS转换为geohash编码
	    val tmp=spark.sql(
	      """
	        |
	        |select
	        | l1.areaname as province,
	        | l2.areaname as city,
	        | l3.areaname as district,
	        | geohash(l4.bd09_lng,l4.bd09_lat) as geohash
	        |
	        |from df l4 join df l3 on l4.parentid=l3.id and l4.level=4
	        |           join df l2 on l3.parentid=l2.id
	        |           join df l1 on l2.parentid=l1.id
	        |
	        |union all
	        |
	        |select
	        | l1.areaname as province,
	        | l2.areaname as city,
	        | l3.areaname as district,
	        | geohash(l3.bd09_lng,l3.bd09_lat) as geohash
	        |
	        | from df l3 join df l2 on l3.parentid=l2.id and l3.level=3
	        |            join df l1 on l2.parentid=l1.id
	        |
	        |""".stripMargin);
	
	    tmp.show(100,false);
	
	    //结果存为hive
	    tmp.write.saveAsTable("dim.geo_area")
	
	    spark.close();
	
	  }
	
	}

### pom.xml


	<?xml version="1.0" encoding="UTF-8"?>
	<project xmlns="http://maven.apache.org/POM/4.0.0"
	         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	    <modelVersion>4.0.0</modelVersion>
	
	    <groupId>org.example</groupId>
	    <artifactId>dw_etl</artifactId>
	    <version>1.0-SNAPSHOT</version>
	
	    <properties>
	        <maven.compiler.source>8</maven.compiler.source>
	        <maven.compiler.target>8</maven.compiler.target>
	    </properties>
	
	
	    <dependencies>
	
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-core_2.12</artifactId>
	            <version>3.1.3</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.scala-lang</groupId>
	            <artifactId>scala-library</artifactId>
	            <version>2.12.10</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-sql_2.12</artifactId>
	            <version>3.1.3</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-hive_2.12</artifactId>
	            <version>3.1.3</version>
	        </dependency>
	
	        <dependency>
	            <groupId>mysql</groupId>
	            <artifactId>mysql-connector-java</artifactId>
	            <version>8.0.17</version>
	        </dependency>
	
	        <dependency>
	            <groupId>ch.hsr</groupId>
	            <artifactId>geohash</artifactId>
	            <version>1.3.0</version>
	        </dependency>
	
	    </dependencies>
	
	
	    <repositories>
	        <repository>
	            <id>nexus-aliyun</id>
	            <name>Nexus aliyun</name>
	            <layout>default</layout>
	            <url>http://maven.aliyun.com/nexus/content/groups/public</url>
	            <snapshots>
	                <enabled>false</enabled>
	                <updatePolicy>never</updatePolicy>
	            </snapshots>
	            <releases>
	                <enabled>true</enabled>
	                <updatePolicy>never</updatePolicy>
	            </releases>
	        </repository>
	
	    </repositories>
	
	    <pluginRepositories>
	        <pluginRepository>
	            <id>ali-plugin</id>
	            <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
	            <snapshots>
	                <enabled>false</enabled>
	                <updatePolicy>never</updatePolicy>
	            </snapshots>
	            <releases>
	                <enabled>true</enabled>
	                <updatePolicy>never</updatePolicy>
	            </releases>
	        </pluginRepository>
	    </pluginRepositories>
	
	    <build>
	        <plugins>
	            <!-- 指定编译java的插件 -->
	            <plugin>
	                <groupId>org.apache.maven.plugins</groupId>
	                <artifactId>maven-compiler-plugin</artifactId>
	                <version>3.5.1</version>
	                <configuration>
	                    <source>1.8</source>
	                    <target>1.8</target>
	                </configuration>
	            </plugin>
	
	            <!-- 指定编译scala的插件 -->
	            <plugin>
	                <groupId>net.alchim31.maven</groupId>
	                <artifactId>scala-maven-plugin</artifactId>
	                <version>3.2.2</version>
	                <executions>
	                    <execution>
	                        <goals>
	                            <goal>compile</goal>
	                            <goal>testCompile</goal>
	                        </goals>
	                        <configuration>
	                            <args>
	                                <arg>-dependencyfile</arg>
	                                <arg>${project.build.directory}/.scala_dependencies</arg>
	                            </args>
	                        </configuration>
	                    </execution>
	                </executions>
	            </plugin>
	
	
	            <!--  把依赖jar中的用到的类，提取到自己的jar中 -->
	            <plugin>
	                <groupId>org.apache.maven.plugins</groupId>
	                <artifactId>maven-assembly-plugin</artifactId>
	                <version>2.6</version>
	                <configuration>
	                    <archive>
	                        <manifest>
	                            <mainClass></mainClass>
	                        </manifest>
	                    </archive>
	                    <descriptorRefs>
	                        <descriptorRef>jar-with-dependencies</descriptorRef>
	                    </descriptorRefs>
	                </configuration>
	                <!--下面是为了使用 mvn package命令，如果不加则使用mvn assembly-->
	                <executions>
	                    <execution>
	                        <id>make-assemble</id>
	                        <phase>package</phase>
	                        <goals>
	                            <goal>single</goal>
	                        </goals>
	                    </execution>
	                </executions>
	            </plugin>
	        </plugins>
	    </build>
	
	</project>

### hive-site.xml

	<configuration>
	    <property>
	        <name>hive.metastore.uris</name>
	        <value>thrift://192.168.2.114:9083</value>
	    </property>
	
	    <property>
	        <name>hive.metastore.warehouse.dir</name>
	        <value>/user/hive/warehouse</value>
	        <description>location of default database for the warehouse</description>
	    </property>
	</configuration>