# 09. DWD ETL 开发流程


## 前提条件




## 结构



## 文件

### ApplogBean
	package com.wzy.beans
	
	case class ApplogBean(
	
	                       account: String,
	                       appid: String,
	                       appversion: String,
	                       carrier: String,
	                       deviceid: String,
	                       devicetype: String,
	                       eventid: String,
	                       ip: String,
	                       latitude: Double,
	                       longitude: Double,
	                       nettype: String,
	                       osname: String,
	                       osversion: String,
	                       properties: Map[String,String],
	                       releasechannel:String,
	                       resolution:String,
	                       sessionid: String,
	                       timestamp: Long,
	                       dt:String,
	                       //新增一个切分之后的session id
	                       var splitSessionId: String="",
	                       //维度退化后，新增省市区
	                       var province:String="",
	                       var city:String="",
	                       var district:String=""
	                     )



### Applog_ODS_TODWD_Integrate

	package com.wzy
	
	import com.wzy.beans.ApplogBean
	import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}
	import ch.hsr.geohash.GeoHash
	
	//维度退化
	//集成地理位置，GUID
	object Applog_ODS_TODWD_Integrate {
	
	  def main(args: Array[String]): Unit = {
	
	    val spark = SparkSession.builder()
	      .appName("dimension degeneration and GUID process from ods to dwd")
	      .master("spark://192.168.2.113:7077")
	      .enableHiveSupport()
	      //本地测试运行需要加这一句话，部署在生产环境则删除
	      .config("spark.jars","/Users/zheyiwang/IdeaProjects/dw_dwd_etl/target/dw_etl-1.0-SNAPSHOT-jar-with-dependencies.jar")
	      .getOrCreate()
	      import spark.implicits._
	
	
	    //读取Applog_ODS_TODWD_Process 保存的文件
	    val ds = spark.read.parquet("hdfs://192.168.2.111:8020/tmp/split/20200917/").as[ApplogBean]
	
	    val geoDataFrame:DataFrame = spark.read.table("dim.geo_area")
	
	    val geoMap = geoDataFrame.rdd.map({
	      case Row(province:String, city:String,district:String,geohash:String) => Some(geohash,(province,city,district))
	      case _ => None
	    }).filter(_.isDefined)
	      .map(_.get)
	      .collectAsMap()
	
	    //广播
	    val bc1= spark.sparkContext.broadcast(geoMap)
	
	    //集成地理位置信息（如果有geohash值，查询市区）
	    val areaIntegrated: Dataset[ApplogBean] =ds.rdd.mapPartitions(iter =>{
	
	      val geoDict = bc1.value
	
	      iter.map( bean =>{
	
	        var province = "未知省"
	        var city = "未知市"
	        var district = "未知区"
	
	        val lng = bean.longitude
	        val lat = bean.latitude
	
	        if(lng !=null && lat !=null) {
	          val geoCode = GeoHash.geoHashStringWithCharacterPrecision(lat, lng, 6)
	
	          val area: Option[(String, String, String)] = geoDict.get(geoCode)
	
	          if (area.isDefined) {
	            province = area.get._1
	            city = area.get._2
	            district = area.get._3
	          }
	        }
	
	        bean.province = province
	        bean.city = city
	        bean.district = district
	
	        bean
	      })
	    }).toDS()
	
	
	    areaIntegrated.show(10,false)
	
	  }
	
	}


### Applog_ODS_TODWD_Process

	package com.wzy
	
	import com.wzy.beans.ApplogBean
	import com.wzy.utils.Row2Bean
	import org.apache.commons.lang.time.DateFormatUtils
	import org.apache.spark.sql.SparkSession
	
	import java.util.{TimeZone, UUID}
	
	
	// 清洗过滤
	// 格式转换
	// 数据规范化
	object Applog_ODS_TODWD_Process {
	  def main(args: Array[String]): Unit = {
	
	  val spark = SparkSession.builder()
	    .appName("app log process from ods to dwd")
	    .master("spark://192.168.2.113:7077")
	    .enableHiveSupport()
	    //本地测试运行需要加这一句话，部署在生产环境则删除
	    .config("spark.jars","/Users/zheyiwang/IdeaProjects/dw_dwd_etl/target/dw_etl-1.0-SNAPSHOT-jar-with-dependencies.jar")
	    .getOrCreate()
	    import spark.implicits._
	
	
	
	    //读ods表
	    val odsTable = spark.read.table("ods.mall_app_log_dts").where("dt='2022-09-17'")
	
	    //注册临时视图
	    odsTable.createTempView("dtl")
	
	    //时间转换UDF
	    val toTimestamp = (ts:Long) =>{
	      DateFormatUtils.format(ts, "yyyy-MM-dd HH:mm:ss.SSS", TimeZone.getTimeZone("GMT+8"))
	    }
	    spark.udf.register("toTimestamp",toTimestamp)
	
	
	    //过滤空字段，限定时间
	    val cleaned = spark.sql(
	      """
	        |
	        |select
	        |
	        |*
	        |
	        |from dtl
	        |
	        |where
	        | deviceid is not null and
	        | properties is not null and
	        | eventid is not null and
	        | sessionid is not null and
	        | toTimestamp(`timestamp`) >='2022-09-01 00:00:00.000' and
	        | toTimestamp(`timestamp`) <= '2022-12-01 00:00:00.000'
	        |
	        |""".stripMargin)
	
	    //省略特征工程
	
	
	    //省略数据规范化
	
	
	    //会话切割
	   val sessionSplit = cleaned.rdd
	      .map(Row2Bean
	      .applogRow2Bean)
	      .filter(_.isDefined)
	      .map(_.get)
	      .groupBy(_.sessionid) //原始id分组
	      .flatMap(tp=>{
	        val actions:List[ApplogBean] = tp._2.toList.sortBy(_.timestamp)
	
	        var newSessionId = UUID.randomUUID().toString
	
	        for( i <- 0 until actions.size){
	          actions(i).splitSessionId = newSessionId
	          //如果时间相差30分钟，就重新生成一个新sessionid
	          if( i < actions.size-1 && (actions(i+1).timestamp - actions(i).timestamp>30*60*1000)){
	            newSessionId = UUID.randomUUID().toString
	          }
	        }
	        actions
	      }).toDF()
	
	    sessionSplit.write
	      .option("encoding", "UTF-8")
	      .option("charset", "UTF-8")
	      .parquet("hdfs://192.168.2.111:9000/tmp/split/20201006")
	
	    spark.close()
	
	  }
	}


### Row2Bean

	package com.wzy.utils
	
	import com.wzy.beans.ApplogBean
	import org.apache.spark.sql.Row
	
	object Row2Bean {
	
	    def  applogRow2Bean(row:Row)={
	
	      row match {
	        case Row(
	          account: String,
	          appid: String,
	          appversion: String,
	          carrier: String,
	          deviceid: String,
	          devicetype: String,
	          eventid: String,
	          ip: String,
	          latitude: Double,
	          longitude: Double,
	          nettype: String,
	          osname: String,
	          osversion: String,
	          properties:Map[String,String],
	          releasechannel:String,
	          resolution:String,
	          sessionid: String,
	          timestamp: Long,
	          dt:String
	        )=>Some(
	          ApplogBean(
	          account,
	          appid,
	          appversion,
	          carrier,
	          deviceid,
	          devicetype,
	          eventid,
	          ip,
	          latitude,
	          longitude,
	          nettype,
	          osname,
	          osversion,
	          properties,
	          releasechannel,
	          resolution,
	          sessionid,
	          timestamp,
	          dt
	        ))
	
	        //必需写，否则如果出现空值，整个过程就会报错
	        case _ => None
	      }
	    }
	
	}


### hive-site.xml

		<configuration>
		    <property>
		        <name>hive.metastore.uris</name>
		        <value>thrift://192.168.2.114:9083</value>
		    </property>
		
		    <property>
		        <name>hive.metastore.warehouse.dir</name>
		        <value>/user/hive/warehouse</value>
		        <description>location of default database for the warehouse</description>
		    </property>
		
		</configuration>
	
	
### pom.xml
	
	<?xml version="1.0" encoding="UTF-8"?>
	<project xmlns="http://maven.apache.org/POM/4.0.0"
	         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	    <modelVersion>4.0.0</modelVersion>
	
	    <groupId>org.example</groupId>
	    <artifactId>dw_etl</artifactId>
	    <version>1.0-SNAPSHOT</version>
	
	    <properties>
	        <maven.compiler.source>8</maven.compiler.source>
	        <maven.compiler.target>8</maven.compiler.target>
	    </properties>
	
	
	    <dependencies>
	
	        <dependency>
	            <groupId>org.apache.hadoop</groupId>
	            <artifactId>hadoop-client</artifactId>
	            <version>3.3.4</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-core_2.12</artifactId>
	            <version>3.1.3</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.scala-lang</groupId>
	            <artifactId>scala-library</artifactId>
	            <version>2.12.10</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-sql_2.12</artifactId>
	            <version>3.1.3</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-hive_2.12</artifactId>
	            <version>3.1.3</version>
	        </dependency>
	
	        <dependency>
	            <groupId>mysql</groupId>
	            <artifactId>mysql-connector-java</artifactId>
	            <version>8.0.17</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.hive.hcatalog</groupId>
	            <artifactId>hive-hcatalog-core</artifactId>
	            <version>3.1.3</version>
	        </dependency>
	
	        <dependency>
	            <groupId>ch.hsr</groupId>
	            <artifactId>geohash</artifactId>
	            <version>1.3.0</version>
	        </dependency>
	
	    </dependencies>
	
	
	    <repositories>
	        <repository>
	            <id>nexus-aliyun</id>
	            <name>Nexus aliyun</name>
	            <layout>default</layout>
	            <url>http://maven.aliyun.com/nexus/content/groups/public</url>
	            <snapshots>
	                <enabled>false</enabled>
	                <updatePolicy>never</updatePolicy>
	            </snapshots>
	            <releases>
	                <enabled>true</enabled>
	                <updatePolicy>never</updatePolicy>
	            </releases>
	        </repository>
	
	    </repositories>
	
	    <pluginRepositories>
	        <pluginRepository>
	            <id>ali-plugin</id>
	            <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
	            <snapshots>
	                <enabled>false</enabled>
	                <updatePolicy>never</updatePolicy>
	            </snapshots>
	            <releases>
	                <enabled>true</enabled>
	                <updatePolicy>never</updatePolicy>
	            </releases>
	        </pluginRepository>
	    </pluginRepositories>
	
	    <build>
	        <plugins>
	            <!-- 指定编译java的插件 -->
	            <plugin>
	                <groupId>org.apache.maven.plugins</groupId>
	                <artifactId>maven-compiler-plugin</artifactId>
	                <version>3.5.1</version>
	                <configuration>
	                    <source>1.8</source>
	                    <target>1.8</target>
	                </configuration>
	            </plugin>
	
	            <!-- 指定编译scala的插件 -->
	            <plugin>
	                <groupId>net.alchim31.maven</groupId>
	                <artifactId>scala-maven-plugin</artifactId>
	                <version>3.2.2</version>
	                <executions>
	                    <execution>
	                        <goals>
	                            <goal>compile</goal>
	                            <goal>testCompile</goal>
	                        </goals>
	                        <configuration>
	                            <args>
	                                <arg>-dependencyfile</arg>
	                                <arg>${project.build.directory}/.scala_dependencies</arg>
	                            </args>
	                        </configuration>
	                    </execution>
	                </executions>
	            </plugin>
	
	
	            <!--  把依赖jar中的用到的类，提取到自己的jar中 -->
	            <plugin>
	                <groupId>org.apache.maven.plugins</groupId>
	                <artifactId>maven-assembly-plugin</artifactId>
	                <version>2.6</version>
	                <configuration>
	                    <archive>
	                        <manifest>
	                            <mainClass></mainClass>
	                        </manifest>
	                    </archive>
	                    <descriptorRefs>
	                        <descriptorRef>jar-with-dependencies</descriptorRef>
	                    </descriptorRefs>
	                </configuration>
	                <!--下面是为了使用 mvn package命令，如果不加则使用mvn assembly-->
	                <executions>
	                    <execution>
	                        <id>make-assemble</id>
	                        <phase>package</phase>
	                        <goals>
	                            <goal>single</goal>
	                        </goals>
	                    </execution>
	                </executions>
	            </plugin>
	        </plugins>
	    </build>
	
	</project>



## 报错解决

### 错误1

	Caused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance 
	
解决方法：

用命令行执行

	mvn clean package


### 错误2

 io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: zheyidembp/192.168.2.113:51466
 
 解决方法：
 
	[zheyi@titan2 bin]$  df -h
	Filesystem                    Size  Used Avail Use% Mounted on
	devtmpfs                      3.9G     0  3.9G   0% /dev
	tmpfs                         3.9G     0  3.9G   0% /dev/shm
	tmpfs                         3.9G  9.3M  3.9G   1% /run
	tmpfs                         3.9G     0  3.9G   0% /sys/fs/cgroup
	/dev/mapper/VolGroup-lv_root   50G  7.5G   40G  17% /
	/dev/sda1                     477M  208M  241M  47% /boot
	/dev/mapper/VolGroup-lv_home   12G   11G   57M 100% /home
	iCloud                        466G  338G  129G  73% /media/psf/iCloud
	T7                            932G  238G  695G  26% /media/psf/T7
	tmpfs                         782M     0  782M   0% /run/user/1000
	
发现有空间100%


  