#02. 日志采集



## 安装Flume

登录三台服务器

* titan1: 192.168.2.111  （Main）
* titan2: 192.168.2.112 （HA）
* titan3: 192.168.2.113 （HA）

两级传输。一般用在服务器隔离的情况下。为了信息解耦和安全性。


### 解压

	sudo mkdir -p /opt/apps
	
	#在三台服务器上上传flume文件
	sudo tar -zxvf apache-flume-1.10.1-bin.tar.gz
	mv apache-flume-1.10.1-bin flume
	

## 安装 HDFS

在4台服务器上安装HDFS。

* TItan1：Master
* TItan2-4： Slave

vi /etc/hosts

	192.168.2.111 titan1
	192.168.2.112 titan2
	192.168.2.113 titan3
	192.168.2.114 titan4



确保环境变量已配置

![](Images/4.png)

## 安装 ZK

![](Images/5.png)
  
## 架构

###总体架构

![](Images/1.svg)

### 细节

![](Images/1.png)

## 文件配置

### 上游配置文件

	cd /home/zheyi/flume/conf

在titan1中创建 

	vim flume_level_1.properties

插入
	
	a1.sources = r1
	a1.channels = c1
	a1.sinks = k1 k2
	
	a1.sources.r1.channels = c1
	a1.sources.r1.type = TAILDIR
	a1.sources.r1.filegroups = g1 g2
	a1.sources.r1.filegroups.g1 = /opt/data/logdata/app/event.*
	a1.sources.r1.filegroups.g2 = /opt/data/logdata/wx/event.*
	a1.sources.r1.headers.g1.datatype = app
	a1.sources.r1.headers.g2.datatype = wx
	a1.sources.r1.batchSize = 100
	
	
	a1.channels.c1.type = file
	a1.channels.c1.checkpointDir = /opt/data/flumedata/file-channel/checkpoint
	a1.channels.c1.dataDirs = /opt/data/flumedata/file-channel/data
	
	a1.sinks.k1.channel = c1
	a1.sinks.k1.type = avro
	a1.sinks.k1.hostname =  titan2
	a1.sinks.k1.port = 41414
	a1.sinks.k1.batch-size = 100
	
	a1.sinks.k2.channel = c1
	a1.sinks.k2.type = avro
	a1.sinks.k2.hostname = titan3
	a1.sinks.k2.port = 41414
	a1.sinks.k2.batch-size = 100
	
	# 定义sink组及其配套的sink处理器
	a1.sinkgroups = g1
	a1.sinkgroups.g1.sinks = k1 k2
	a1.sinkgroups.g1.processor.type = failover
	a1.sinkgroups.g1.processor.priority.k1 = 5
	a1.sinkgroups.g1.processor.priority.k2 = 1
	a1.sinkgroups.g1.processor.maxpenalty = 10000



### 下游配置文件

	cd /home/zheyi/flume/conf

在titan2和titan3中创建

	vim flume_level_2.properties

插入

	a1.sources = r1
	a1.channels = c1
	a1.sinks = k1
	
	a1.sources.r1.channels = c1
	a1.sources.r1.type = avro
	a1.sources.r1.bind = 0.0.0.0
	a1.sources.r1.port = 41414
	a1.sources.r1.batchSize = 100
	
	a1.channels.c1.type = file
	a1.channels.c1.checkpointDir = /opt/data/flumedata/file-channel/checkpoint
	a1.channels.c1.dataDirs = /opt/data/flumedata/file-channel/data
	
	a1.sinks.k1.channel = c1
	a1.sinks.k1.type = hdfs
	a1.sinks.k1.hdfs.path = hdfs://titan1:9000/logdata/%{datatype}/%Y-%m-%d
	a1.sinks.k1.hdfs.filePrefix = zheyi_data
	a1.sinks.k1.hdfs.fileSuffix = .log
	a1.sinks.k1.hdfs.rollInterval = 60
	a1.sinks.k1.hdfs.rollSize = 268435456
	a1.sinks.k1.hdfs.rollCount = 0
	a1.sinks.k1.hdfs.batchSize = 100
	a1.sinks.k1.hdfs.fileType = DataStream
	a1.sinks.k1.hdfs.useLocalTimeStamp = true

## 替换Guava包

	cd /home/zheyi/flume/lib
	rm  guava-11.0.2.jar
	cp /home/zheyi/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar /home/zheyi/flume/lib
	
	
## 增加Hadoop包	

	cp /home/zheyi/hadoop/share/hadoop/common/hadoop-common-3.3.4.jar /home/zheyi/flume/lib
	
	cp /home/zheyi/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.4.jar /home/zheyi/flume/lib

	cp /home/zheyi/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar /home/zheyi/flume/lib

	cp /home/zheyi/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.4.jar /home/zheyi/flume/lib
	
	cp /home/zheyi/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar /home/zheyi/flume/lib
	
	cp /home/zheyi/hadoop/share/hadoop/yarn/timelineservice/lib/htrace-core-3.1.0-incubating.jar /home/zheyi/flume/lib


	cp /home/zheyi/hadoop/share/hadoop/common/lib/woodstox-core-5.3.0.jar /home/zheyi/flume/lib
	
	cp /home/zheyi/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar /home/zheyi/flume/lib

	cp /home/zheyi/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar /home/zheyi/flume/lib

	cp /home/zheyi/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar /home/zheyi/flume/lib
	
	cp /home/zheyi/hadoop/share/hadoop/client/hadoop-client-api-3.3.4.jar /home/zheyi/flume/lib
	
	cp /home/zheyi/hadoop/share/hadoop/client/hadoop-client-runtime-3.3.4.jar /home/zheyi/flume/lib
	
	cp /home/zheyi/hadoop/share/hadoop/client/hadoop-client-minicluster-3.3.4.jar /home/zheyi/flume/lib

## 给HDFS赋权

非常重要，否则会出现权限问题

在titan1-4中：

	 hadoop fs -chmod -R 777 /


## 启动flume

在titan2 和 titan3：

	cd /home/zheyi/flume/bin/
	sudo sh flume-ng agent -c /home/zheyi/flume/conf -f /home/zheyi/flume/conf/flume_level_2.properties -n a1 -Dflume.root.logger=INFO,console & 
	

在 titan1：

	cd /home/zheyi/flume/bin/
	sudo sh flume-ng agent -c /home/zheyi/flume/conf/ -f /home/zheyi/flume/conf/flume_level_1.properties -n a1 -Dflume.root.logger=INFO,console & 


## 结果
![](Images/3.png)




## HDFS 安装过程中关键文件

### /etc/profile

	export JAVA_HOME=/home/zheyi/java
	export PATH=$PATH:$JAVA_HOME/bin
	
	#zk
	export ZK_HOME=/home/zheyi/zookeeper
	export PATH=$PATH:$ZK_HOME/bin
	
	#hadoop
	export HADOOP_HOME=/home/zheyi/hadoop
	export PATH=$PATH:$HADOOP_HOME/bin
	export PATH=$PATH:$HADOOP_HOME/sbin
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export HADOOP_YARN_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
	export JAVA_LIBRARY_PATH=$HADDOP_HOME/lib/native:$JAVA_LIBRARY_PATH
	
### core-site.xml

	<configuration>
	    <!--指定zookeeper地址-->
	    <property>
	        <name>ha.zookeeper.quorum</name>
	        <value>titan2:2181, titan3:2181, titan4:2181</value>
	    </property>
	    <!--指定Hadoop临时目录-->
	    <property>
	        <name>hadoop.temp.dir</name>
	        <value>/home/zheyi/tmp</value>
	    </property>
	    <!--指定Namenode访问端口-->
	    <property>
	        <name>fs.defaultFS</name>
	        <value>hdfs://titan1:9000</value>
	    </property>
	</configuration>
	
###  hdfs-site.xml

	<configuration>
	
	<!--指定设备备份数量-->
	    <property>
	        <name>dfs.replication</name>
	        <value>3</value>
	    </property>
	
	<!--指定高可用集群的名字空间-->
	    <property>
	        <name>dfs.nameservices</name>
	        <value>mycluster</value>
	    </property>
	
	<!-- 指定NameNode节点的名字空间 -->
	    <property>
	      <name>dfs.ha.namenodes.mycluster</name>
	      <value>nn1,nn2</value>
	    </property>
	
	<!--指定nn1,nn2的RPC地址-->
	    <property>
	      <name>dfs.namenode.rpc-address.mycluster.nn1</name>
	      <value>titan1:9000</value>
	    </property>
	    <property>
	      <name>dfs.namenode.rpc-address.mycluster.nn2</name>
	      <value>titan2:9000</value>
	    </property>
	
	<!-- 指定nn1、nn2的http地址 -->
	        <property>
	        <name>dfs.namenode.http-address.mycluster.nn1</name>
	        <value>titan1:50070</value>
	    </property>
	    <property>
	      <name>dfs.namenode.http-address.mycluster.nn2</name>
	      <value>titan2:50070</value>
	    </property>
	
	<!--解释：hadoop 守护进程一般同时运行RPC 和HTTP两个服务器，RPC服务器支持守护进程间的通信，HTTP服务器则提供与用户交互的Web页面。-->
	
	
	<!--设置共享edits的存放地址，将共享edits文件存放在哎QJournal集群中的QJCluster目录下-->
	    <property>
	      <name>dfs.namenode.shared.edits.dir</name>
	      <value>qjournal://titan2:8485;titan3:8485;titan4:8485/QJCluster</value>
	    </property>
	
	
	<!--指定JournalNode集群在对NameNode的目录进行共享时，自己存储数据的磁盘路径-->
	
	    <property>
	      <name>dfs.journalnode.edits.dir</name>
	      <value>/home/zheyi/hadoop/QJEditsData</value>
	    </property>
	
	
	<!--指定是否启动自动故障恢复，即当NameNode出故障时，是否自动切换到另一台NameNode-->
	
	    <property>
	       <name>dfs.ha.automatic-failover.enabled</name>
	       <value>true</value>
	     </property>
	
	
	<!--指定cluster1出故障时，哪个实现类负责执行故障切换-->
	
	    <property>
	      <name>dfs.client.failover.proxy.provider.mycluster</name>
	      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	    </property>
	
	
	<!--一旦需要NameNode切换，使用ssh方式进行操作-->
	
	    <property>
	      <name>dfs.ha.fencing.methods</name>
	      <value>sshfence</value>
	    </property>
	
	
	<!--如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置-->
	
	    <property>
	      <name>dfs.ha.fencing.ssh.private-key-files</name>
	      <value>/home/zheyi/.ssh/id_rsa</value>
	    </property>
	    
	 <!--关闭权限校验-->   
	    <property>
		 	<name>dfs.permissions</name>
		 	<value>false</value>
		 </property>	
	
	</configuration>
	
### yarn-site.xml

	<configuration>
	
	<!-- Site specific YARN configuration properties -->
	 <property>
	        <name>yarn.nodemanager.aux-services</name>
	        <value>mapreduce_shuffle</value>
	        <description>NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序</description>
	    </property>
	
	<property>
	    <name>yarn.resourcemanager.hostname</name>
	    <value>titan1</value>
	    <description>titan1</description>
	</property>	
	
	<property>
		<name>yarn.scheduler.maximum-allocation-mb</name>
		<value>2048</value>
	</property>
	
	<property>
		<name>yarn.scheduler.minimum-allocation-mb</name>
		<value>2048</value>
	</property>
	
	<property>
		<name>yarn.nodemanager.vmem-pmem-ratio</name>
		<value>2.1</value>
	</property>
	
	<property>
		<name>mapred.child.java.opts</name>
		<value>-Xmx2048m</value>
	</property>
	
	
	</configuration>