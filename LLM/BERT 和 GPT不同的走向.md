# BERT 和 GPT不同的走向

##神经网络的发展

![](Images/13.png)

## BERT的优势

![](Images/14.png)

利用**预训练模型**，做少量增量训练，得到最终的模型。

Unlabeled sentence A and B pair： 不用做标注了。

![](Images/15.png)

### 预训练

预训练的transformer

![](Images/16.png)

比如queen 和 king可以如上图所示变成2个向量。“king减去queue的差值”与“国王减去王后的差值”类似。


### 双向模型

![](Images/17.png)


## BERT 和 GPT的差异

![](Images/18.png)

## BERT 和 GPT的共识

![](Images/19.png)



