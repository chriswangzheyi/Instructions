# 主流的开源模型体系 





| 模型类型            | 架构                       | 代表模型                     | 主要能力                 | 典型任务                     |
| ------------------- | -------------------------- | ---------------------------- | ------------------------ | ---------------------------- |
| **Encoder-only**    | 仅使用 Transformer Encoder | BERT、RoBERTa、DeBERTa       | 强表征能力、上下文理解   | 分类、命名实体识别、语义匹配 |
| **Decoder-only**    | 仅使用 Transformer Decoder | GPT、LLaMA、Falcon、Baichuan | 强生成能力、上下文续写   | 文本生成、问答、对话         |
| **Encoder-Decoder** | 编码器+解码器结构          | T5、BART、mT5、Flan-T5       | 输入输出对齐、多任务转换 | 翻译、摘要、对话生成         |



## Transformer架构



![](Images/1.svg)



## 主要开源模型体系与技术内核



### 1️⃣ GPT 系列（Decoder-only）

- **提出者**：OpenAI（但 GPT-2 以前部分开源）
- **核心思想**：基于 Transformer Decoder 的自回归（Autoregressive）语言建模。
- **预训练目标**：最大化 `P(next_token | previous_tokens)`。
- **创新点**：
  - 使用**海量无监督语料**（数百GB以上）进行语言建模；
  - 自回归结构擅长长文本生成；
  - 强调生成连续性与上下文一致性。
- **技术影响**：
  - 引领了「大模型即通用智能」的范式；
  - 带动了 LLaMA、Falcon、Mistral、ChatGLM 等开源分支；
  - 推动了 RLHF（人类反馈强化学习）成为标准流程。

### 2️⃣ BERT 系列（Encoder-only）

- **提出者**：Google AI Language（2018）
- **核心思想**：双向掩码语言建模（Masked Language Modeling）。
- **训练方式**：
  - 随机遮盖句子中的部分 Token，让模型预测被遮盖的内容；
  - 同时进行 Next Sentence Prediction 任务（预测两句是否相邻）。
- **技术突破**：
  - 通过双向注意力捕获更丰富语义；
  - 在多项 NLP benchmark（如 GLUE、SQuAD）上刷新 SOTA；
  - 引发“**预训练 + 微调**”范式革命。
- **生态延伸**：
  - RoBERTa、ALBERT、ELECTRA、DeBERTa 均是 BERT 的改进版；
  - 广泛用于搜索排序、信息抽取、句子匹配等。

### 3️⃣ RoBERTa（BERT 改进版）

- **提出者**：Meta AI（Facebook AI，2019）
- **主要改进**：
  - 移除 NSP 任务；
  - 使用更大的 batch、更长的训练、更高学习率；
  - 更充分的动态 masking 策略。
- **效果**：在同等参数量下显著优于原始 BERT，成为业界预训练 Encoder 的常用基线。

### 4️⃣ XLNet（融合自回归与掩码）

- **提出者**：CMU + Google Brain（2019）
- **思路**：
  - 弥补 BERT 的“独立预测 Token”问题；
  - 通过**排列语言建模（Permutation LM）**实现双向上下文学习；
  - 保留自回归特性，提高生成能力。
- **核心创新**：引入**Transformer-XL**的长程记忆机制（Segment-Level Recurrence）。
- **应用特点**：在问答、阅读理解任务上强于 BERT，但训练代价更高。

### 5️⃣ T5 / Text-to-Text Transfer Transformer（Encoder-Decoder）

- **提出者**：Google Research（2019）
- **统一框架思想**：
  - 把所有 NLP 任务都转化为“文本到文本”问题：
    - 翻译任务 → “translate English to German: …”
    - 分类任务 → “classify sentiment: …”
    - 问答任务 → “question: … context: …”
  - 极大简化了多任务微调流程。
- **技术亮点**：
  - 使用巨量多任务语料（C4 数据集）；
  - 在模型规模从 220M 到 11B 之间线性扩展；
  - 促进了 Flan-T5、mT5、UL2 等后续模型的出现。
- **应用广度**：翻译、摘要、问答、指令生成、提示学习等。