# 多轮对话任务如何微调模型



##  先选“对话类型”

- **开放域闲聊**（聊天、问答、写作辅助）
- **任务型对话**（预订/办理业务/技术支持，含意图与槽位）
- **工具/函数调用对话**（检索、数据库/日程/代码执行等）

> 不同类型决定你**数据结构**与**评测指标**





##  数据格式（强烈建议统一“对话转写 Schema”）

只在 **assistant** 的输出上计算 loss（mask 其余角色），保证“只学会模型的发言”。



#### 通用多轮 Schema（JSONL，一条样本=一段对话）

```python
{
  "id": "dlg_0001",
  "conversation": [
    {"role": "system", "content": "你是专业旅行顾问，回答要有行程表。"},
    {"role": "user", "content": "我们两大一小，周末重庆玩两天，预算3000。"},
    {"role": "assistant", "content": "收到，我先问几个偏好：更倾向博物馆还是自然？"},
    {"role": "user", "content": "小朋友喜欢科技馆，想吃特色美食。"},
    {"role": "assistant", "content": "第1天：上午××科技馆… 晚餐：××火锅…\n第2天：…\n总预算约2800。"}
  ],
  "metadata": {"domain": "travel", "difficulty": "medium"}
}

```

#####  任务型（意图/槽位/动作可选—用于训练结构化思维）

```python
{
  "id": "tod_0102",
  "conversation": [
    {"role":"system","content":"你是客服机器人。"},
    {"role":"user","content":"帮我订明天中午上海到北京的高铁。"},
    {"role":"assistant","state":{"intent":"book_train","slots":{"from":"上海","to":"北京","date":"明天","time":"中午"}},"content":"好的，我来确认时间段与车次偏好。"}
  ],
  "metadata": {"domain":"ticketing"}
}
```

##### 工具调用（函数/检索）

```python
{
  "id": "tool_0007",
  "conversation": [
    {"role":"system","content":"你可以调用工具 get_fare(from,to,date)。"},
    {"role":"user","content":"查下明早重庆到成都票价。"},
    {"role":"assistant","tool_call":{"name":"get_fare","arguments":{"from":"重庆","to":"成都","date":"2025-10-20"}}},
    {"role":"tool","name":"get_fare","result":{"lowest":128,"avg":156}},
    {"role":"assistant","content":"最低128元，平均156元；建议提前1小时到站。"}
  ]
}
```



## 数据来源与构建

- **收集真实对话**：客服/工单/Chat 记录（脱敏！）
- **高质量合成**：让强模型扮演用户与助手**自博弈/自对话**生成多轮（人工抽检）
- **多样化增强**：同义改写、扰动问题、插入澄清轮次、加入失败/拒答示例
- **负样本/对比样本**：错误答案、跑题、缺少澄清 → 供 DPO/SimPO 等偏好学习

> 规模建议：**1–5万段对话**（每段 3–8 轮）即可把多轮行为学稳；任务型需要**覆盖所有意图与边界条件**。



## 训练要点（LoRA/QLoRA 为主）

- **只计算 assistant token 的 loss**：避免模型学到用户话术
- **使用“Chat Template”**：与推理时完全一致（HF 模型通常自带）
- **长上下文策略**：窗口截断优先保留**最近多轮 + 摘要**
- **节省显存**：LoRA(r=16/32)、QLoRA(4bit)；开启 gradient checkpointing
- **反复读写**：多轮场景更吃“轮次记忆”，适度增大训练步数而不是盲目扩数据



示例（以 LLaMA-Factory/Axolotl/Transformers 为例）：

```shell
python train_sft.py \
  --model meta-llama/Meta-Llama-3-8B-Instruct \
  --data_path data/dialogue.jsonl \
  --chat_template llama3 \
  --train_on_assistant_only true \
  --lora_r 16 --lora_alpha 32 --lora_dropout 0.1 \
  --bf16 true --gradient_checkpointing true \
  --per_device_train_batch_size 2 --accumulation_steps 8 \
  --lr 2e-5 --epochs 3 --max_seq_length 4096

```



## 专为多轮对话的“行为”训练技巧

- **澄清/确认范式**：在训练集中**显式包含**“先问再答”“边问边补齐槽位”的轮次
- **拒答与安全**：提供“无法回答/需要授权/越权拒绝”的正例与反例
- **结构化输出**：每次回答既给“自然语言”，又给**机器可解析的JSON**（用于下游执行）
- **少犯复读**：加入解码参数&惩罚（repetition penalty/top-p/temperature），并在数据中给“避免重复”的演示
- **函数调用**：把“思考→调用→读取→整合”完整多轮链路放进数据，教会回合间的状态机



## 评测（不要只看 BLEU/ROUGE）

按**回合**与**会话**双层评估：

**回合级（Turn）**

- 合规/安全：是否正确拒答或脱敏
- 任务前置：该问澄清时有没有先问
- 结构化有效：JSON/字段可解析率
- 事实一致性：基于提供 context 的支持率

**会话级（Dialog）**

- **Task Success**（是否达成目标：订票/生成合格文档等）
- **对话步数**（能否少问多得）
- **用户满意度**（人工 1–5 分）
- **稳定性**（同义改写后成功率变化 ≤ 3%）

> 任务型建议引入 **MultiWOZ 风格**的成功率与槽位填充正确率；工具调用评测 **函数参数正确率 / 引用命中率**。



## 进阶：SFT + 偏好优化

- **先 SFT** 学会“流程与格式”，再用 **DPO/SimPO** 做偏好学习（更自然、更稳重）
- **生成对比样本**：正确答复 vs 啰嗦/幻觉/跳步 的答案对；用偏好损失拉开差距
- **在线/离线 A/B**：以会话成功率、人工打分、召回工单率作为线下-线上闭环



## 常见坑位与排查

- **只喂单轮数据** → 多轮学不会：务必 ≥3 轮的真实对话样本
- **训练/推理模板不一致** → 模型“听不懂”：统一 chat template
- **loss 覆盖到 user** → 学坏用户口吻：只训练 assistant token （属于模型回复部分的 token）
- **忘记拒答** → 安全事故：加入边界与拒答正反例
- **复读/拖沓** → 解码参数+样本演示+重复惩罚
- **记忆丢失** → 引入摘要/状态对象/RAG，并在训练时对齐



## 微调过程

使用多轮对话数据集对预训练模型进行微调。微调的过程类似于监督学习，通过最小化 模型在训练集上的损失函数来优化模型参数。可以使用常⻅的优化算法，如随机梯度下降(SGD)或 Adam。



## 超参数调整

微调过程中需要选择合适的学习率、批次大小、训练轮数等超参数。可以通过交叉验 证或其他调参方法来选择最佳的超参数组合。



## 补充 

### assistant token 

> **assistant token = 属于模型回复部分的 token。**

也就是说，在一条对话样本中，我们通常会有多种角色：

- `system`：系统设定（定义身份）
- `user`：用户输入
- `assistant`：模型（AI）输出

训练时我们只希望模型**学会生成 AI 的回答部分**，
 而不是去模仿用户输入、系统提示。



举个例子：



假设你的多轮对话样本是这样：

```
{
  "conversation": [
    {"role": "system", "content": "你是一个专业旅行顾问。"},
    {"role": "user", "content": "帮我设计一个去重庆的两天行程。"},
    {"role": "assistant", "content": "第1天：解放碑、长江索道... 第2天：磁器口、洪崖洞..."}
  ]
}
```

在训练时，模型看到的 token 序列其实是类似这样的：

```
<bos> system: 你是一个专业旅行顾问。
user: 帮我设计一个去重庆的两天行程。
assistant: 第1天：解放碑、长江索道... 第2天：磁器口、洪崖洞...
<eos>
```



为什么只计算 assistant token 的 loss？

因为在微调时：

- 我们希望模型 **学会“怎么回答”**，
- 而不是去**记住用户的提问内容**（那是输入，不是输出）。

如果你对所有 token 都计算 loss，模型就会：

- 模仿用户语气、说“请帮我…”；
- 输出中混进奇怪提示；
- 学不到真正的“问→答”模式。

✅ 所以正确做法是：

> 只计算 assistant 的 token 的预测损失（loss）
>
> 

代码实现方式举例:

Hugging Face (transformers):

```python
labels = input_ids.clone()
labels[mask_not_assistant] = -100   # -100 = 不计算loss的token
```

. LLaMA-Factory、FastChat 等框架:

```
--train_on_assistant_only true
```

