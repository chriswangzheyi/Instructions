# LLM+向量库的文档对话



## 为什么大模型需要外挂（向量）知识库？

**LLM 的知识局限性：**

- 大模型的知识来源于训练语料，具有**时间滞后性**（无法知道训练后发生的事件）。
- 模型上下文有限（Context Length），不能一次性加载所有外部资料。
- 企业或私有文档（如合同、报告、代码）通常不在预训练数据中。



**外挂向量知识库是让 LLM 获得“外部事实记忆”的关键方式**，使其能回答：

- 公司私有知识问答；
- 技术文档对话；
- 论文检索；
- 法律、财务、合同等专业内容。



## 基于 LLM + 向量库的文档对话思路是怎么样的

```
用户问题
   ↓
语义向量化 (Embedding)
   ↓
在向量库中检索最相关文档片段
   ↓
构造 Prompt（问题 + 检索到的上下文）
   ↓
输入给 LLM 生成最终答案
```





## 基于 LLM + 向量库的文档对话核心技术是什么？



| 技术模块                        | 说明                                  | 常见实现                                   |
| ------------------------------- | ------------------------------------- | ------------------------------------------ |
| **文本切分 (Text Chunking)**    | 将长文档分段，以适合向量化            | LangChain `RecursiveCharacterTextSplitter` |
| **向量化 (Embedding)**          | 将文本转换为高维向量                  | OpenAI Embedding, BGE, GTE, Instructor 等  |
| **向量数据库 (Vector Store)**   | 存储与检索向量                        | FAISS, Chroma, Milvus, Pinecone, Weaviate  |
| **语义检索 (Semantic Search)**  | 根据向量相似度找到相关片段            | cosine / dot product 相似度计算            |
| **Prompt 构造**                 | 将检索结果 + 问题拼入上下文           | LangChain PromptTemplate / 自定义模板      |
| **大模型生成 (LLM Generation)** | 最终生成自然语言答案                  | GPT, Claude, Qwen, Llama 等                |
| **评估与反馈 (Evaluation)**     | 基于 BLEU、ROUGE、Faithfulness 等指标 | LangSmith / Ragas                          |





## 基于 LLM + 向量库的文档对话 —— 存在的痛点



| 问题                            | 说明                                           | 解决方向                                                     |
| ------------------------------- | ---------------------------------------------- | ------------------------------------------------------------ |
| **1️⃣ 向量检索精度不稳定**        | 语义相似度不代表语义正确性；容易检索到干扰片段 | 使用高质量 Embedding 模型 + Reranker 重排序（如 bge-reranker） |
| **2️⃣ 文档切分不合理**            | 太长丢上下文，太短丢语义                       | 动态窗口切分 + Overlap 策略                                  |
| **3️⃣ Token 成本高**              | 检索太多片段导致输入过长                       | 限制上下文数量，做内容摘要或压缩                             |
| **4️⃣ 多文档融合难**              | 模型容易混淆不同来源内容                       | 按文档分段生成，最后合并总结                                 |
| **5️⃣ 模型幻觉（Hallucination）** | 模型编造不存在的信息                           | 在 Prompt 中明确“不知道请说不知道”，并添加可信度评分         |
| **6️⃣ 更新延迟**                  | 向量库内容不实时                               | 增量更新 + 周期重建 Index                                    |
| **7️⃣ 私有部署难**                | LLM、向量库、文件解析依赖多                    | 使用 LangChain + Chroma / Milvus + FastAPI 一体化部署        |



## 为什么大模型会出现幻觉

LLM 的本质是 **概率语言模型**，它根据上下文预测最可能出现的下一个词。它**不理解真相，只统计相关性**。



| 原因类别                                 | 说明                                         |
| ---------------------------------------- | -------------------------------------------- |
| **1️⃣ 数据缺失**                           | 模型训练语料中不存在该知识，只能“猜”         |
| **2️⃣ 模型本质概率化**                     | LLM 根据概率采样生成文本，不具备事实校验机制 |
| **3️⃣ Prompt 诱导错误**                    | 提示词不明确或暗示性强，模型“迎合”用户期待   |
| **4️⃣ 上下文不足**                         | 提供的信息不够，模型用语言填补空白           |
| **5️⃣ 多文档融合错误**                     | 从多个来源拼接时语义冲突                     |
| **6️⃣ 长上下文遗忘（Context Forgetting）** | 输入超过模型记忆长度，早期内容被截断         |
| **7️⃣ 没有外部验证通道**                   | 纯生成模式下，模型无法“查证”自己输出是否真实 |



## 常见幻觉类型



| 类型                                        | 示例                                        | 说明             |
| ------------------------------------------- | ------------------------------------------- | ---------------- |
| **事实性幻觉 (Factual Hallucination)**      | “特斯拉创立于 2005 年。”                    | 事实错误         |
| **引用幻觉 (Citation Hallucination)**       | “根据维基百科第 3 章，...”（实际上不存在）  | 虚构来源         |
| **命名实体幻觉 (Entity Hallucination)**     | “教授 John Li 发表了论文《AI Ethics 2023》” | 虚构人物或论文   |
| **推理幻觉 (Reasoning Hallucination)**      | 推理逻辑错误，例如错误的因果关系            | 逻辑链断裂       |
| **解释性幻觉 (Explanatory Hallucination)**  | 模型自己“编造原理”来解释未知现象            | 伪科学倾向       |
| **忠实度幻觉 (Faithfulness Hallucination)** | 与给定上下文内容不一致                      | 无法对齐引用材料 |



## 缓解幻觉的常见方法

| 方法                                | 思路                           | 实现方式                          |
| ----------------------------------- | ------------------------------ | --------------------------------- |
| **1️⃣ 引入外部知识 (RAG)**            | 在生成前检索真实文档供模型参考 | 向量库 / 知识图谱                 |
| **2️⃣ 提示工程 (Prompt Engineering)** | 明确要求模型仅基于提供内容回答 | “If not found, say I don’t know.” |
| **3️⃣ 自我验证 (Self-consistency)**   | 模型多次回答并投票取一致结论   | chain-of-thought + self-check     |
| **4️⃣ 模型裁判 (Verifier Model)**     | 第二个模型对输出进行事实核查   | LLM-as-a-Judge / Fact-Checker     |
| **5️⃣ 结构化输出约束**                | 要求模型返回 JSON 或可解析结构 | Output Parser / Guardrail         |
| **6️⃣ 多阶段生成 (Refine/MapReduce)** | 分段生成 + 汇总检查            | LangChain “map_reduce” 模式       |
| **7️⃣ 引入引用（Citations）**         | 每个回答附带来源链接或文档编号 | 检索式问答必备机制                |
| **8️⃣ 人机协同审核**                  | 人工验证高风险场景输出         | 金融/法律/医疗领域必做            |



## 基于**LLM+**向量库的文档对话 **prompt** 模板 如何构建?

Prompt 模板设计的三原则：

| 原则                        | 说明                             |
| --------------------------- | -------------------------------- |
| 🎯 **相关性 (Relevance)**    | 模型的回答必须与检索文档内容一致 |
| 🧩 **约束性 (Constraint)**   | 限定回答来源，减少幻觉           |
| 🔁 **灵活性 (Adaptability)** | 模板可复用、可参数化、可扩展     |



#### 确定查询类型（Query Type）

| 查询类型                        | 示例 Prompt 模板                                             | 适用场景                 |
| ------------------------------- | ------------------------------------------------------------ | ------------------------ |
| **问题查询 (Question Query)**   | “我有一个关于 **{topic}** 的问题：{question}。请根据以下文档回答。” | 用户直接提问（常见问答） |
| **主题查询 (Topic Query)**      | “我想了解关于 **{topic}** 的信息，请从文档中提取核心内容。”  | 概览、主题信息收集       |
| **摘要查询 (Summary Query)**    | “请根据以下内容生成简洁摘要，突出关键信息。”                 | 多文档摘要或段落总结     |
| **对比查询 (Comparison Query)** | “请比较文档中提到的 **{item1}** 与 **{item2}** 的区别。”     | 报告分析、政策对比       |



#### 查询内容（Query Content）

| 文档类型                | 可提取字段                 | 示例                                              |
| ----------------------- | -------------------------- | ------------------------------------------------- |
| **新闻类**              | 标题、日期、主题、事件描述 | “请问有关于 {keyword} 的新闻报道吗？”             |
| **学术论文**            | 作者、摘要、年份、关键词   | “请总结 {author} 在 {field} 方向的主要研究结论。” |
| **公司内部文档**        | 部门、时间、政策、流程     | “根据公司文件，{topic} 的具体流程是什么？”        |
| **产品文档 / API 文档** | 方法名、参数、返回值       | “请解释函数 {method} 的作用与用法。”              |



#### 上下文信息（Contextual Information

考虑多轮对话与上下文依赖：

- 当前问题可能依赖上一次回答；
- 模型需要记住上文主题、文件来源等信息

示例模版：

```
上次我们讨论的是「{previous_topic}」，  
我现在想进一步了解「{new_question}」。  
请基于以下文档内容回答：
{retrieved_text}
```





## 文档切分粒度不好把控，既担心噪声太多又担心语义信息丢失怎么办



#### 动态切分（Dynamic Chunking）

不要用固定字数（如每 500 字切一块），而是根据**语义结构**自适应切分。
 常用方法：

- 按 **自然段落（换行符 / 标题符）**；
- 按 **句法边界（句号、冒号、分号）**；
- 对长段落再设定上限（如 800~1000 tokens）；
- 若过短（<200 tokens）则与下段合并



#### 语义重叠（Chunk Overlap）

上下文连续性问题可通过“**Chunk Overlap**”解决。
 即每个 chunk 与下一个 chunk 保留部分重复内容（如 100–200 tokens）

一般建议：

- chunk_size ≈ 800–1000 tokens
- overlap ≈ 10–20%（100–200 tokens）



#### 分层索引（Hierarchical Indexing）

对不同粒度的片段分别建索引：

- 小粒度（句级）→ 高召回；
- 中粒度（段级）→ 高精度；
- 大粒度（章节级）→ 全局理解。

检索时可以：

- 先召回小粒度；
- 再聚合成更大上下文段；
- 最后送入 LLM。



#### 基于 Embedding 密度的自适应切分

部分前沿方案（如 **Adaptive Chunking / Semantic Text Splitter**）
 通过 **embedding 相似度变化** 自动判断“语义断点”：

- 如果相邻句子 embedding 距离大 → 切断；
- 如果距离小 → 合并。

这比固定长度更智能，也能减少语义割裂。



## 评估切分效果的指标



| 维度                          | 评估方法                     |
| ----------------------------- | ---------------------------- |
| **检索召回率（Recall）**      | 检索到的文档是否包含正确答案 |
| **检索精度（Precision）**     | 检索片段是否与问题真正相关   |
| **上下文一致性（Coherence）** | 模型回答是否逻辑连贯         |
| **Token 成本**                | 平均上下文长度 × 查询次数    |
| **知识覆盖度**                | 不同 chunk 被命中的比例      |