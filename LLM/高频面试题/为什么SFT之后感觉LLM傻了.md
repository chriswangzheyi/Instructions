# 为什么**SFT**之后感觉**LLM**傻了?


## SFT 是什么？



SFT（监督微调）指的是在预训练后的基础上，用**人工标注的指令-回答数据**对模型再训练。例如：

```
输入：请解释牛顿第三定律。
输出：作用力与反作用力总是大小相等、方向相反。
```

SFT 的目标是让模型“听懂人话”，输出更符合人类语言风格。
 所以它本质上是：

> 从「预测下一个词」 → 「模仿人类回答」。



## 问题现象：SFT 之后模型变“傻”

很多开发者发现：

- 原本基础模型（Base Model）**推理、计算能力很强**；
- 但经过 SFT 之后：
  - 输出更客气了（“好的，我来帮您…”），但逻辑更差；
  - 以前能算的算术不准了；
  - 原本能自己拆解问题、分析过程，现在直接给出模糊答案；
  - 多轮任务（复杂指令）更容易“卡死”。

👉 简单说：

> **模型更“听话”，但更“笨”了。**



## 核心原因



##### 1️⃣ 数据问题：指令数据太“模板化”

SFT 数据集往往是“人造问答”：

- 一问一答格式化；
- 大量重复句式；
- 强调礼貌语气、形式正确；
- 但缺乏**多步推理、复杂逻辑、开放场景**。

🧩 结果：
 模型在微调中“遗忘”了原本复杂的语言和逻辑分布，
 被迫学成“客服机器人”。

> 就像一个天才学生，被反复训练去背标准答案，
>  久而久之他忘记了怎么思考。





##### 2️⃣ 目标函数问题：SFT 追求“模仿”，不是“最优解”

SFT 使用的损失函数是 **交叉熵（Cross-Entropy Loss）**：



它只要求模型**模仿标注答案**，而不会判断“这个答案是不是最合理的”。所以即使模型知道更好的答案，也会选择“模仿训练集里看起来对的那句”。导致：模型从“主动思考” → “迎合模板”。



##### 3️⃣ 过度微调（Overfitting on Style）

SFT 通常 batch 小、epoch 多，模型容易过拟合在语言风格上。
 它会：

- 优先输出“形式上正确”的句子；
- 忽略潜在逻辑或隐含条件。

> 就像一个学生只记作文格式：“首先、其次、最后”，
>  但完全不理解内容。



4️⃣ 忘却现象（Catastrophic Forgetting）

SFT 没有冻结底层参数，而是直接在整个模型上更新。结果模型会：

- **遗忘原有的知识分布；**
- **损伤预训练时学到的推理表征。**

尤其是 LoRA / Adapter 层太大或学习率过高时，
 这种“灾难性遗忘”更明显。





##  如何避免让模型“变傻”



| 策略                                    | 原理                                  | 说明                                      |
| --------------------------------------- | ------------------------------------- | ----------------------------------------- |
| **混合训练数据（Mix Pretrain & SFT）**  | 保留部分原始预训练数据混合SFT数据训练 | 避免语言能力退化                          |
| **分层冻结（Layer Freeze）**            | 冻结底层，微调高层                    | 保留底层语言理解、语义表示能力            |
| **多阶段训练（Multi-Stage）**           | 先SFT，后RLHF（奖励强化）             | 用奖励模型恢复“思考与判断”能力            |
| **使用高质量指令数据**                  | 包含推理、分析、链式思考（CoT）数据   | 保持模型逻辑链条能力                      |
| **正则化 & 小学习率**                   | 防止参数大范围漂移                    | 减少遗忘效应                              |
| **混合损失函数（CE + Reasoning Loss）** | 同时优化模仿与思考                    | 新兴研究方向（如 DeepSeek R1、OpenAI o1） |



## 现实工程里的经验总结



| 场景               | 建议做法                         | 说明                         |
| ------------------ | -------------------------------- | ---------------------------- |
| 教育/客服型对话    | 可以用 SFT 强化语气与安全性      | 风格优先，逻辑要求低         |
| 法律/科研/代码任务 | 不建议纯SFT，需混合CoT或RLHF     | 防止推理能力丢失             |
| 二次产品微调       | 用 **LoRA + 小学习率**，冻结底层 | 既改风格又保留智能           |
| 开源模型改造       | 选 “Instruct + Base 混合” 微调   | 避免变成只会说“对不起”的模型 |