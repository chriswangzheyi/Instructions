# 分布式训练中的加速卡选择



## 核心指标



| 维度         | 说明                             | 影响因素                                   |
| ------------ | -------------------------------- | ------------------------------------------ |
| **计算密度** | 每秒可执行的浮点运算量（TFLOPS） | 模型规模、Batch 大小                       |
| **显存容量** | 决定一次能放多大的模型和输入序列 | 模型参数规模、输入长度                     |
| **通信带宽** | 决定多卡之间同步效率             | GPU 互联方式（NVLink / PCIe / InfiniBand） |



## NVIDIA 主流加速卡家族与定位



| GPU型号            | 架构          | 显存          | 精度性能（FP16/BF16）         | NVLink支持 | 典型用途                      |
| ------------------ | ------------- | ------------- | ----------------------------- | ---------- | ----------------------------- |
| **RTX 4090**       | Ada Lovelace  | 24GB GDDR6X   | ~82 TFLOPS                    | ❌          | 入门级科研 / 单机实验         |
| **L4**             | Ada Lovelace  | 24GB GDDR6    | ~30 TFLOPS                    | ❌          | 轻量推理、语音识别、AI SaaS   |
| **A100 40GB/80GB** | Ampere        | 40–80GB HBM2e | ~155 TFLOPS                   | ✅          | 主流大模型训练                |
| **L40 / L40S**     | Ada Lovelace  | 48GB GDDR6    | ~90–90+ TFLOPS                | ✅（部分）  | 中型模型训练、生成式AI        |
| **H100 80GB**      | Hopper        | 80GB HBM3     | ~197 TFLOPS (FP16) / FP8 支持 | ✅          | 高端大模型训练 (GPT-3, LLaMA) |
| **A800 / H800**    | A/H100 降频版 | 80GB          | 中国大陆供应版                | ✅          | 国内高端部署                  |
| **T4**             | Turing        | 16GB GDDR6    | ~8 TFLOPS                     | ❌          | 推理、微服务                  |
| **V100 32GB**      | Volta         | 32GB HBM2     | ~112 TFLOPS                   | ✅          | 传统科研、旧集群              |



## 不同任务场景的推荐选型



| 场景                        | 模型规模 | 推荐GPU         | 理由                                 |
| --------------------------- | -------- | --------------- | ------------------------------------ |
| **语音识别 / 翻译 / OCR**   | <7B      | L4 / RTX 4090   | 成本低、算力够、显存足够             |
| **LLM 微调 (LoRA / QLoRA)** | 7B–13B   | L40S / A100 40G | 显存够、显存带宽高                   |
| **大模型 SFT / RLHF**       | 13B–70B  | A100 80G / H100 | 高显存 + NVLink 加速                 |
| **大规模预训练**            | 70B+     | H100 / H800     | 三维并行最佳，FP8/Transformer Engine |
| **高并发推理服务**          | 任意     | L4 / A10 / T4   | 性价比高，功耗低                     |
| **多模态 (图文视频)**       | 7B–30B   | L40S / H100     | FP8 / TensorCore 对多模态更友好      |



## 显存与模型规模对照表



| 模型规模 | 所需显存（单卡） | 典型方案          |
| -------- | ---------------- | ----------------- |
| 1–3B     | 12–24 GB         | RTX 4090 / L4     |
| 7B       | 24–32 GB         | L4 / L40S         |
| 13B      | 48 GB+           | L40S / A100       |
| 30B      | 80 GB+           | A100 80G / H100   |
| 70B      | ≥2×80 GB         | 多机并行 (ZeRO-3) |



## 通信架构的重要性

### 1. NVLink / NVSwitch

- GPU-GPU 直接高速互联，带宽高达 600GB/s。
- 用于 A100 / H100 / L40S 等专业卡。
- 单机 8 卡服务器（如 DGX）性能最优。

### 2. PCIe 互联

- 桌面卡（如 RTX 系列）主要使用。
- 受限于主板 PCIe 拓扑，通信瓶颈明显。
- 适合低成本实验或单机训练。

### 3. InfiniBand / RoCE 网络

- 跨节点通信标准（RDMA 支持）。
- 适合多机分布式训练；
- 一般要求 100Gbps 或以上链路。





## 综合评估三角：**性能 × 显存 × 成本**



| 档位       | GPU             | 单卡成本   | 优点                         | 缺点               |
| ---------- | --------------- | ---------- | ---------------------------- | ------------------ |
| 💡 入门级   | RTX 4090 / L4   | ¥1.5–3 万  | 性价比高、易部署             | 通信带宽弱         |
| ⚙️ 中级     | L40S / A100 40G | ¥6–9 万    | 显存够、适合多模态 / 中型LLM | 功耗高、需机架     |
| 🚀 高端     | A100 80G / H100 | ¥10–25 万  | 超高显存 + NVLink            | 成本高、需高端散热 |
| 🏢 专业集群 | H800 / DGX H100 | ¥80–200 万 | 极致性能、三维并行友好       | 采购周期长         |