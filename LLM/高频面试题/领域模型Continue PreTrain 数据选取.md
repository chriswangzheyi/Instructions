# 领域模型Continue PreTrain 数据选取?

###### 

## 领域模型Continue PreTrain 数据选取?

区别于 SFT（监督微调），CPT 的目标是：

- 不教模型“回答问题”；
- 而是让它**“吸收领域知识、习惯领域语体”**。

例如：

- 用法律语料继续预训练 → “法律大模型”；
- 用旅游评论和攻略数据 → “旅行推荐模型”；
- 用法院庭审录音转写数据 → “智慧法院模型”。



## 为什么要做 Continue Pretraining



| 问题                     | Continue Pretrain 的作用                 |
| ------------------------ | ---------------------------------------- |
| 通用模型对领域术语理解差 | 让模型掌握领域词汇、专有名词、缩写、句法 |
| 逻辑/语体风格不同        | 让模型学会行业书面语、文书结构、术语规范 |
| 知识时效性               | 更新模型知识到最近年份                   |
| 模型“幻觉”严重           | 通过高质量真实语料强化事实记忆           |
| 后续 SFT 难收敛          | 提前适配语言分布，为微调打底             |



## 数据选取的核心原则

CPT ≠ “把所有文本再喂一遍”。
 关键是 **“挑对语料 + 控制比例 + 保留通用性”。**



##### ✅ 原则一：贴合领域但不过窄

- 用于领域强化的语料占比一般建议 **30%–70%**
- 保留一部分通用语料（百科、新闻、论文）避免“语言退化”（catastrophic forgetting）

##### ✅ 原则二：覆盖真实任务语体

| 领域 | 推荐语料类型                       |
| ---- | ---------------------------------- |
| 法律 | 判决书、起诉状、法律条文、庭审转写 |
| 医疗 | 病历、论文、药品说明书、医学问答   |
| 金融 | 报告、招股书、行业研报、公告       |
| 旅游 | 游记、点评、攻略、景点介绍、问答   |
| 教育 | 讲义、教材、考试题、论文摘要       |
| 政务 | 政策文件、政府公告、新闻稿         |

##### ✅ 原则三：时效与可信

- 优先选择近 3 年语料；
- 官方来源（法院网、医药监管、政府网站）；
- 避免采集论坛灌水、爬虫噪声数据。

##### ✅ 原则四：格式统一与文本干净

- 清理 HTML、emoji、特殊符号；
- 控制每篇文档长度（200–2000 tokens）；
- 移除重复句子、空行、模板化文本；
- 可使用 `langdetect` 检测语言、`deduplicate` 过滤重复。





## ✅ 原则三：时效与可信

- 优先选择近 3 年语料；
- 官方来源（法院网、医药监管、政府网站）；
- 避免采集论坛灌水、爬虫噪声数据。

### ✅ 原则四：格式统一与文本干净

- 清理 HTML、emoji、特殊符号；
- 控制每篇文档长度（200–2000 tokens）；
- 移除重复句子、空行、模板化文本；
- 可使用 `langdetect` 检测语言、`deduplicate` 过滤重复。



## 比例设计建议



| 类型                          | 占比   | 作用               |
| ----------------------------- | ------ | ------------------ |
| **领域核心语料**              | 60–80% | 让模型“专业”       |
| **通用语料（新闻、百科）**    | 10–20% | 保留语言流畅度     |
| **跨领域补充（技术、社交）**  | 10–20% | 保留泛化与多样性   |
| **合成语料（AI生成 / 增广）** | ≤10%   | 增强推理和稀缺样本 |





## 数据构建流程



```
Step 1: 数据源选取
   - 确定领域：法律 / 金融 / 旅游 / 医疗 ...
   - 采集来源：官网、专业论坛、学术库、PDF转TXT

Step 2: 数据清洗
   - 正则化文本（去标签、去重复、去广告）
   - 分句与分段
   - 过滤异常长度、乱码、空内容

Step 3: 数据筛选
   - 关键词过滤（保留领域关键词）
   - 语言检测（langdetect）
   - 句法质量过滤（例如用GPT评分）

Step 4: 语料平衡与采样
   - 控制不同子域占比（刑事 / 民事 / 行政）
   - 混入部分通用语料

Step 5: 格式化与Token化
   - 统一UTF-8格式
   - 使用原始模型的Tokenizer进行BPE分词统计
   - 控制数据总token数（建议几亿到百亿级别）

Step 6: 校验
   - 用小模型做 sanity check：随机采样推理，检查是否“懂”术语。

```



## 规模与显存估算



| 模型规模      | CPT 数据规模    | 训练GPU配置  | 备注           |
| ------------- | --------------- | ------------ | -------------- |
| 7B            | 10–50B tokens   | 4×A100 80GB  | 半精度（BF16） |
| 13B           | 20–100B tokens  | 8×A100 80GB  | 支持混合精度   |
| 70B           | 100–500B tokens | 多节点集群   | 通常由大厂执行 |
| 小模型（≤3B） | 1–5B tokens     | 单卡 RTX4090 | 用于实验验证   |





## 让 CPT 更聪明的技巧



| 技术                        | 说明                                              |
| --------------------------- | ------------------------------------------------- |
| **Domain-Adaptive Masking** | 对领域关键词加权mask概率，更集中学习专业词汇      |
| **Curriculum Learning**     | 从通用 → 行业 → 子领域，分阶段喂数据              |
| **Contrastive Pretraining** | 让模型区分领域 vs 非领域文本，提升专注度          |
| **Knowledge Injection**     | 将领域知识图谱、术语表融入文本语料                |
| **Continual Evaluation**    | 每N步测试一次：Perplexity是否下降，幻觉率是否上升 |





## CPT 和 SFT的区别



| 项目     | **CPT（持续预训练）**                 | **SFT（监督微调）**               |
| -------- | ------------------------------------- | --------------------------------- |
| 中文名   | 持续预训练 / 继续预训练               | 监督微调 / 指令微调               |
| 英文全称 | Continue Pre-Training                 | Supervised Fine-Tuning            |
| 训练目标 | 让模型**掌握领域语言与知识**          | 让模型**学会执行任务与指令**      |
| 数据类型 | **无标签语料**（自监督学习）          | **有标签指令-回答对**（监督学习） |
| 典型任务 | 让模型更懂专业文本                    | 让模型更像人回答问题              |
| 损失函数 | Masked LM / Causal LM（预测下一个词） | Cross-Entropy（模仿答案）         |
| 输出形式 | 继续生成文本                          | 精确模仿答案                      |
| 训练阶段 | 在预训练之后、SFT之前                 | 在CPT之后、RLHF之前               |
| 结果     | 模型“更懂行”                          | 模型“更听话”                      |



直观的理解：



| 阶段                   | 比喻                                       | 模型状态                 |
| ---------------------- | ------------------------------------------ | ------------------------ |
| **预训练（Pretrain）** | 模型上小学：学语言、背单词                 | 能理解语言、不会做题     |
| **CPT（继续预训练）**  | 上“专业高中”：学法律、医学、金融等领域知识 | 懂行话，语感贴合领域     |
| **SFT（监督微调）**    | 接受“家教辅导”：学习怎么回答人类的问题     | 知道怎么礼貌、格式化回答 |



数据区别示例：



| 类型     | CPT 数据                                              | SFT 数据                                                     |
| -------- | ----------------------------------------------------- | ------------------------------------------------------------ |
| 数据形式 | 原始文本、文章、报告                                  | 指令 + 回复（有明确答案）                                    |
| 示例     | `"人工智能是一门研究如何让机器模仿人类智能的科学..."` | `{"instruction": "请解释什么是人工智能", "output": "人工智能是模仿人类认知能力的技术"}` |
| 是否标注 | ❌ 无需人工标注                                        | ✅ 需人工或AI标注                                             |
| 数据规模 | 通常数十亿 token 级                                   | 通常几十万至几百万条样本                                     |
| 生成目标 | 学习语言与知识分布                                    | 学习任务执行方式与回答风格                                   |



训练流程上的位置：

```
          ┌────────────────────────────┐
          │        Pretraining          │
          │ (大规模通用语料，自监督训练) │
          └────────────┬──────────────┘
                       ↓
          ┌────────────────────────────┐
          │    CPT (Continue Pretrain)  │
          │ 领域语料继续训练 → 更懂行业 │
          └────────────┬──────────────┘
                       ↓
          ┌────────────────────────────┐
          │    SFT (Supervised Fine-Tune) │
          │ 指令数据微调 → 更听人话 │
          └────────────┬──────────────┘
                       ↓
          ┌────────────────────────────┐
          │   RLHF / DPO / PPO 阶段    │
          │ 人类反馈优化 → 更有判断力 │
          └────────────────────────────┘

```



关键差异总结表：



| 对比维度         | CPT                     | SFT                                |
| ---------------- | ----------------------- | ---------------------------------- |
| **学习方式**     | 自监督（无标签）        | 有监督（带指令）                   |
| **目的**         | 语言适配、知识补充      | 行为适配、风格控制                 |
| **典型损失函数** | 语言建模损失（LM Loss） | 交叉熵损失（CE Loss）              |
| **典型语料**     | 法律文书、病历、报告    | “请总结...”“请解释...”这类对话数据 |
| **训练成本**     | 高（需大算力）          | 低（可单卡LoRA）                   |
| **效果变化**     | 模型知识增强            | 模型行为增强                       |
| **风险**         | 容易遗忘原有知识        | 容易逻辑退化（“变傻”）             |



## CPT 和 RAG在场景上的区别



| 需求类型                                   | 推荐方案           | 理由                                   |
| ------------------------------------------ | ------------------ | -------------------------------------- |
| ✅ **你要模型理解行业语言、语体风格**       | **CPT**            | 让模型“语言上贴近领域”，能更懂语义。   |
| ✅ **你要模型记住领域知识（相对稳定）**     | **CPT**            | 固化知识到参数里，比如医学、法律定义。 |
| ✅ **你要模型访问海量、经常变动的资料**     | **RAG**            | 知识可随时更新，如旅游推荐、公司文档。 |
| ✅ **你要模型生成带引用的答案**             | **RAG**            | 能返回“来源”，增强可信度。             |
| ✅ **你资源有限，不想重训模型**             | **RAG**            | 不需要GPU训练，只需构建向量索引。      |
| ✅ **你已有领域模型，但要实时补充外部知识** | **CPT + RAG 结合** | CPT 提升理解力，RAG 负责更新。         |



| 场景                               | 推荐策略                                            |
| ---------------------------------- | --------------------------------------------------- |
| **初创或小团队，快速上线领域助手** | 直接用 **RAG**（向量库 + 通用LLM）                  |
| **行业深耕（如法院、医疗、金融）** | **CPT → SFT → RAG 三阶段组合**                      |
| **需要长期积累知识、风格一致性**   | 重点做 **CPT + 小规模 SFT**                         |
| **数据量巨大或实时性强**           | 重点做 **RAG + 高质量检索系统（BM25 + Embedding）** |





| 想要的能力           | 选哪种方案       |
| -------------------- | ---------------- |
| **让模型“懂行话”**   | CPT              |
| **让模型“会办事”**   | SFT              |
| **让模型“能查资料”** | RAG              |
| **让模型“既懂又准”** | ✅ CPT + RAG 结合 |