## LLM的架构



## 1️⃣ Transformer 架构

LLM 通常采用 **Transformer** 架构，这是一种基于 **自注意力机制（Self-Attention）** 的神经网络结构。
 Transformer 由多个层堆叠而成，每层包含：

- **多头自注意力层（Multi-Head Self-Attention）**
- **前馈神经网络（Feed-Forward Network）**
- **残差连接（Residual Connection）和层归一化（Layer Normalization）**

这种架构能捕捉**长距离的语义依赖**，理解整段文本的上下文关系，是现代语言模型的核心基础。





## 2️⃣ 自注意力机制（Self-Attention）

自注意力机制让模型在处理一个词时，能够“关注”到句子中其他词的重要程度。
 例如在句子“猫追老鼠，因为它饿了”中，模型能判断“它”指的是“猫”。
 👉 每个词的表示不是固定的，而是动态地根据上下文加权形成的。





## 3️⃣ 多头注意力机制（Multi-Head Attention）

多头注意力是自注意力的扩展，它会用多个“注意力头”分别去关注不同的语义层面，比如：

- 一个头关注句法关系；
- 一个头关注情绪色彩；
- 一个头关注主谓结构。
   然后把这些信息整合起来，形成更丰富、更全面的上下文表示。



## 4️⃣ 前馈神经网络（Feed-Forward Network, FFN）

在每个注意力层后，Transformer 会接一个 **两层的全连接网络**（FFN），用于进一步特征映射。
 它对每个位置的向量独立处理，常用激活函数如 **ReLU** 或 **GELU**，帮助模型增强非线性表达能力。





## 5️⃣ 预训练与微调（Pre-training & Fine-tuning）

- **预训练阶段**：
   模型在超大规模无标签语料（如网页、书籍、维基百科）上通过**自监督学习**训练，比如：
  - 预测被遮挡的词（Masked LM）
  - 预测下一个词（Causal LM）
- **微调阶段**：
   预训练好的模型会在**特定任务数据**上进行微调，比如对话、摘要、代码生成、法律问答等，使模型更贴合实际应用场景。



## 6️⃣ 架构变体与发展方向

不同研究机构和应用会对 Transformer 架构进行优化，比如：

- **Decoder-only 模型（如 GPT 系列）**：只保留解码器层，用于文本生成。

- **Encoder-only 模型（如 BERT）**：只保留编码器层，用于理解类任务。

- **Encoder–Decoder 模型（如 T5、BART）**：结合两者优势，适用于翻译、摘要等任务。

  