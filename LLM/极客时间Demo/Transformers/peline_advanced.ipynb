{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HF Transformers 核心模块学习：Pipelines 进阶\n",
        "\n",
        "- 使用 Pipeline 如何与现代的大语言模型结合，以完成各类下游任务\n",
        "- 使用 Tokenizer 编解码文本\n",
        "- 使用 Models 加载和保存模型"
      ],
      "metadata": {
        "id": "TlUJtI6SsI63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用 Pipeline 调用大语言模型\n",
        "\n",
        "### Language Modeling\n",
        "\n",
        "语言建模是一项预测文本序列中的单词的任务。它已经成为非常流行的自然语言处理任务，因为预训练的语言模型可以用于许多其他下游任务的微调。最近，对大型语言模型（LLMs）产生了很大兴趣，这些模型展示了零或少量样本学习能力。这意味着该模型可以解决其未经明确训练过的任务！虽然语言模型可用于生成流畅且令人信服的文本，但需要小心使用，因为文本可能并不总是准确无误。\n",
        "\n",
        "通过理论篇学习，我们了解到有两种典型的语言模型：\n",
        "\n",
        "- 自回归：模型目标是预测序列中的下一个 Token（文本），训练时对下文进行了掩码。如：GPT-3。\n",
        "- 自编码：模型目标是理解上下文后，补全句子中丢失/掩码的 Token（文本）。如：BERT。"
      ],
      "metadata": {
        "id": "Gbf_p8hjsRYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用 GPT-2 实现文本生成"
      ],
      "metadata": {
        "id": "7tuUYbgVsvv5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSLRzCHdsD9l",
        "outputId": "efe75e6e-74e8-42d5-820c-d162476b05dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Hugging Face is a community-based open-source platform for machine learning. While it is an open-source framework, it is not designed for use on the production network yet. We have a project in our community based on this. We are'}]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n",
        "generator = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
        "generator(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在这个例子中，给定了一个提示（prompt）文本：\"Hugging Face is a community-based open-source platform for machine learning.\"，然后使用 GPT-2 模型来生成接下来的文本。\n",
        "\n",
        "GPT-2 是一个生成式预训练模型，它可以接收一个输入文本，并根据这个输入文本生成接下来的文本。在这个例子中，模型将根据给定的提示文本生成一个或多个与该提示相关的文本片段。"
      ],
      "metadata": {
        "id": "SLl9I5xGF4x_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 设置文本生成返回条数"
      ],
      "metadata": {
        "id": "tRe5TO57szQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"You are very smart\"\n",
        "generator = pipeline(task=\"text-generation\", model=\"gpt2\", num_return_sequences=3)"
      ],
      "metadata": {
        "id": "k1WsrMYWs0Wy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDVOQmyXt6tz",
        "outputId": "8d3370a6-d5cb-4109-82f7-4fb17e2f1551"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'You are very smart. You will learn to play.\" When asked about a potential matchmaking solution on Summoner\\'s Rift, Lee said of what he did in the past, \"I just went against the wrong people.\"\\n\\nThis was, as Lee'},\n",
              " {'generated_text': 'You are very smart. You understand. Yes. You understand that.\\n\\n[Chalk]\\n\\nNow I do not see the point of asking you to take time off to get the book out then, you are asking myself to spend my'},\n",
              " {'generated_text': 'You are very smart.\"\\n\\nBenson said her own parents were not.\\n\\nShe said they were trying to learn from the first day of work, but got so angry they sent her a video showing their daughter being verbally abused by one.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator(prompt, num_return_sequences=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0U8-x4VuBo_",
        "outputId": "4592e9b1-fe3d-4a79-99a5-e259275936d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'You are very smart and I am not a child and not intelligent and that I could be making mistakes or exaggerating,\" he said as he stood in the corner of his room at home with three other friends.\\n\\n\"If you don\\'t understand'},\n",
              " {'generated_text': \"You are very smart. She has always given her mind to things outside of my life. I don't mean to imply I'm a psychopath or that I've been mentally ill for the last decade or two, but she is very smart. I don\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 设置文本生成最大长度"
      ],
      "metadata": {
        "id": "wHwMA5JyxLys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator(prompt, num_return_sequences=2, max_length=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRvxzy7vxLfT",
        "outputId": "ca27a91d-7132-4d21-ce5d-a189489dbec4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'You are very smart and highly skilled in your job. Let the boss know she'},\n",
              " {'generated_text': 'You are very smart of you,\" he says, as you turn to the second'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用 BERT-Base-Chinese 实现中文补全\n"
      ],
      "metadata": {
        "id": "ihtvKhzUxTYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(task=\"fill-mask\", model=\"bert-base-chinese\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tZrI8isxVFM",
        "outputId": "66ce2768-eb6a-41d2-ab5e-9f7b65b3b10c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"人民是[MASK]可战胜的\"\n",
        "\n",
        "fill_mask(text, top_k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmOXcbR_xYjL",
        "outputId": "94fb24b4-0149-433e-d2d6-1f8e2b90cb27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.9203746318817139,\n",
              "  'token': 679,\n",
              "  'token_str': '不',\n",
              "  'sequence': '人 民 是 不 可 战 胜 的'}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 设置文本补全的条数"
      ],
      "metadata": {
        "id": "8zRJzF35xdLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"美国的首都是[MASK]\"\n",
        "\n",
        "fill_mask(text, top_k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgu6X5M2xe5b",
        "outputId": "86daf434-c266-4387-8a1f-2649b270233a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.7596935629844666,\n",
              "  'token': 8043,\n",
              "  'token_str': '？',\n",
              "  'sequence': '美 国 的 首 都 是 ？'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "这个结果是填充式模型的预测结果，其中包含了以下信息：\n",
        "\n",
        "score: 表示模型预测该词的置信度得分，得分为 0.7597。\n",
        "token: 表示预测的词在词汇表中的索引。\n",
        "token_str: 表示预测的词的字符串形式，这里是一个中文问号 \"？\"。\n",
        "sequence: 表示填充后的完整文本序列，即模型填充了 \"[MASK]\" 标记后得到的完整句子，这里是 \"美国的首都是？\"。\n",
        "因此，"
      ],
      "metadata": {
        "id": "PqJMJgn8G1Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"巴黎是[MASK]国的首都。\"\n",
        "fill_mask(text, top_k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM3LA5GXxj4q",
        "outputId": "161ac22b-4c89-4092-8823-307f9fd071bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.9911921620368958,\n",
              "  'token': 3791,\n",
              "  'token_str': '法',\n",
              "  'sequence': '巴 黎 是 法 国 的 首 都 。'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"美国的首都是[MASK]\"\n",
        "fill_mask(text, top_k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtuPdPSLxmuw",
        "outputId": "27616b38-4886-4a33-c251-7bdaa4f1e327"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.7596935629844666,\n",
              "  'token': 8043,\n",
              "  'token_str': '？',\n",
              "  'sequence': '美 国 的 首 都 是 ？'},\n",
              " {'score': 0.21126732230186462,\n",
              "  'token': 511,\n",
              "  'token_str': '。',\n",
              "  'sequence': '美 国 的 首 都 是 。'},\n",
              " {'score': 0.02683420106768608,\n",
              "  'token': 8013,\n",
              "  'token_str': '！',\n",
              "  'sequence': '美 国 的 首 都 是 ！'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"美国的首都是[MASK][MASK][MASK]\"\n",
        "\n",
        "fill_mask(text, top_k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYMIr0AixqFx",
        "outputId": "7939322e-27a9-438c-d8df-4d870aa34c8d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'score': 0.5740304589271545,\n",
              "   'token': 5294,\n",
              "   'token_str': '纽',\n",
              "   'sequence': '[CLS] 美 国 的 首 都 是 纽 [MASK] [MASK] [SEP]'}],\n",
              " [{'score': 0.4926770329475403,\n",
              "   'token': 5276,\n",
              "   'token_str': '约',\n",
              "   'sequence': '[CLS] 美 国 的 首 都 是 [MASK] 约 [MASK] [SEP]'}],\n",
              " [{'score': 0.9353275895118713,\n",
              "   'token': 511,\n",
              "   'token_str': '。',\n",
              "   'sequence': '[CLS] 美 国 的 首 都 是 [MASK] [MASK] 。 [SEP]'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 BERT 模型中，[SEP] 标记表示分隔符（separator）。在输入文本中，它通常用于分隔不同的句子或文本段落。"
      ],
      "metadata": {
        "id": "qpPP63fqHRGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用 AutoClass 高效管理 `Tokenizer` 和 `Model`\n",
        "\n",
        "通常，您想要使用的模型（网络架构）可以从您提供给 `from_pretrained()` 方法的预训练模型的名称或路径中推测出来。\n",
        "\n",
        "AutoClasses就是为了帮助用户完成这个工作，以便根据`预训练权重/配置文件/词汇表的名称/路径自动检索相关模型`。\n",
        "\n",
        "比如手动加载`bert-base-chinese`模型以及对应的 `tokenizer` 方法如下：\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
        "```\n",
        "\n",
        "以下是我们实际操作和演示：\n",
        "\n",
        "### 使用 `from_pretrained` 方法加载指定 Model 和 Tokenizer"
      ],
      "metadata": {
        "id": "W68jZ8VtyrBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_name = \"bert-base-chinese\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "MFwBB9_hyvMl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 使用 BERT Tokenizer 编码文本\n",
        "\n",
        "编码 (Encoding) 过程包含两个步骤：\n",
        "\n",
        "- 分词：使用分词器按某种策略将文本切分为 tokens；\n",
        "- 映射：将 tokens 转化为对应的 token IDs。"
      ],
      "metadata": {
        "id": "sVctRdWMy2PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 第一步：分词\n",
        "sequence = \"美国的首都是华盛顿特区\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sAAwhmjy3qe",
        "outputId": "84d32282-cba9-4cb6-8c56-aecaa3147ae7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['美', '国', '的', '首', '都', '是', '华', '盛', '顿', '特', '区']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 第二步：映射\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "metadata": {
        "id": "LD4IkUnXy6xS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xv673bhy-P9",
        "outputId": "5980ed3e-d4ae-4b92-f0e2-cc59a515ff22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 使用 Tokenizer.encode 方法端到端处理"
      ],
      "metadata": {
        "id": "cOIfioRnzBzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids_e2e = tokenizer.encode(sequence) #具体来说，tokenizer.encode(sequence) 的作用是将输入的文本序列经过分词器处理后，转换为模型所需的输入表示形式。"
      ],
      "metadata": {
        "id": "shkUBMdTzD-A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids_e2e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2gWgQENzGUW",
        "outputId": "33413334-d675-4b3f-b5c2-957b0ec4b422"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XeFloGrVzKwP",
        "outputId": "e281f5be-09a2-476f-add4-b5e04a31ed08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'美 国 的 首 都 是 华 盛 顿 特 区'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 编解码多段文本"
      ],
      "metadata": {
        "id": "_LV-jU1L0OSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_batch = [\"美国的首都是华盛顿特区\", \"中国的首都是北京\"]"
      ],
      "metadata": {
        "id": "jMOzW_HL0Ox6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids_batch = tokenizer.encode(sequence_batch)"
      ],
      "metadata": {
        "id": "cfp9qeQ10RLJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(token_ids_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dmSVkYXZ0S97",
        "outputId": "c88f3d59-32f5-4c6e-80af-7127e0a11834"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 美 国 的 首 都 是 华 盛 顿 特 区 [SEP] 中 国 的 首 都 是 北 京 [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_batch = tokenizer(\"美国的首都是华盛顿特区\", \"中国的首都是北京\")\n",
        "print(embedding_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_49XW70J0YiS",
        "outputId": "a6f13a7f-8052-4246-ff99-c41f42c5d91c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 优化下输出结构\n",
        "for key, value in embedding_batch.items():\n",
        "    print(f\"{key}: {value}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQndcVqI0byC",
        "outputId": "bdf9095a-63f7-4c45-ea21-3103e747aa0b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids: [101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102]\n",
            "\n",
            "token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 添加新 Token\n",
        "\n",
        "当出现了词表或嵌入空间中不存在的新Token，需要使用 Tokenizer 将其添加到词表中。 Transformers 库提供了两种不同方法：\n",
        "\n",
        "- add_tokens: 添加常规的正文文本 Token，以追加（append）的方式添加到词表末尾。\n",
        "- add_special_tokens: 添加特殊用途的 Token，优先在已有特殊词表中选择（`bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token`）。如果预定义均不满足，则都添加到`additional_special_tokens`。\n",
        "\n",
        "#### 添加常规 Token\n",
        "\n",
        "先查看已有词表，确保新添加的 Token 不在词表中："
      ],
      "metadata": {
        "id": "3AGEdjnd0eAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afADwsG90elh",
        "outputId": "9da7db23-44c1-44c6-e2f5-ea41e7bfdffe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21128"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "\n",
        "# 使用 islice 查看词表部分内容\n",
        "for key, value in islice(tokenizer.vocab.items(), 10):\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFLqHnxj0lAV",
        "outputId": "bc047129-27a4-4af0-81e8-b0bf86c1b3db"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10℃: 9115\n",
            "##鰲: 20872\n",
            "喺: 1615\n",
            "钥: 7170\n",
            "life: 8562\n",
            "営: 1612\n",
            "芎: 5694\n",
            "##へて: 12864\n",
            "ヌ: 616\n",
            "##ᆯ: 11596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_tokens = [\"天干\", \"地支\"]"
      ],
      "metadata": {
        "id": "Ln67hHPM0oRt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将集合作差结果添加到词表中\n",
        "new_tokens = set(new_tokens) - set(tokenizer.vocab.keys())"
      ],
      "metadata": {
        "id": "BB1ev7I50pqt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4162w3Ln0tIA",
        "outputId": "2bf11f45-b700-4e43-b1bf-913f747bc163"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'地支', '天干'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_tokens(list(new_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X07jjQjC0vD7",
        "outputId": "bbe7f522-96f4-4059-89d2-0788fd348e9a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 新增加了2个Token，词表总数由 21128 增加到 21130\n",
        "len(tokenizer.vocab.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4OjbR1k0wf1",
        "outputId": "0169c729-e6b6-4226-a576-a1a4d9317ad8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21130"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_special_token = {\"sep_token\": \"NEW_SPECIAL_TOKEN\"}"
      ],
      "metadata": {
        "id": "w0ecSHi50715"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens(new_special_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGBNHCkF09Fq",
        "outputId": "b3fbdc96-4847-4c31-80c0-4b01b1782f3c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 新增加了1个特殊Token，词表总数由 21128 增加到 21131\n",
        "len(tokenizer.vocab.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjDOH8XJ0-p6",
        "outputId": "14da1c6e-d1f8-454b-fbe4-9a0a779c1118"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21131"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用 `save_pretrained` 方法保存指定 Model 和 Tokenizer\n",
        "\n",
        "借助 `AutoClass` 的设计理念，保存 Model 和 Tokenizer 的方法也相当高效便捷。\n",
        "\n",
        "假设我们对`bert-base-chinese`模型以及对应的 `tokenizer` 做了修改，并更名为`new-bert-base-chinese`，方法如下：\n",
        "\n",
        "```python\n",
        "tokenizer.save_pretrained(\"./models/new-bert-base-chinese\")\n",
        "model.save_pretrained(\"./models/new-bert-base-chinese\")\n",
        "```\n",
        "\n",
        "保存 Tokenizer 会在指定路径下创建以下文件：\n",
        "- tokenizer.json: Tokenizer 元数据文件；\n",
        "- special_tokens_map.json: 特殊字符映射关系配置文件；\n",
        "- tokenizer_config.json: Tokenizer 基础配置文件，存储构建 Tokenizer 需要的参数；\n",
        "- vocab.txt: 词表文件；\n",
        "- added_tokens.json: 单独存放新增 Tokens 的配置文件。\n",
        "\n",
        "保存 Model 会在指定路径下创建以下文件：\n",
        "- config.json：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；\n",
        "- pytorch_model.bin：又称为 state dictionary，存储模型的权重。"
      ],
      "metadata": {
        "id": "eE1W3w2F1BIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"./models/new-bert-base-chinese\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arLTlePf1Byb",
        "outputId": "ea29d1a1-0274-4b40-ec9c-68c9c2e015bc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./models/new-bert-base-chinese/tokenizer_config.json',\n",
              " './models/new-bert-base-chinese/special_tokens_map.json',\n",
              " './models/new-bert-base-chinese/vocab.txt',\n",
              " './models/new-bert-base-chinese/added_tokens.json',\n",
              " './models/new-bert-base-chinese/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./models/new-bert-base-chinese\")"
      ],
      "metadata": {
        "id": "C8saFknk1FRj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_kiF_-9G0375"
      }
    }
  ]
}