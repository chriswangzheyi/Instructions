# å®æˆ˜QLoRAå¾®è°ƒChatGLM3-6B å¤§æ¨¡å‹



## é‡ç‚¹è¯´æ˜å†…å®¹

ç”¨ QLoRA è®ºæ–‡ä¸­ä»‹ç»çš„é‡åŒ–æŠ€æœ¯ï¼š**NF4 æ•°æ®ç±»å‹ã€åŒé‡åŒ–å’Œæ··åˆç²¾åº¦è®¡ç®—**ï¼Œ
 åœ¨ **ChatGLM3-6B** æ¨¡å‹ä¸Šå®ç° QLoRA å¾®è°ƒã€‚

------

##### ğŸ“¦ æ•°æ®å‡†å¤‡

- **ä¸‹è½½æ•°æ®é›†**
- **è®¾è®¡ Tokenizer å‡½æ•°** å¤„ç†æ ·æœ¬ï¼ˆ`map`ã€`shuffle`ã€`flatten`ï¼‰
- **è‡ªå®šä¹‰æ‰¹é‡æ•°æ®å¤„ç†ç±»** `DataCollatorForChatGLM`

------

##### ğŸ§  è®­ç»ƒæ¨¡å‹

- **åŠ è½½ ChatGLM3-6B é‡åŒ–æ¨¡å‹**
- **PEFT é‡åŒ–æ¨¡å‹é¢„å¤„ç†** `prepare_model_for_kbit_training`
- **é…ç½® QLoRA é€‚é…å™¨**
   `TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING`
- **è®¾ç½®å¾®è°ƒè®­ç»ƒè¶…å‚æ•°** `TrainingArguments`
- **å¯åŠ¨è®­ç»ƒ** `trainer.train()`
- **ä¿å­˜ QLoRA æ¨¡å‹** `trainer.model.save_pretrained()`

------

##### ğŸ” æ¨¡å‹æ¨ç†

- **åŠ è½½ ChatGLM3-6B åŸºç¡€æ¨¡å‹**
- **åŠ è½½ ChatGLM3-6B QLoRA æ¨¡å‹ï¼ˆPEFT Adapterï¼‰**
- **å¯¹æ¯”å¾®è°ƒå‰åçš„ç”Ÿæˆç»“æœ**





## ä»£ç 

```python
# =========================================================
# 0ï¸âƒ£ å®‰è£…ä¾èµ–
# =========================================================
# QLoRA ä¾èµ– peftã€transformersã€bitsandbytesã€datasetsã€accelerate
# è¿™é‡Œå»ºè®®ä½¿ç”¨ Transformers â‰¥ 4.46
# =========================================================
# !pip install -U "transformers>=4.46" "peft>=0.10" "bitsandbytes" "accelerate" "datasets"

# =========================================================
# 1ï¸âƒ£ å¯¼å…¥æ¨¡å—
# =========================================================
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from transformers import BitsAndBytesConfig

# =========================================================
# 2ï¸âƒ£ é…ç½®æ¨¡å‹åŠ è½½å‚æ•°ï¼ˆä½¿ç”¨ 4bit é‡åŒ–ï¼‰
# =========================================================
model_name = "THUDM/chatglm3-6b"

# bitsandbytes é‡åŒ–é…ç½®ï¼šNF4 æ•°æ®ç±»å‹ + åŒé‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                 # ä½¿ç”¨ 4-bit é‡åŒ–åŠ è½½
    bnb_4bit_quant_type="nf4",         # NormalFloat4 æ•°æ®æ ¼å¼
    bnb_4bit_use_double_quant=True,    # åŒé‡åŒ–ï¼Œè¿›ä¸€æ­¥å‡å°æ˜¾å­˜å ç”¨
    bnb_4bit_compute_dtype=torch.bfloat16,  # è®¡ç®—ç²¾åº¦ä½¿ç”¨ bfloat16
)

# =========================================================
# 3ï¸âƒ£ åŠ è½½ ChatGLM3-6B åŸºç¡€æ¨¡å‹ä¸åˆ†è¯å™¨
# =========================================================
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True, #å…è®¸Transformersæ‰§è¡Œè¯¥æ¨¡å‹ä»“åº“ä¸­ä¸Šä¼ çš„è‡ªå®šä¹‰Pythonä»£ç 
)

# é¢„å¤„ç†ï¼šè®©æ¨¡å‹é€‚é… k-bitï¼ˆ4bitï¼‰è®­ç»ƒæ¨¡å¼
model = prepare_model_for_kbit_training(model)

# =========================================================
# 4ï¸âƒ£ é…ç½® LoRA é€‚é…å™¨ï¼ˆQLoRAï¼‰
# =========================================================
# åªåœ¨æ³¨æ„åŠ›å±‚æ·»åŠ ä½ç§©çŸ©é˜µ LoRAï¼Œä»¥èŠ‚çœå‚æ•°
lora_config = LoraConfig(
    r=8,                          # LoRA ç§©ï¼ˆä½ç»´å­ç©ºé—´å¤§å°ï¼‰
    lora_alpha=32,                # LoRA ç¼©æ”¾å› å­
    target_modules=[
        "query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"
    ],                            # ChatGLM3 çš„æ³¨æ„åŠ›ä¸ MLP å±‚åç§°
    lora_dropout=0.05,            # é˜²æ­¢è¿‡æ‹Ÿåˆ
    bias="none",
    task_type="CAUSAL_LM"         # å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡
)

# å°† LoRA æ¨¡å—æ³¨å…¥æ¨¡å‹
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# =========================================================
# 5ï¸âƒ£ åŠ è½½å¾®è°ƒæ•°æ®é›†ï¼ˆAdvertiseGenï¼‰
# =========================================================
# å®˜æ–¹é“¾æ¥ï¼šhttps://huggingface.co/datasets/HasturOfficial/adgen
dataset = load_dataset("HasturOfficial/adgen", split="train[:1%]")  # å¯å…ˆç”¨ 1% åšæ¼”ç¤º

# æ•°æ®ç¤ºä¾‹ï¼š
# {"content": "ä¿ƒé”€æ´»åŠ¨ï¼šä¹°ä¸€é€ä¸€ï¼Œé™æ—¶ä¼˜æƒ ã€‚", "summary": "å•†åœºä¿ƒé”€æ´»åŠ¨å¹¿å‘Š"}
def format_sample(example):
    text = f"ç”¨æˆ·: {example['content']}\nå¹¿å‘Š: {example['summary']}"
    return {"text": text}

dataset = dataset.map(format_sample)

# =========================================================
# 6ï¸âƒ£ Tokenize ç¼–ç 
# =========================================================
def tokenize(batch):
    outputs = tokenizer(
        batch["text"],
        truncation=True,
        max_length=512,
        padding="max_length",
    )
    outputs["labels"] = outputs["input_ids"].copy()
    return outputs

tokenized_ds = dataset.map(tokenize, batched=True, remove_columns=["text"])

# =========================================================
# 7ï¸âƒ£ Data Collatorï¼ˆç»„ batchï¼‰
# =========================================================
collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

# =========================================================
# 8ï¸âƒ£ è®­ç»ƒå‚æ•°é…ç½®
# =========================================================
training_args = TrainingArguments(
    output_dir="./chatglm3_qlora_output",
    per_device_train_batch_size=1,   #æ¯ä¸ª GPU çš„ batch å¤§å°
    gradient_accumulation_steps=4,   #æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç­‰ä»·äºâ€œè™šæ‹Ÿ batch = 1Ã—4 = 4â€
    num_train_epochs=1,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    save_strategy="steps",
    optim="paged_adamw_8bit",       # bitsandbytes ä¼˜åŒ–å™¨
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    report_to="none",
)

# =========================================================
# 9ï¸âƒ£ åˆ›å»º Trainer å¹¶å¼€å§‹å¾®è°ƒ
# =========================================================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds,
    data_collator=collator,
)
trainer.train()

# =========================================================
# ğŸ”Ÿ ä¿å­˜ LoRA é€‚é…å™¨
# =========================================================
model.save_pretrained("./chatglm3_qlora_adapter")
tokenizer.save_pretrained("./chatglm3_qlora_adapter")

print("âœ… LoRA adapter å·²ä¿å­˜è‡³ ./chatglm3_qlora_adapter")

# =========================================================
# ğŸ” æ¨ç†æµ‹è¯•ï¼ˆåŠ è½½é€‚é…å™¨ï¼‰
# =========================================================
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)
lora_model = PeftModel.from_pretrained(base_model, "./chatglm3_qlora_adapter")

prompt = "ç”¨æˆ·ï¼šå†™ä¸€ä¸ªå…³äºæ™ºèƒ½æ‰‹è¡¨çš„å¹¿å‘Šæ–‡æ¡ˆã€‚\nå¹¿å‘Šï¼š"
inputs = tokenizer(prompt, return_tensors="pt").to(lora_model.device)
outputs = lora_model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

```



## å…³é”®å†…å®¹è§£é‡Š

### ä»€ä¹ˆæ˜¯ NF4ï¼ˆNormalFloat4ï¼‰

**NF4** å…¨ç§°æ˜¯ **Normal Float 4-bit**ï¼Œ æ˜¯ QLoRAï¼ˆQuantized LoRAï¼‰è®ºæ–‡æå‡ºçš„ä¸€ç§ **4ä½é‡åŒ–æ•°æ®æ ¼å¼**ï¼Œä¸“é—¨ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡åŒ–ä¼˜åŒ–çš„ **éå‡åŒ€åˆ†å¸ƒæµ®ç‚¹æ•°è¡¨ç¤ºæ–¹æ³•**ã€‚NF4 æ˜¯ä¸€ç§ç»Ÿè®¡åˆ†å¸ƒæ„ŸçŸ¥çš„ 4-bit æµ®ç‚¹æ ¼å¼ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ­£æ€åˆ†å¸ƒï¼ˆnormal distributionï¼‰çš„æƒé‡åˆ†å¸ƒæ¥è®¾è®¡é‡åŒ–æ˜ å°„ï¼Œèƒ½åœ¨ 4bit ç²¾åº¦ä¸‹æ¥è¿‘ 16bit çš„è¡¨ç°ã€‚

åœ¨ LLM é‡åŒ–ä¸­ï¼Œæˆ‘ä»¬è¦æŠŠæ¨¡å‹æƒé‡ä» 16-bitï¼ˆFP16/BF16ï¼‰å‹ç¼©æˆ 4-bitã€‚ä¼ ç»Ÿåšæ³•æ˜¯ï¼š**çº¿æ€§é‡åŒ–ï¼ˆLinear Quantizationï¼‰**ï¼šæŠŠæœ€å°å€¼åˆ°æœ€å¤§å€¼çº¿æ€§æ˜ å°„åˆ° 16 ä¸ªç­‰çº§ã€‚ä½†æ¨¡å‹æƒé‡å¾€å¾€ **ä¸æ˜¯å‡åŒ€åˆ†å¸ƒçš„** ï¼Œè€Œæ˜¯ **æ¥è¿‘æ­£æ€åˆ†å¸ƒï¼ˆNormal Distributionï¼‰**çº¿æ€§é‡åŒ– â†’ æŠŠå¤§éƒ¨åˆ†æƒé‡â€œæŒ¤â€åœ¨ä¸­é—´ï¼Œå¾ˆå®¹æ˜“ä¸¢ç²¾åº¦ã€‚NF4 çš„æƒ³æ³•æ˜¯ï¼šâ€œæ—¢ç„¶æƒé‡æœä»æ­£æ€åˆ†å¸ƒï¼Œé‚£æˆ‘å°±æŒ‰ç…§æ­£æ€åˆ†å¸ƒå»è®¾è®¡é‡åŒ–æ˜ å°„è¡¨ã€‚â€





## target_modules æ€ä¹ˆå¡«

`target_modules` å‘Šè¯‰ PEFTï¼šâ€œè¯·åœ¨æ¨¡å‹ä¸­å“ªäº›å±‚ä¸Šæ’å…¥ LoRA æ¨¡å—ï¼Ÿâ€ã€‚é€šå¸¸æˆ‘ä»¬åªå¯¹æ³¨æ„åŠ›å±‚ï¼ˆQã€Kã€Vã€O æŠ•å½±çŸ©é˜µï¼‰æˆ–éƒ¨åˆ† MLP å±‚åš LoRAã€‚å› ä¸ºè¿™äº›å±‚**å‚æ•°é‡å¤§ã€å½±å“åŠ›å¼º**ï¼Œèƒ½æ˜¾è‘—è°ƒèŠ‚æ¨¡å‹è¾“å‡ºé£æ ¼ã€‚é€šè¿‡ä¸‹é¢çš„æ–¹å¼æŸ¥çœ‹ï¼š

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("facebook/opt-1.3b")
# æ‰“å°æ¨¡å‹ç»“æ„
print(model)

```

è¾“å‡ºç±»ä¼¼ï¼ˆèŠ‚é€‰ï¼‰ï¼š

```python
OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(...)
      (layers): ModuleList(
        (0-23): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
            (k_proj): Linear(...)
            (v_proj): Linear(...)
            (out_proj): Linear(...)
          )
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        )
      )
    )
  )
)
```

å¯ä»¥çœ‹åˆ°ï¼š

```python
self_attn:
  q_proj, k_proj, v_proj, out_proj
```

