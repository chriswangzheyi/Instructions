## HF Transformers  å¾®è°ƒè®­ç»ƒæ¨¡å— Trainer



## Demoä»£ç 



```python
1# ==========================================================
# ç¯å¢ƒå‡†å¤‡ä¸ä¾èµ–
# ==========================================================
# è‹¥é¦–æ¬¡è¿è¡Œï¼Œè¯·åœ¨å‘½ä»¤è¡Œæˆ– Notebook ç¬¬ä¸€ä¸ªå•å…ƒæ ¼æ‰§è¡Œï¼š
# !pip install -U "transformers>=4.44" datasets evaluate accelerate torch

import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    set_seed,
)
import evaluate

# å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°ï¼ˆå°¤å…¶æ˜¯åœ¨shuffleã€åˆå§‹åŒ–ç­‰ç¯èŠ‚ï¼‰
set_seed(42)

# ==========================================================
# 1ï¸âƒ£ åŠ è½½ Yelp è¯„è®ºæ•°æ®é›†
# ==========================================================
# yelp_review_full æ˜¯ HuggingFace å®˜æ–¹æä¾›çš„å…¬å¼€æ•°æ®é›†ï¼Œ
# åŒ…å« 65 ä¸‡æ¡è‹±æ–‡è¯„è®ºï¼Œæ ‡ç­¾ä» 0~4 åˆ†åˆ«è¡¨ç¤º 1~5 æ˜Ÿè¯„åˆ†ã€‚
dataset = load_dataset("yelp_review_full")

# æŸ¥çœ‹æ•°æ®é›†çš„ç»„æˆï¼ˆtrain/test åˆ’åˆ†ï¼‰
print(dataset)

# æŸ¥çœ‹è®­ç»ƒé›†ä¸­çš„ç¬¬ä¸€æ¡æ ·æœ¬ï¼ˆåŒ…å« text å’Œ labelï¼‰
print(dataset["train"][0])

# ==========================================================
# 2ï¸âƒ£ åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨ï¼ˆTokenizerï¼‰
# ==========================================================
# bert-base-casedï¼šBERT è‹±æ–‡å¤§å°å†™æ•æ„Ÿç‰ˆæœ¬ï¼ˆä¼šä¿ç•™å•è¯å¤§å†™ä¿¡æ¯ï¼‰
# åˆ†è¯å™¨è´Ÿè´£æŠŠåŸå§‹æ–‡æœ¬æ‹†åˆ†ä¸º tokenï¼Œå¹¶æ˜ å°„æˆæ¨¡å‹è¾“å…¥çš„ IDã€‚
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# å®šä¹‰ä¸€ä¸ªåˆ†è¯å‡½æ•°ï¼Œç”¨äºå¯¹æ¯æ¡æ ·æœ¬çš„ text å­—æ®µè¿›è¡Œç¼–ç ã€‚
def tokenize_function(examples):
    return tokenizer(
        examples["text"],         # è¾“å…¥å­—æ®µåä¸º "text"
        padding="max_length",     # è‡ªåŠ¨å¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼ˆä¾¿äº batch è®­ç»ƒï¼‰
        truncation=True,          # è¶…å‡ºæœ€å¤§é•¿åº¦çš„æ–‡æœ¬ä¼šè¢«æˆªæ–­
        max_length=256,           # é™å®šæœ€å¤§åºåˆ—é•¿åº¦ï¼ŒYelp è¯„è®ºè¾ƒé•¿æ—¶å¾ˆæœ‰ç”¨
    )

# ==========================================================
# 3ï¸âƒ£ å¯¹æ•´ä¸ªæ•°æ®é›†æ‰¹é‡åˆ†è¯
# ==========================================================
# map ä¼šæŠŠ tokenize_function åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚
# batched=True è¡¨ç¤ºæ¯æ¬¡å¤„ç†ä¸€ä¸ª batchï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰
tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,
)

# å°† "label" åˆ—æ”¹åä¸º "labels"ï¼ŒTrainer é»˜è®¤è¯»å– "labels" ä½œä¸ºç›‘ç£ä¿¡å·
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

# ç§»é™¤åŸå§‹çš„ "text" åˆ—ï¼ŒèŠ‚çœå†…å­˜ä¸æ˜¾å­˜
tokenized_datasets = tokenized_datasets.remove_columns(["text"])

# ==========================================================
# 4ï¸âƒ£ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
# ==========================================================
# AutoModelForSequenceClassification è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ¨¡å‹æ¶æ„ã€‚
# num_labels=5 ä»£è¡¨æ˜¯ä¸€ä¸ª 5 åˆ†ç±»ä»»åŠ¡ï¼ˆå¯¹åº” Yelp çš„ 1~5 æ˜Ÿï¼‰ã€‚
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-cased",
    num_labels=5
)

# ==========================================================
# 5ï¸âƒ£ å®šä¹‰è¯„ä¼°æŒ‡æ ‡å‡½æ•°
# ==========================================================
# ä½¿ç”¨ evaluate åº“åŠ è½½å¸¸ç”¨æŒ‡æ ‡ï¼Œè¿™é‡Œé€‰ accuracy å’Œ f1
metric_acc = evaluate.load("accuracy")
metric_f1 = evaluate.load("f1")

# compute_metrics ä¼šåœ¨æ¯æ¬¡éªŒè¯åè‡ªåŠ¨è°ƒç”¨
def compute_metrics(eval_pred):
    logits, labels = eval_pred          # eval_pred æ˜¯ä¸€ä¸ª (logits, labels) å…ƒç»„
    preds = np.argmax(logits, axis=-1)  # å–æ¯è¡Œæœ€å¤§å€¼å¯¹åº”çš„ç±»åˆ«
    return {
        "accuracy": metric_acc.compute(predictions=preds, references=labels)["accuracy"],
        "f1": metric_f1.compute(predictions=preds, references=labels, average="weighted")["f1"],
    }

# ==========================================================
# 6ï¸âƒ£ å®šä¹‰è®­ç»ƒå‚æ•° TrainingArguments
# ==========================================================
# è¿™æ˜¯ HuggingFace Trainer çš„æ ¸å¿ƒé…ç½®é¡¹
training_args = TrainingArguments(
    output_dir="./results",          # æ¨¡å‹ã€æ—¥å¿—ç­‰è¾“å‡ºç›®å½•
    eval_strategy="epoch",           # æ¯ä¸ª epoch ç»“æŸåè¿›è¡Œä¸€æ¬¡éªŒè¯ï¼ˆ4.46+ ç‰ˆæœ¬ç”¨ eval_strategyï¼‰
    save_strategy="epoch",           # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    learning_rate=2e-5,              # å¾®è°ƒæ—¶çš„å­¦ä¹ ç‡ï¼ˆBERT é€šå¸¸åœ¨ 1e-5 ~ 5e-5 ä¹‹é—´ï¼‰
    per_device_train_batch_size=8,   # æ¯å¼  GPU ä¸Šçš„ batch å¤§å°ï¼ˆå¯æŒ‰æ˜¾å­˜è°ƒèŠ‚ï¼‰
    per_device_eval_batch_size=8,
    num_train_epochs=3,              # è®­ç»ƒè½®æ•°
    weight_decay=0.01,               # æƒé‡è¡°å‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    logging_dir="./logs",            # æ—¥å¿—è¾“å‡ºè·¯å¾„
    logging_steps=100,               # æ¯éš”å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—
    load_best_model_at_end=True,     # è®­ç»ƒç»“æŸåè‡ªåŠ¨åŠ è½½æœ€ä¼˜æ¨¡å‹ï¼ˆæ ¹æ®éªŒè¯é›†æŒ‡æ ‡ï¼‰
    metric_for_best_model="accuracy",# ä»¥ accuracy ä½œä¸ºæœ€ä¼˜æ¨¡å‹åˆ¤æ–­æ ‡å‡†
    report_to="none",                # ä¸ä¸Šä¼ åˆ° wandbã€tensorboard ç­‰
)

# ==========================================================
# 7ï¸âƒ£ åˆå§‹åŒ– Trainer
# ==========================================================
# Trainer å°è£…äº†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ä¸ä¿å­˜é€»è¾‘
trainer = Trainer(
    model=model,                             # æ¨¡å‹
    args=training_args,                      # è®­ç»ƒå‚æ•°
    train_dataset=tokenized_datasets["train"], # è®­ç»ƒé›†
    eval_dataset=tokenized_datasets["test"],   # éªŒè¯é›†
    tokenizer=tokenizer,                     # åˆ†è¯å™¨ï¼ˆç”¨äºåŠ¨æ€ paddingï¼‰
    compute_metrics=compute_metrics,         # æŒ‡æ ‡è®¡ç®—å‡½æ•°
)

# ==========================================================
# 8ï¸âƒ£ å¼€å§‹è®­ç»ƒä¸éªŒè¯
# ==========================================================
# è®­ç»ƒè¿‡ç¨‹ä¼šè‡ªåŠ¨è¾“å‡º lossã€accuracy ç­‰ä¿¡æ¯
trainer.train()

# è®­ç»ƒç»“æŸååœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹è¡¨ç°
eval_res = trainer.evaluate()
print("Eval Results:", eval_res)

# ==========================================================
# 9ï¸âƒ£ ä¿å­˜æœ€ä¼˜æ¨¡å‹ä¸åˆ†è¯å™¨
# ==========================================================
# å°†æ¨¡å‹å’Œåˆ†è¯å™¨ä¿å­˜åˆ°æœ¬åœ° ./results/best ç›®å½•
trainer.save_model("./results/best")
tokenizer.save_pretrained("./results/best")

# ==========================================================
# ğŸ”Ÿ ç®€å•æ¨ç†ç¤ºä¾‹
# ==========================================================
# éšä¾¿è¾“å…¥ä¸¤æ¡è¯„è®ºæµ‹è¯•æ¨¡å‹æ•ˆæœ
texts = [
    "The food was amazing and the service was excellent!",
    "Terrible experience. I will never come back.",
]

# å°†æ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹è¾“å…¥å¼ é‡
enc = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=256,
    return_tensors="pt",   # è¿”å› PyTorch å¼ é‡
)

import torch
# å…³é—­æ¢¯åº¦è®¡ç®—ï¼Œä»…æ¨ç†
with torch.no_grad():
    out = model(**enc)

# è·å–é¢„æµ‹ç±»åˆ«ï¼ˆå–æœ€å¤§æ¦‚ç‡æ‰€åœ¨çš„ç´¢å¼•ï¼‰
preds = out.logits.argmax(dim=-1).tolist()

# Yelp æ ‡ç­¾æ˜¯ 0~4ï¼Œæˆ‘ä»¬æ˜¾ç¤ºä¸º 1~5 æ˜Ÿæ›´ç›´è§‚
print("Predictions (1-5 stars):", [p + 1 for p in preds])

```



## åšäº†ä»€ä¹ˆå¾®è°ƒå·¥ä½œ



| é˜¶æ®µ                      | æ¨¡å‹åœ¨åšä»€ä¹ˆ                           | è®­ç»ƒæ•°æ®                                        | ç›®çš„                                 |
| ------------------------- | -------------------------------------- | ----------------------------------------------- | ------------------------------------ |
| **é¢„è®­ç»ƒ (pre-training)** | è®©æ¨¡å‹ç†è§£è¯­è¨€çš„åŸºæœ¬è§„å¾‹               | å¤§è§„æ¨¡æ— æ ‡ç­¾æ–‡æœ¬ï¼ˆä¾‹å¦‚ Wikipedia, BooksCorpusï¼‰ | å­¦ä¹ é€šç”¨è¯­è¨€çŸ¥è¯†                     |
| **å¾®è°ƒ (fine-tuning)**    | è®©æ¨¡å‹é€‚åº”æŸä¸ªå…·ä½“ä»»åŠ¡ï¼ˆæ¯”å¦‚æƒ…æ„Ÿåˆ†ç±»ï¼‰ | å°‘é‡**å¸¦æ ‡ç­¾**çš„æ•°æ®ï¼ˆæ¯”å¦‚ Yelp è¯„è®º + æ˜Ÿçº§ï¼‰   | æŠŠé€šç”¨è¯­è¨€çŸ¥è¯†è½¬åŒ–æˆå¯æ‰§è¡Œçš„ä»»åŠ¡èƒ½åŠ› |





## ä»ä»£ç è§’åº¦çœ‹â€œå¾®è°ƒâ€åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆ

1. **åŠ è½½ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹**

   ```
   model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
   ```

   è¿™ä¸€æ­¥ä¸æ˜¯éšæœºåˆå§‹åŒ–å‚æ•°ï¼Œè€Œæ˜¯ä» Hugging Face ä¸‹è½½ BERT çš„æƒé‡ï¼ˆé€šå¸¸æ˜¯åœ¨ Wikipedia ä¸Šé¢„è®­ç»ƒå‡ ç™¾äº¿ tokens å¾—åˆ°çš„ï¼‰ã€‚
    æ‰€ä»¥å®ƒå·²ç»â€œæ‡‚è‹±è¯­â€ï¼ŒçŸ¥é“è¯­æ³•ã€ä¸Šä¸‹æ–‡ã€å¸¸è§æ­é…ç­‰ã€‚

2. **åŠ è½½ä½ çš„ä»»åŠ¡æ•°æ®ï¼ˆYelpï¼‰**
    è¿™æ˜¯ä¸€ä¸ªç›‘ç£ä»»åŠ¡ï¼šè¾“å…¥æ˜¯è¯„è®ºæ–‡æœ¬ï¼Œè¾“å‡ºæ˜¯ 1~5 æ˜Ÿæ ‡ç­¾ã€‚

   ```
   â­ï¸ Label: 4
   ğŸ“ Text: My wife took me here on my birthday for breakfast and it was excellent. The food was tasty and the service was fast. I definitely recommend this place if you like great breakfast and friendly staff.
   
   â­ï¸ Label: 0
   ğŸ“ Text: I ordered a small cheese pizza and it came burnt. The crust was dry and the cheese tasted old. Never coming back here again.
   
   â­ï¸ Label: 2
   ğŸ“ Text: The food was okay, not bad but nothing special. Service could have been faster. Might come back if Iâ€™m in the area.
   ```

3. **å†è®­ç»ƒå‡ è½®ï¼ˆnum_train_epochs=3ï¼‰**
    è¿™æ—¶ï¼Œæ¨¡å‹ä¸æ˜¯ä»é›¶å­¦è¯­è¨€ï¼Œè€Œæ˜¯åœ¨ä¿ç•™é€šç”¨èƒ½åŠ›çš„åŸºç¡€ä¸Šï¼š

   - æœ€åå‡ å±‚æƒé‡è¢«æ›´æ–°ï¼Œé€‚åº”â€œæƒ…æ„Ÿè¯„åˆ†â€è¿™ä¸€å…·ä½“ä»»åŠ¡ï¼›
   - æ—©æœŸå±‚ï¼ˆè¯­è¨€ç‰¹å¾ï¼‰å¤§å¤šä¿æŒä¸å˜ï¼Œåªåšç»†å¾®è°ƒæ•´ã€‚

   æ‰€ä»¥è¿™ä¸ªé˜¶æ®µå«â€œfine-tuningâ€è€Œä¸æ˜¯â€œtraining from scratchâ€ã€‚

   

   ä»æ¨¡å‹çš„è§’åº¦çœ‹ï¼š

   ```
   ä¸€ä¸ª BERT åˆ†ç±»æ¨¡å‹é€šå¸¸é•¿è¿™æ ·ï¼š
   
   [Embedding å±‚]
   [Transformer ç¼–ç å±‚ Ã—12]
   [åˆ†ç±»å±‚ï¼šä¸€ä¸ªçº¿æ€§å±‚ + softmax è¾“å‡º5ç±»]
   
   å¾®è°ƒæ—¶ï¼š
   
   1.Embedding å’Œå‰é¢çš„ Transformer å±‚
      è¿™äº›å±‚å·²ç»èƒ½æŠŠè¯­è¨€å˜æˆæœ‰æ„ä¹‰çš„å‘é‡è¡¨ç¤ºã€‚
      è®­ç»ƒæ—¶åªä¼šè½»å¾®è°ƒæ•´ï¼ˆå­¦ä¹ ç‡å°ã€å˜åŒ–æ…¢ï¼‰ã€‚
   
   2. æœ€åçš„åˆ†ç±»å±‚
      è¿™å±‚æ˜¯æ–°åŠ çš„ï¼ˆé’ˆå¯¹ Yelp ä»»åŠ¡ï¼‰ã€‚
      å®ƒä» Transformer çš„è¾“å‡ºä¸­å­¦ä¹ â€œè¿™å¥è¯æ˜¯å‡ æ˜Ÿâ€ã€‚
      æ‰€ä»¥è¿™éƒ¨åˆ†è®­ç»ƒå˜åŒ–æœ€å¤§ã€‚
   
   â¡ï¸ ä¹Ÿå°±æ˜¯è¯´ï¼š
   
   æ¨¡å‹å‰é¢çš„éƒ¨åˆ†ï¼ˆç†è§£è¯­è¨€çš„èƒ½åŠ›ï¼‰ä¿æŒä¸å˜ï¼›
   åªè°ƒæ•´æœ€åå‡ å±‚æ¥è®©å®ƒâ€œå­¦ä¼šæ‰“åˆ†â€ã€‚
   
   è¿™å°±å« fine-tuningï¼ˆå¾®è°ƒï¼‰ï¼Œè€Œä¸æ˜¯ training from scratchï¼ˆä»é›¶å¼€å§‹è®­ç»ƒï¼‰ã€‚
   ```

   

4. **æ•ˆæœï¼š**

   - å¾®è°ƒåæ¨¡å‹èƒ½åŒºåˆ†æ­£é¢/è´Ÿé¢è¯„è®ºï¼›
   - å¦‚æœæ¢æˆ IMDb æˆ– Amazon Review æ•°æ®å†è®­ç»ƒï¼Œå®ƒè¿˜èƒ½ç»§ç»­é€‚åº”ã€‚





