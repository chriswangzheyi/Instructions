# HF PEFT è°ƒä¼˜å·¥å…·



## LORA Adapter é…ç½® Demo



##### â‘  å¯¼å…¥æ¨¡å—

```
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
```

------

##### â‘¡ åˆ›å»º LoRA é…ç½®å¯¹è±¡

```
config = LoraConfig(
    r=8,                      # LoRA çŸ©é˜µçš„ç§©ï¼ˆç§©è¶Šå¤§ï¼Œå¯è®­ç»ƒå‚æ•°è¶Šå¤šï¼‰
    lora_alpha=32,            # LoRA ç¼©æ”¾å› å­ï¼Œå½±å“æ›´æ–°å¹…åº¦
    target_modules=["c_attn", "c_proj"],  # GPT-2 çš„æ³¨æ„åŠ›å±‚æ¨¡å—åç§°
    lora_dropout=0.05,        # Dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    bias="none",              # ä¸æ›´æ–° bias å‚æ•°
    task_type="CAUSAL_LM"     # ä»»åŠ¡ç±»å‹ï¼Œè¿™é‡Œæ˜¯å› æœè¯­è¨€æ¨¡å‹ï¼ˆGPTç±»ï¼‰
)
```

------

##### â‘¢ è®©æ¨¡å‹æ”¯æŒ LoRA

```
# åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# åº”ç”¨ LoRA é…ç½®
model = get_peft_model(model, config)
```

------

##### â‘£ æ‰“å°å¯è®­ç»ƒå‚æ•°

```
model.print_trainable_parameters()
```

è¾“å‡ºç»“æœç¤ºä¾‹ï¼š

```
trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475
```



| é¡¹ç›®                 | å«ä¹‰                                                         |
| -------------------- | ------------------------------------------------------------ |
| **trainable params** | å½“å‰å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ˆLoRA æ–°å¢çš„éƒ¨åˆ†ï¼‰= **811,008 ä¸ª**        |
| **all params**       | æ¨¡å‹å…¨éƒ¨å‚æ•°æ•°é‡ï¼ˆGPT-2 åŸå§‹ + LoRA æ’å…¥ï¼‰= **125,250,816 ä¸ª** |
| **trainable%**       | å¯è®­ç»ƒå‚æ•°å æ¯” = **çº¦ 0.65%**                                |



## **å®æˆ˜** LoRA - OPT-6.7B **æ–‡æœ¬ç”Ÿæˆ**



```python
# -*- coding: utf-8 -*-
# =========================================================
# 0) å®‰è£…ä¾èµ–ï¼ˆNotebook ç¬¬ä¸€æ ¼æ‰§è¡Œä¸€æ¬¡å³å¯ï¼‰
# =========================================================
# !pip install -U "transformers>=4.46" peft accelerate bitsandbytes datasets




# =========================================================
# 1) åŸºç¡€å¯¼å…¥
# =========================================================
import os
import torch
from datasets import Dataset

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,  # 8bit/4bit é‡åŒ–å‚æ•°è®­ç»ƒå‰çš„å¿…è¦å‡†å¤‡
)

# æ¢¯å­
os.environ['HTTP_PROXY'] = 'http://127.0.0.1:10887'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:10887'

# é¿å… CUDA è­¦å‘Šï¼Œç¡®å®šè®¾å¤‡
if torch.backends.mps.is_available():
    device = "mps"
elif torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

use_8bit = torch.cuda.is_available()
print("device:", device)

# =========================================================
# 2) æ¨¡å‹ä¸åˆ†è¯å™¨ï¼ˆ8-bit é‡åŒ–åŠ è½½ï¼‰
# =========================================================
model_id = "facebook/opt-6.7b"

# åŠ è½½åˆ†è¯å™¨
# OPT ç³»åˆ—å…¼å®¹ GPT2 tokenizerï¼›ç”¨ AutoTokenizer æ›´é€šç”¨
tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)
# Causal LM éœ€è¦å®šä¹‰ pad_tokenï¼Œä¸€èˆ¬å¤ç”¨ eos_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ä»¥ 8-bit é‡åŒ–åŠ è½½æ¨¡å‹ï¼ˆæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ï¼‰
# device_map="auto" è®© accelerate è‡ªåŠ¨æŠŠæ¨¡å‹æ”¾åˆ° GPU
if use_8bit:
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        load_in_8bit=True,              # â† å…³é”®ï¼š8-bit é‡åŒ–åŠ è½½
        device_map="auto",
    )

    # æŠŠé‡åŒ–æ¨¡å‹åšä¸€æ¬¡â€œk-bit è®­ç»ƒå‡†å¤‡â€
    # ä¼šå¼€å¯è¾“å…¥æ¢¯åº¦ã€ç¦ç”¨æŸäº›å±‚çš„ç¼“å­˜ç­‰ï¼Œè®© 8-bit/4-bit ä¸‹èƒ½ç¨³å®šè®­ç»ƒ
    model = prepare_model_for_kbit_training(model)
else:
    dtype = torch.float16 if device != "cpu" else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=dtype,
    )
    model.to(device)

# =========================================================
# 3) é…ç½® LoRAï¼ˆåªåœ¨æ³¨æ„åŠ›/MLPçš„å…³é”®æŠ•å½±å±‚æ’å…¥ LoRAï¼‰
# =========================================================
# å¯¹ OPTï¼šæ³¨æ„åŠ›å±‚é€šå¸¸åŒ…å« q_proj/k_proj/v_proj/out_proj
# MLP å±‚åŒ…å« fc1/fc2ï¼ˆå¯é€‰ï¼Œå…ˆä»æ³¨æ„åŠ›åšèµ·ä¹Ÿå¯ä»¥ï¼‰
lora_config = LoraConfig(
    r=8,                      # LoRA ä½ç§©ç»´åº¦ï¼ˆ4~16 å¸¸ç”¨ï¼‰
    lora_alpha=32,            # LoRA ç¼©æ”¾å› å­
    lora_dropout=0.05,        # é˜²è¿‡æ‹Ÿåˆ
    target_modules=["q_proj", "k_proj", "v_proj", "out_proj"],  # å…ˆé‡åŒ–æ³¨æ„åŠ›éƒ¨åˆ†
    bias="none",
    task_type="CAUSAL_LM",    # ä»»åŠ¡ï¼šè‡ªå›å½’è¯­è¨€å»ºæ¨¡
)

# å°† LoRA é€‚é…å™¨æŒ‚åˆ°æ¨¡å‹ä¸Šï¼ˆå†»ç»“åŸæ¨¡å‹ï¼Œä»…è®­ç»ƒ LoRAï¼‰
model = get_peft_model(model, lora_config)

# æ‰“å°å¯è®­ç»ƒå‚æ•°å æ¯”ï¼ˆé€šå¸¸ <1%ï¼‰
model.print_trainable_parameters()

# =========================================================
# 4) æ„é€ ä¸€ä¸ªç®€å•æ¼”ç¤ºæ•°æ®é›†ï¼ˆä¹Ÿå¯ä»¥æ¢æˆ wikitext-2 æˆ–ä½ çš„ç§æœ‰è¯­æ–™ï¼‰  
# è®©æ¨¡å‹å­¦ä¹ è¾“å…¥è¾“å‡ºçš„â€œæ ¼å¼â€ä¸â€œè¯­æ°”â€ï¼Œæµ‹è¯• LoRA å¾®è°ƒæµç¨‹æ˜¯å¦é€šç•…
# =========================================================
texts = [
    "User: Hello, can you write a short poem about the ocean?\nAssistant:",
    "User: Explain what LoRA is in one sentence.\nAssistant:",
    "User: Give me three fun facts about dolphins.\nAssistant:",
    "User: Summarize why quantization helps large language models run faster.\nAssistant:",
]

raw_ds = Dataset.from_dict({"text": texts})

# ç®€å•çš„ç¼–ç å‡½æ•°ï¼ˆæŒ‰æœ€å¤§é•¿åº¦æˆªæ–­/è¡¥é½ï¼‰
# æŠŠåŸå§‹æ–‡æœ¬æ ·æœ¬è½¬æ¢æˆæ¨¡å‹å¯ä»¥è®­ç»ƒçš„è¾“å…¥æ ¼å¼
max_length = 512
def tokenize(example):
    # åœ¨çº¯ Causal LM è®­ç»ƒé‡Œï¼Œè¾“å…¥=æ ‡ç­¾ï¼›è¿™é‡Œæ¼”ç¤ºç›®çš„ç›´æ¥æ˜ å°„
    enc = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=max_length,
        return_tensors=None,
    )
    enc["labels"] = enc["input_ids"].copy()
    return enc

tokenized_ds = raw_ds.map(tokenize, remove_columns=["text"])

# DataCollatorï¼šæŒ‰ Causal LM ç»„ batchï¼›è¿™é‡Œå·² pad æˆå›ºå®šé•¿åº¦ï¼Œcollator ç®€å•åŒ–
collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# =========================================================
# 5) è®­ç»ƒå‚æ•°
# =========================================================
# å°æ•°æ®æ¼”ç¤ºï¼šç”¨ steps è¯„ä¼°/ä¿å­˜ï¼Œfp16 æ‰“å¼€æ··åˆç²¾åº¦ï¼Œä½¿ç”¨ bitsandbytes çš„ 8-bit ä¼˜åŒ–å™¨
training_args = TrainingArguments(
    output_dir="./opt67b_lora_8bit_demo",
    eval_strategy="steps",           # æ¯éš”å¤šå°‘æ­¥è¯„ä¼°ä¸€æ¬¡æ¨¡å‹ï¼ˆè¿™é‡ŒæŒ‰ stepï¼Œè€Œä¸æ˜¯ epochï¼‰
    eval_steps=20,                   # æ¯è®­ç»ƒ 20 æ­¥è¯„ä¼°ä¸€æ¬¡
    save_strategy="steps",           # æ¯éš”å›ºå®šæ­¥æ•°ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    save_steps=20,
    logging_steps=10,                # æ¯ 10 æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼ˆlossã€å­¦ä¹ ç‡ç­‰ï¼‰

    num_train_epochs=1,              # æ•´ä¸ªæ•°æ®é›†å®Œæ•´è®­ç»ƒ1æ¬¡
    per_device_train_batch_size=1,   # æ¯ä¸ª GPUï¼ˆè®¾å¤‡ï¼‰æ¯æ¬¡è¿­ä»£é€å…¥ 1 ä¸ªæ ·æœ¬
    per_device_eval_batch_size=1,    # è¯„ä¼°é˜¶æ®µåŒæ ·ä¸€æ¬¡å¤„ç† 1 ä¸ªæ ·æœ¬
    gradient_accumulation_steps=8,   # æ¯ 8 æ¬¡å‰å‘è®¡ç®—ç´¯ç§¯ä¸€æ¬¡æ¢¯åº¦ï¼Œç›¸å½“äº batch=8 çš„æ•ˆæœ
    learning_rate=2e-4,              # åˆå§‹å­¦ä¹ ç‡ï¼ˆLoRA å¸¸ç”¨ 1e-4 ~ 3e-4ï¼‰
    weight_decay=0.0,                # æƒé‡è¡°å‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ï¼Œæ­¤å¤„ç¦ç”¨

    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8,
    fp16=torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] < 8,

    optim="paged_adamw_8bit" if use_8bit else "adamw_torch",        # bitsandbytes çš„ 8-bit ä¼˜åŒ–å™¨ï¼Œçœæ˜¾å­˜
    lr_scheduler_type="cosine",     # å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼šä½™å¼¦é€€ç«
    warmup_ratio=0.03,              # é¢„çƒ­é˜¶æ®µæ¯”ä¾‹ï¼ˆå‰ 3% çš„ steps é€æ­¥å‡é«˜å­¦ä¹ ç‡ï¼Œé˜²æ­¢åˆæœŸéœ‡è¡ï¼‰

    gradient_checkpointing=True,     # æ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œè¿›ä¸€æ­¥çœæ˜¾å­˜
    ddp_find_unused_parameters=False,

    report_to="none",
    load_best_model_at_end=False,    # æ¼”ç¤ºç”¨ï¼Œå…³é—­
)

# =========================================================
# 6) è®­ç»ƒï¼ˆLoRA é€‚é…å™¨å‚æ•°ï¼‰
# =========================================================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds,
    eval_dataset=tokenized_ds.select(range(2)),   # æ¼”ç¤ºï¼šæ‹¿å‰ä¸¤æ¡å½“ eval
    data_collator=collator,
)

trainer.train()

# ä¿å­˜ LoRA é€‚é…å™¨ä¸åˆ†è¯å™¨ï¼ˆå°ä½“ç§¯ï¼‰
adapter_dir = "./opt67b_lora_8bit_demo/adapter"
trainer.model.save_pretrained(adapter_dir)
tokenizer.save_pretrained(adapter_dir)
print("LoRA adapter saved to:", adapter_dir)

# =========================================================
# 7) ç®€å•æ¨ç†æµ‹è¯•ï¼ˆåˆå¹¶ LoRA è¿›è¡Œç”Ÿæˆï¼‰
# =========================================================
model.eval()

def chat(prompt, max_new_tokens=64):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        gen = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,                        # å¼€å¯éšæœºé‡‡æ ·ï¼Œè€Œéè´ªå¿ƒå–æœ€å¤§æ¦‚ç‡
            temperature=0.8,
            top_p=0.9,                             # nucleus samplingï¼Œä»…ä¿ç•™ç´¯è®¡æ¦‚ç‡å‰ 90% çš„è¯è¿›è¡Œé‡‡æ ·
            repetition_penalty=1.1,                # æƒ©ç½šé‡å¤è¯ï¼Œé˜²æ­¢æ¨¡å‹å•°å—¦
            pad_token_id=tokenizer.eos_token_id,   # æŒ‡å®šå¡«å…… tokenï¼Œé˜²æ­¢è­¦å‘Šæˆ–é”™ä½
        )
    return tokenizer.decode(gen[0], skip_special_tokens=True)

test_prompt = "User: Explain LoRA in one sentence.\nAssistant:"
print(chat(test_prompt))
```



æ–‡å­—æè¿°æµç¨‹ï¼š



```
## ğŸ§© ä¸€ã€ç¯å¢ƒå‡†å¤‡
å®‰è£…å¿…è¦ä¾èµ–å¹¶æ£€æµ‹è®¾å¤‡ç¯å¢ƒï¼š

- `transformers`
- `peft`
- `bitsandbytes`
- `datasets`
- æ£€æŸ¥æ˜¯å¦å¯ç”¨ GPUï¼ˆCUDAï¼‰

---

## ğŸ§  äºŒã€åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨
- é€‰æ‹©æ¨¡å‹ï¼š`facebook/opt-6.7b`
- ä½¿ç”¨ `AutoTokenizer` åŠ è½½åˆ†è¯å™¨ï¼Œå¹¶è¡¥é½ `pad_token`
- ç”¨ 8-bit é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼ˆ`load_in_8bit=True`ï¼‰ï¼ŒèŠ‚çœæ˜¾å­˜
- ä½¿ç”¨ `prepare_model_for_kbit_training()` åšé‡åŒ–è®­ç»ƒå‡†å¤‡

---

## âš™ï¸ ä¸‰ã€æ’å…¥ LoRA ç»“æ„
- åˆ›å»º `LoraConfig`ï¼ˆè®¾ç½®ç§© `r=8`ã€ç¼©æ”¾ç³»æ•° `lora_alpha=32`ã€ç›®æ ‡å±‚å¦‚ `q_proj`ã€`v_proj`ï¼‰
- è°ƒç”¨ `get_peft_model()` å°† LoRA æ¨¡å—æŒ‚åˆ°æ¨¡å‹æŒ‡å®šå±‚
- å†»ç»“åŸæ¨¡å‹å‚æ•°ï¼Œä»…è®­ç»ƒ LoRA æ’å…¥éƒ¨åˆ†ï¼ˆçº¦å  <1% å‚æ•°é‡ï¼‰

---

## ğŸ“š å››ã€æ„é€ è®­ç»ƒæ•°æ®
- è‡ªå®šä¹‰å‡ æ¡ç¤ºä¾‹æ–‡æœ¬ï¼ˆå¦‚å¯¹è¯æˆ–æŒ‡ä»¤å½¢å¼ï¼‰
- ä½¿ç”¨åˆ†è¯å™¨å°†æ–‡æœ¬è½¬ä¸º `input_ids` ä¸ `labels`
- åˆ›å»ºå°å‹ `Dataset` ç”¨äºæ¼”ç¤º

---

## ğŸ§© äº”ã€é…ç½®è®­ç»ƒå‚æ•°
- ä½¿ç”¨ `TrainingArguments` è®¾ç½®è®­ç»ƒç»†èŠ‚ï¼š
  - `eval_strategy`ã€`save_strategy`
  - å­¦ä¹ ç‡ã€æ‰¹å¤§å°ã€æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
  - ä½¿ç”¨ `paged_adamw_8bit` ä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜
  - å¼€å¯ `fp16` æˆ– `bf16` æ··åˆç²¾åº¦
  - å¯ç”¨ `gradient_checkpointing` è¿›ä¸€æ­¥é™ä½å†…å­˜æ¶ˆè€—

---

## ğŸš€ å…­ã€è®­ç»ƒ LoRA å‚æ•°
- ä½¿ç”¨ `Trainer` æ‰§è¡Œ `.train()`ï¼Œä»…æ›´æ–° LoRA å±‚å‚æ•°
- ä¿å­˜è®­ç»ƒåçš„ **LoRA é€‚é…å™¨** ä¸åˆ†è¯å™¨

---

## ğŸ’¬ ä¸ƒã€æ¨ç†æµ‹è¯•
- åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼ `model.eval()`
- è¾“å…¥æµ‹è¯•æç¤ºï¼ˆpromptï¼‰
- è°ƒç”¨ `generate()` ç”Ÿæˆå›ç­”
- æŸ¥çœ‹æ¨¡å‹åœ¨å¾®è°ƒåçš„æ–‡æœ¬ç”Ÿæˆæ•ˆæœ

```



## LoRAå®æˆ˜- OpenAl Whisper-large-v2



```python
# =========================================================
# 0) ç¯å¢ƒæ£€æµ‹ & ä¾èµ–å¯¼å…¥
# =========================================================
import os
import torch
from datasets import load_dataset, DatasetDict, Audio

from transformers import (
    AutoProcessor,
    AutoModelForSpeechSeq2Seq,
    BitsAndBytesConfig,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

print("Torch:", torch.__version__)
device = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", device)

# =========================================================
# 1) å…¨å±€å‚æ•°
# =========================================================
model_name_or_path = "openai/whisper-large-v2"   # Whisper åŸºåº§
language_abbr = "zh-CN"                          # Common Voice è¯­è¨€ä»£ç 
task = "transcribe"                              # è¯­éŸ³è½¬æ–‡å­—
dataset_name = "mozilla-foundation/common_voice_11_0"

# è®­ç»ƒè¶…å‚ï¼ˆç¤ºä¾‹ç”¨è¾ƒå°è®¾ç½®ï¼Œè·‘é€šæµç¨‹ï¼‰
num_train_epochs = 1
per_device_train_batch_size = 2
gradient_accumulation_steps = 8
learning_rate = 1e-4
warmup_steps = 200
output_dir = "./whisper_lora_zh"
logging_steps = 50

# æ˜¯å¦æŠ½æ ·å­é›†ï¼ˆç¤ºä¾‹ç”¨ Trueï¼›æ­£å¼è®­ç»ƒè¯·æ”¹ä¸º Falseï¼‰
use_small_subset = True
train_take = 500    # è®­ç»ƒå­é›†æ¡æ•°
eval_take = 50      # éªŒè¯å­é›†æ¡æ•°

# =========================================================
# 2) åŠ è½½æ•°æ®é›†ï¼ˆCommon Voice 11.0 zh-CNï¼‰
#    Whisper æœŸæœ›éŸ³é¢‘ä¸º 16k é‡‡æ ·ç‡ï¼Œå­—æ®µä¸º "audio"ï¼Œè½¬å½•æ–‡æœ¬å­—æ®µä¸º "sentence"
# =========================================================
common_voice = DatasetDict()
common_voice["train"] = load_dataset(dataset_name, language_abbr, split="train+validation")
common_voice["test"]  = load_dataset(dataset_name, language_abbr, split="test")

# ç»Ÿä¸€ä¸º 16k é‡‡æ ·ç‡
common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000))

if use_small_subset:
    common_voice["train"] = common_voice["train"].select(range(min(train_take, len(common_voice["train"]))))
    common_voice["test"]  = common_voice["test"].select(range(min(eval_take, len(common_voice["test"]))))

print(common_voice)

# =========================================================
# 3) å¤„ç†å™¨ï¼ˆå«ç‰¹å¾æå– + åˆ†è¯å™¨ï¼‰
#    Whisper çš„ AutoProcessor åŒæ—¶æä¾›ï¼š
#    - feature_extractorï¼šå°†æ³¢å½¢ â†’ log-Mel é¢‘è°±
#    - tokenizerï¼šå°†æ–‡æœ¬ â†’ token ids
# =========================================================
processor = AutoProcessor.from_pretrained(model_name_or_path, language=language_abbr, task=task)

def prepare_batch(batch):
    # å–å‡ºéŸ³é¢‘æ³¢å½¢ä¸é‡‡æ ·ç‡
    audio = batch["audio"]
    # 1) éŸ³é¢‘ç‰¹å¾ï¼š80 ç»´ log-Mel é¢‘è°±ï¼›Whisper å›ºå®š 16k
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    # 2) æ–‡æœ¬æ ‡ç­¾ï¼šè½¬å½•æ–‡æœ¬ -> token ids
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# æ˜ å°„é¢„å¤„ç†ï¼›ä¿ç•™å¿…è¦å­—æ®µ
cols_to_remove = list(set(common_voice["train"].column_names) - {"audio", "sentence"})
cv_proc = DatasetDict()
cv_proc["train"] = common_voice["train"].map(prepare_batch, remove_columns=cols_to_remove, num_proc=1)
cv_proc["eval"]  = common_voice["test"].map(prepare_batch,  remove_columns=cols_to_remove, num_proc=1)

print(cv_proc["train"][0].keys())  # åº”åŒ…å«ï¼šinput_features, labels

# =========================================================
# 4) åŠ è½½ Whisper æ¨¡å‹
#    - æœ‰ GPUï¼šä½¿ç”¨ bitsandbytes 8bit é‡åŒ– + LoRA å¾®è°ƒï¼ˆçœæ˜¾å­˜ï¼‰
#    - æ—  GPUï¼šå›é€€ä¸ºå…¨ç²¾åº¦ï¼›å»ºè®®åªåšæ¨ç†æˆ–é€‰æ‹©å°æ¨¡å‹ï¼ˆå¦‚ tiny/baseï¼‰
# =========================================================
use_bnb = torch.cuda.is_available()  # åªæœ‰åœ¨ CUDA ä¸‹æ‰å¯ç”¨ bnb é‡åŒ–

if use_bnb:
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True,
        llm_int8_enable_fp32_cpu_offload=True,  # å°‘é‡ CPU offloadï¼Œé¿å…æ˜¾å­˜æ‰“æ»¡
    )
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_name_or_path,
        quantization_config=bnb_config,
        device_map="auto",
    )
    model = prepare_model_for_kbit_training(model)  # k-bit è®­ç»ƒå¿…è¦å‡†å¤‡
else:
    print("[æç¤º] æœªæ£€æµ‹åˆ° CUDAï¼›ä»¥å…¨ç²¾åº¦åŠ è½½æ¨¡å‹ã€‚å»ºè®®æ”¹ç”¨ openai/whisper-base / small ä»¥èŠ‚çœå†…å­˜ã€‚")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path)
    model.to(device)

# =========================================================
# 5) æ³¨å…¥ LoRA é€‚é…å™¨ï¼ˆä»…è®­ç»ƒæå°‘é‡å‚æ•°ï¼‰
#    å¸¸è§åšæ³•ï¼šå¯¹æ³¨æ„åŠ›æƒé‡ q_proj / v_proj æ³¨å…¥ LoRA
# =========================================================
lora_cfg = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"],  # Whisper çš„æ³¨æ„åŠ›æŠ•å½±
    bias="none",
    task_type="SEQ_2_SEQ_LM",
)
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()  # æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°å æ¯”

# =========================================================
# 6) DataCollator & è®­ç»ƒå‚æ•°
#    DataCollatorForSeq2Seq ä¼šæŒ‰æœ€é•¿æ ·æœ¬å¯¹ batch åšåŠ¨æ€ padding
# =========================================================
data_collator = DataCollatorForSeq2Seq(
    tokenizer=processor.tokenizer,  # ç”¨ tokenizer åšæ–‡æœ¬ padding
    model=model,
    padding=True,
)

training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    per_device_eval_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    learning_rate=learning_rate,
    warmup_steps=warmup_steps,
    num_train_epochs=num_train_epochs,

    evaluation_strategy="epoch",
    save_strategy="epoch",
    predict_with_generate=True,       # eval æ—¶ç”¨ generate() ç”Ÿæˆæ–‡æœ¬
    fp16=torch.cuda.is_available(),   # æœ‰ CUDA åˆ™å¯ç”¨åŠç²¾åº¦
    logging_steps=logging_steps,
    report_to="none",                 # ä¸ä¸ŠæŠ¥åˆ° wandb ç­‰
)

# =========================================================
# 7) Trainer è®­ç»ƒ
#    æ³¨æ„ï¼šè¿™é‡Œçš„ tokenizer ä¼ å…¥çš„æ˜¯ processor.tokenizerï¼ˆæ–‡æœ¬ä¾§ï¼‰ï¼Œ
#    è€Œè¾“å…¥ç‰¹å¾ç”± DataCollator + input_features æä¾›
# =========================================================
trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=cv_proc["train"],
    eval_dataset=cv_proc["eval"],
    data_collator=data_collator,
    tokenizer=processor.tokenizer,
)

trainer.train()

# ä¿å­˜ LoRA é€‚é…å™¨ä¸å¤„ç†å™¨
adapter_dir = os.path.join(output_dir, "adapter")
trainer.model.save_pretrained(adapter_dir)
processor.save_pretrained(adapter_dir)
print("LoRA adapter saved to:", adapter_dir)

# =========================================================
# 8) æ¨ç†è§£ç ç¤ºä¾‹
#    å–ä¸€æ¡ eval æ ·æœ¬ï¼šaudio -> input_features -> generate -> æ–‡æœ¬
# =========================================================
model.eval()

def transcribe_example(sample):
    # å‡†å¤‡è¾“å…¥ç‰¹å¾
    feats = processor.feature_extractor(
        sample["audio"]["array"], sampling_rate=sample["audio"]["sampling_rate"]
    ).input_features
    feats = torch.tensor([feats]).to(next(model.parameters()).device)

    with torch.no_grad():
        gen_ids = model.generate(
            feats,
            max_new_tokens=128,
            do_sample=False,   # æ¼”ç¤ºç”¨è´ªå¿ƒè§£ç ï¼Œæ›´ç¨³å®š
        )

    text = processor.batch_decode(gen_ids, skip_special_tokens=True)[0]
    return text

sample = common_voice["test"][0]
pred = transcribe_example(sample)
print("é¢„æµ‹ç»“æœï¼š", pred)
print("çœŸå®æ–‡æœ¬ï¼š", sample["sentence"])
```

