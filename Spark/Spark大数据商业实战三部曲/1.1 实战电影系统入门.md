# 1.1 实战电影系统入门

## 文件格式

user.dat:

	UserID::Gender::Age::OccupationID::Zip-code

ratings.dat:

	UserID::MovieID::Rating::Timestamp
	
movies.dat:

	MovieID::Title::Generes
	
generes: 电影类型

## 代码

	package com.wzy
	
	import org.apache.spark.sql.SparkSession
	
	object RDD_Movie_Users_Analyzer {
	
	  def main(args: Array[String]): Unit = {
	
	    val spark = SparkSession.builder()
	      .appName("app log process from ods to dwd")
	      .master("spark://192.168.2.123:7077")
	      //本地测试运行需要加这一句话，部署在生产环境则删除
	      .config("spark.jars", "/Users/zheyiwang/IdeaProjects/spark_study/target/spark_study-1.0-SNAPSHOT-jar-with-dependencies.jar")
	      .getOrCreate()
	    val sc = spark.sparkContext
	    import spark.implicits._
	
	    val usersRDD = sc.textFile("hdfs://192.168.2.121:9000/data/users.dat")
	    val moviesRDD = sc.textFile("hdfs://192.168.2.121:9000/data/movies.dat")
	    val ratingsRDD = sc.textFile("hdfs://192.168.2.121:9000/data/ratings.dat")
	
	    println("所有电影中平均得分最高(口碑最好)的电影:")
	    val movieInfo = moviesRDD.map(_.split("::")).map(x => (x(0), x(1))).cache()
	    val ratings = ratingsRDD.map(_.split("::")).map(x => (x(0), x(1), x(2))).cache()
	
	    //x._2: movie id , x._3 评分. 整个公式得到的是（MovieID，(Sum(Ratings),Count(Ratings)））
	    val moviesAndRatings = ratings.map(x => (x._2, (x._3.toDouble, 1)))
	      .reduceByKey((x,y) => (x._1+y._1,x._2+y._2))
	
	    val avgRatings = moviesAndRatings.map(x => (x._1, x._2._1.toDouble / x._2._2))
	
	    //avgRatings.join(movieInfo) 得到 （movieId, (Avg_score, name)）
	    avgRatings.join(movieInfo).map( item => (item._2._1, item._2._2))
	      .sortByKey(false).take(10)
	      .foreach( record => println(record._2+"评分为："+record._1))
	
	    val usersGender = usersRDD.map(_.split("::")).map(x => (x(0), x(1)))
	    //得到（userID, ( (userID,MovieID,Rating),Gender) )
	    val genderRatings = ratings.map( x => (x._1,(x._1,x._2,x._3)))
	      .join(usersGender).cache()
	    //genderRatings.take(10).foreach(println)
	
	    //maleFilteredRatings 得到所有性别是M的用户的 (userID,MovieID,Rating)
	    val maleFilteredRatings = genderRatings.filter(x => x._2._2.equals("M")).map( x => x._2._1)
	    val femaleFilteredRatings = genderRatings.filter( x => x._2._2.equals("F")).map(x => x._2._1)
	    
	    println("所有电影中最受男性喜欢的电影top10：")
	    //(x._2,(x._3.toDouble,1))) 得到（MovieID, (rating,1)）
	    maleFilteredRatings.map(x=>(x._2,(x._3.toDouble,1))).reduceByKey((x,y)=>(x._1+y._1,x._2+y._2))
	      //(movieID, avg_score)
	      .map(x=>(x._1,x._2._1.toDouble/x._2._2))
	      // (movieID, (avg_score,move_name))
	      .join(movieInfo)
	      .map(item=>(item._2._1,item._2._2))
	      .sortByKey(false)
	      .take(10)
	      .foreach(record=>println(record._2+"评分为："+record._1))
	
	    println("所有电影中最受女性喜爱的电影Top10:")
	    femaleFilteredRatings.map(x=>(x._2,(x._3.toDouble,1))).reduceByKey((x,y)=>(x._1+y._1,x._2+y._2))
	      .map(x=>(x._1,x._2._1.toDouble/x._2._2))
	      .join(movieInfo)
	      .map(item=>(item._2._1,item._2._2))
	      .sortByKey(false)
	      .take(10)
	      .foreach(record=>println(record._2+"评分为："+record._1))
	
	  }
	
	}

### 输出

	所有电影中平均得分最高(口碑最好)的电影:
	Baby, The (1973)评分为：5.0
	Smashing Time (1967)评分为：5.0
	Ulysses (Ulisse) (1954)评分为：5.0
	Schlafes Bruder (Brother of Sleep) (1995)评分为：5.0
	Gate of Heavenly Peace, The (1995)评分为：5.0
	Lured (1947)评分为：5.0
	Bittersweet Motel (2000)评分为：5.0
	Follow the Bitch (1998)评分为：5.0
	Song of Freedom (1936)评分为：5.0
	One Little Indian (1973)评分为：5.0
	
	所有电影中最受男性喜欢的电影top10：
	Baby, The (1973)评分为：5.0
	Smashing Time (1967)评分为：5.0
	Ulysses (Ulisse) (1954)评分为：5.0
	Schlafes Bruder (Brother of Sleep) (1995)评分为：5.0
	Gate of Heavenly Peace, The (1995)评分为：5.0
	Lured (1947)评分为：5.0
	Bells, The (1926)评分为：5.0
	Dangerous Game (1993)评分为：5.0
	Small Wonders (1996)评分为：5.0
	Follow the Bitch (1998)评分为：5.0
	
	所有电影中最受女性喜爱的电影Top10:
	Big Combo, The (1955)评分为：5.0
	Gate of Heavenly Peace, The (1995)评分为：5.0
	Ayn Rand: A Sense of Life (1997)评分为：5.0
	Raw Deal (1948)评分为：5.0
	Woman of Paris, A (1923)评分为：5.0
	Bittersweet Motel (2000)评分为：5.0
	I Am Cuba (Soy Cuba/Ya Kuba) (1964)评分为：5.0
	Coldblooded (1995)评分为：5.0
	Twice Upon a Yesterday (1998)评分为：5.0
	Song of Freedom (1936)评分为：5.0


## 通过DataFrame实现

	package com.wzy
	
	import org.apache.spark.sql.{Row, SparkSession}
	import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}
	
	
	object RDD_Movie_Users_Analyzer {
	
	  def main(args: Array[String]): Unit = {
	
	    val spark = SparkSession.builder()
	      .appName("dataframe demo")
	      .master("spark://192.168.2.123:7077")
	      //本地测试运行需要加这一句话，部署在生产环境则删除
	      .config("spark.jars", "/Users/zheyiwang/IdeaProjects/spark_study/target/spark_study-1.0-SNAPSHOT-jar-with-dependencies.jar")
	      .getOrCreate()
	    val sc = spark.sparkContext
	    import spark.implicits._
	
	    val usersRDD = sc.textFile("hdfs://192.168.2.121:9000/data/users.dat")
	    val moviesRDD = sc.textFile("hdfs://192.168.2.121:9000/data/movies.dat")
	    val ratingsRDD = sc.textFile("hdfs://192.168.2.121:9000/data/ratings.dat")
	
	    println("功能一：通过DataFrame实现某部电影观看者中男性和女性不同年龄的人数")
	    // Users
	    val schemaForUsers = StructType(
	      "UserID::Gender::Age::OccupationID::Zip-code".split("::")
	        .map(column => StructField(column, StringType, true)))
	
	    val usersRDDRows = usersRDD.map(_.split("::"))
	      .map( line =>  Row( line (0).trim, line(1).trim, line(2).trim, line(3).trim, line(4).trim))
	
	    val usersDataFrame = spark.createDataFrame(usersRDDRows, schemaForUsers)
	
	    //Ratings
	    val schemaForRatings = StructType("UserID::MovieID".split("::").
	      map(column => StructField(column, StringType, true))).
	      add("Rating", DoubleType, true).
	      add("Timestamp",StringType, true)
	
	
	    val ratingsRDDRows = ratingsRDD.map(_.split("::"))
	      .map(line => Row (line(0).trim, line(1).trim, line(2).trim.toDouble, line(3).trim))
	
	    val ratingsDataFrame = spark.createDataFrame(ratingsRDDRows, schemaForRatings)
	
	    // Movies
	    val schemaForMovies = StructType("MovieID::Title::Generes".split("::")
	      .map(column => StructField(column,StringType,true)))
	
	    val moviesRDDRows = moviesRDD.map(_.split("::"))
	      .map(line => Row(line(0).trim,line(1).trim,line(2).trim))
	
	    val moviesDataFrame=spark.createDataFrame(moviesRDDRows, schemaForMovies)
	
	    //过滤电影MovieID=1193的记录
	    ratingsDataFrame.filter("MovieID = 1193")
	      .join(usersDataFrame,"UserID")
	      .select("Gender","Age")
	      .groupBy("Gender","Age")
	      .count().show(10)
	
	
	    println("功能二：用LocalTempView实现某部电影观看者中不同性别不同年龄分别有多少人？")
	    //创建会话级别的临时表
	    ratingsDataFrame.createTempView("ratings")
	    usersDataFrame.createTempView("users")
	
	    val sql_local="SELECT Gender,Age,count(*) from users u join ratings as r on u.UserID=r.UserID where MovieID = 1103 group by Gender, Age"
	    spark.sql(sql_local).show(10)
	
	    //创建Application级别的临时表
	    ratingsDataFrame.createGlobalTempView("ratings")
	    usersDataFrame.createGlobalTempView("users")
	
	    val sql = "SELECT Gender,Age,count(*) from users u join ratings as r on u.UserID=r.UserID where MovieID = 1103 group by Gender, Age"
	    spark.sql(sql).show(10)
	
	  }
	
	}


输出：

	功能一：通过DataFrame实现某部电影观看者中男性和女性不同年龄的人数
	+------+---+-----+
	|Gender|Age|count|
	+------+---+-----+
	|     F| 45|   55|
	|     M| 50|  102|
	|     M|  1|   26|
	|     F| 56|   39|
	|     F| 50|   43|
	|     F| 18|   57|
	|     F|  1|   10|
	|     M| 18|  192|
	|     F| 25|  140|
	|     M| 45|  136|
	+------+---+-----+
	only showing top 10 rows
	
	功能二：用LocalTempView实现某部电影观看者中不同性别不同年龄分别有多少人？
	+------+---+--------+
	|Gender|Age|count(1)|
	+------+---+--------+
	|     F| 45|       8|
	|     M| 50|      26|
	|     M|  1|       5|
	|     F| 56|       9|
	|     F| 50|      10|
	|     F| 18|      18|
	|     F|  1|       3|
	|     M| 18|      37|
	|     F| 25|      43|
	|     M| 45|      31|
	+------+---+--------+
	only showing top 10 rows
	
	+------+---+--------+
	|Gender|Age|count(1)|
	+------+---+--------+
	|     F| 45|       8|
	|     M| 50|      26|
	|     M|  1|       5|
	|     F| 56|       9|
	|     F| 50|      10|
	|     F| 18|      18|
	|     F|  1|       3|
	|     M| 18|      37|
	|     F| 25|      43|
	|     M| 45|      31|
	+------+---+--------+
	only showing top 10 rows


## 通过DataSet实现

### 逻辑

yourDataFrame.as[your class]

### 代码


	package com.wzy
	
	import org.apache.spark.sql.{SparkSession}
	
	
	
	object RDD_Movie_Users_Analyzer {
	
	  case class User(UserID:String, Gender:String, Age:String, OccupationID:String, Zip_Code:String)
	  case class Rating(UserID:String, MovieID:String, Rating:Double, Timestamp:String)
	
	  def main(args: Array[String]): Unit = {
	
	    val spark = SparkSession.builder()
	      .appName("spark dataset")
	      .master("spark://192.168.2.123:7077")
	      //本地测试运行需要加这一句话，部署在生产环境则删除
	      .config("spark.jars", "/Users/zheyiwang/IdeaProjects/spark_study/target/spark_study-1.0-SNAPSHOT-jar-with-dependencies.jar")
	      .getOrCreate()
	    val sc = spark.sparkContext
	    import spark.implicits._
	
	    val usersRDD = sc.textFile("hdfs://192.168.2.121:9000/data/users.dat")
	    val moviesRDD = sc.textFile("hdfs://192.168.2.121:9000/data/movies.dat")
	    val ratingsRDD = sc.textFile("hdfs://192.168.2.121:9000/data/ratings.dat")
	
	
	    val usersForDSRDD = usersRDD.map(_.split("::")).map( line =>
	      User(line(0).trim,line(1).trim,line(2).trim,line(3).trim,line(4).trim))
	
	    val usersDataSet = spark.createDataset(usersForDSRDD)
	    usersDataSet.show(10)
	
	    val ratingsForDSRDD = ratingsRDD.map(_.split("::")).map( line =>
	    Rating(line(0).trim, line(1).trim, line(2).trim.toDouble,line(3).trim ))
	
	    val ratingsDataSet = spark.createDataset(ratingsForDSRDD)
	    ratingsDataSet.show(10)
	
	    ratingsDataSet.filter("MovieID=1193")
	      .join(usersDataSet,"UserID")
	      .select("Gender","Age")
	      .groupBy("Gender","Age")
	      .count()
	      .show(10)
	
	    spark.stop()
	
	  }
	}


打印：

	+------+------+---+------------+--------+
	|UserID|Gender|Age|OccupationID|Zip_Code|
	+------+------+---+------------+--------+
	|     1|     F|  1|          10|   48067|
	|     2|     M| 56|          16|   70072|
	|     3|     M| 25|          15|   55117|
	|     4|     M| 45|           7|   02460|
	|     5|     M| 25|          20|   55455|
	|     6|     F| 50|           9|   55117|
	|     7|     M| 35|           1|   06810|
	|     8|     M| 25|          12|   11413|
	|     9|     M| 25|          17|   61614|
	|    10|     F| 35|           1|   95370|
	+------+------+---+------------+--------+
	only showing top 10 rows
	
	+------+-------+------+---------+
	|UserID|MovieID|Rating|Timestamp|
	+------+-------+------+---------+
	|     1|   1193|   5.0|978300760|
	|     1|    661|   3.0|978302109|
	|     1|    914|   3.0|978301968|
	|     1|   3408|   4.0|978300275|
	|     1|   2355|   5.0|978824291|
	|     1|   1197|   3.0|978302268|
	|     1|   1287|   5.0|978302039|
	|     1|   2804|   5.0|978300719|
	|     1|    594|   4.0|978302268|
	|     1|    919|   4.0|978301368|
	+------+-------+------+---------+
	only showing top 10 rows
	
	+------+---+-----+
	|Gender|Age|count|
	+------+---+-----+
	|     F| 45|   55|
	|     M| 50|  102|
	|     M|  1|   26|
	|     F| 56|   39|
	|     F| 50|   43|
	|     F| 18|   57|
	|     F|  1|   10|
	|     M| 18|  192|
	|     F| 25|  140|
	|     M| 45|  136|
	+------+---+-----+
	only showing top 10 rows
	

## POM

	<?xml version="1.0" encoding="UTF-8"?>
	<project xmlns="http://maven.apache.org/POM/4.0.0"
	         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	    <modelVersion>4.0.0</modelVersion>
	
	    <groupId>org.example</groupId>
	    <artifactId>spark_study</artifactId>
	    <version>1.0-SNAPSHOT</version>
	
	    <properties>
	        <maven.compiler.source>8</maven.compiler.source>
	        <maven.compiler.target>8</maven.compiler.target>
	    </properties>
	
	    <dependencies>
	
	        <dependency>
	            <groupId>org.apache.hadoop</groupId>
	            <artifactId>hadoop-client</artifactId>
	            <version>3.3.4</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-core_2.12</artifactId>
	            <version>3.1.2</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.scala-lang</groupId>
	            <artifactId>scala-library</artifactId>
	            <version>2.12.10</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-sql_2.12</artifactId>
	            <version>3.1.2</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.spark</groupId>
	            <artifactId>spark-hive_2.12</artifactId>
	            <version>3.1.2</version>
	        </dependency>
	
	        <dependency>
	            <groupId>mysql</groupId>
	            <artifactId>mysql-connector-java</artifactId>
	            <version>8.0.17</version>
	        </dependency>
	
	        <dependency>
	            <groupId>org.apache.hive.hcatalog</groupId>
	            <artifactId>hive-hcatalog-core</artifactId>
	            <version>3.1.2</version>
	        </dependency>
	
	        <dependency>
	            <groupId>com.google.guava</groupId>
	            <artifactId>guava</artifactId>
	            <version>27.1-jre</version>
	        </dependency>
	
	        <dependency>
	            <groupId>com.fasterxml.jackson.core</groupId>
	            <artifactId>jackson-core</artifactId>
	            <version>2.11.4</version>
	        </dependency>
	        <dependency>
	            <groupId>com.fasterxml.jackson.core</groupId>
	            <artifactId>jackson-databind</artifactId>
	            <version>2.11.4</version>
	        </dependency>
	        <dependency>
	            <groupId>com.fasterxml.jackson.core</groupId>
	            <artifactId>jackson-annotations</artifactId>
	            <version>2.11.4</version>
	        </dependency>
	        <dependency>
	            <groupId>com.fasterxml.jackson.module</groupId>
	            <artifactId>jackson-module-scala_2.12</artifactId>
	            <version>2.11.4</version>
	        </dependency>
	
	    </dependencies>
	
	    <repositories>
	        <repository>
	            <id>nexus-aliyun</id>
	            <name>Nexus aliyun</name>
	            <layout>default</layout>
	            <url>http://maven.aliyun.com/nexus/content/groups/public</url>
	            <snapshots>
	                <enabled>false</enabled>
	                <updatePolicy>never</updatePolicy>
	            </snapshots>
	            <releases>
	                <enabled>true</enabled>
	                <updatePolicy>never</updatePolicy>
	            </releases>
	        </repository>
	
	    </repositories>
	
	    <pluginRepositories>
	        <pluginRepository>
	            <id>ali-plugin</id>
	            <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
	            <snapshots>
	                <enabled>false</enabled>
	                <updatePolicy>never</updatePolicy>
	            </snapshots>
	            <releases>
	                <enabled>true</enabled>
	                <updatePolicy>never</updatePolicy>
	            </releases>
	        </pluginRepository>
	    </pluginRepositories>
	
	    <build>
	        <plugins>
	            <!-- 指定编译java的插件 -->
	            <plugin>
	                <groupId>org.apache.maven.plugins</groupId>
	                <artifactId>maven-compiler-plugin</artifactId>
	                <version>3.5.1</version>
	                <configuration>
	                    <source>1.8</source>
	                    <target>1.8</target>
	                </configuration>
	            </plugin>
	
	            <!-- 指定编译scala的插件 -->
	            <plugin>
	                <groupId>net.alchim31.maven</groupId>
	                <artifactId>scala-maven-plugin</artifactId>
	                <version>3.2.2</version>
	                <executions>
	                    <execution>
	                        <goals>
	                            <goal>compile</goal>
	                            <goal>testCompile</goal>
	                        </goals>
	                        <configuration>
	                            <args>
	                                <arg>-dependencyfile</arg>
	                                <arg>${project.build.directory}/.scala_dependencies</arg>
	                            </args>
	                        </configuration>
	                    </execution>
	                </executions>
	            </plugin>
	
	
	            <!--  把依赖jar中的用到的类，提取到自己的jar中 -->
	            <plugin>
	                <groupId>org.apache.maven.plugins</groupId>
	                <artifactId>maven-assembly-plugin</artifactId>
	                <version>2.6</version>
	                <configuration>
	                    <archive>
	                        <manifest>
	                            <mainClass></mainClass>
	                        </manifest>
	                    </archive>
	                    <descriptorRefs>
	                        <descriptorRef>jar-with-dependencies</descriptorRef>
	                    </descriptorRefs>
	                </configuration>
	                <!--下面是为了使用 mvn package命令，如果不加则使用mvn assembly-->
	                <executions>
	                    <execution>
	                        <id>make-assemble</id>
	                        <phase>package</phase>
	                        <goals>
	                            <goal>single</goal>
	                        </goals>
	                    </execution>
	                </executions>
	            </plugin>
	        </plugins>
	    </build>
	
	</project>