

# 10. 模型可解释性与模型安全

可行AI的三大支柱：

| 支柱                      | 核心目标                         | 关键技术 / 方法                          | 代表问题                         |
| ------------------------- | -------------------------------- | ---------------------------------------- | -------------------------------- |
| 可解释性（XAI）           | 让人类“看懂”AI 决策过程          | LIME、SHAP、模型可视化、规则抽取         | 为什么模型会这样判断？           |
| 鲁棒性与泛化              | 让 AI 在环境变化中仍保持稳定可靠 | 对抗训练、正则化、交叉验证、数据增强     | 面对噪声与新环境还能可靠吗？     |
| 负责 AI（Responsible AI） | 让 AI 符合道德与规则             | 公平性度量、偏见检测、算法治理、审计机制 | 模型决策是否公正、合规、可问责？ |



## 可解释性AI（XAI）



| 目标                                | 场景                               | 价值                                                         |
| ----------------------------------- | ---------------------------------- | ------------------------------------------------------------ |
| 建立信任（Build Trust）             | 医生采纳 AI 诊断、法官参考量刑建议 | 用户理解判断依据，才会信任结果；AI 才能被采纳，而非被排斥    |
| 调试与优化（Debug & Improve）       | 推荐系统突然出现异常内容           | 解释有助于快速定位问题根源（数据异常或错误关联），实现精准修复 |
| 合规与审计（Compliance & Auditing） | 用户贷款被拒，要求“知道为什么”     | GDPR、AI 法规要求“可解释决策”；无法解释 = 无法合规           |
| 发现新洞察（Discover Insights）     | 科研挖掘基因数据、企业分析客户群体 | 解释可揭示隐藏规律，推动科学发现与商业创新                   |



