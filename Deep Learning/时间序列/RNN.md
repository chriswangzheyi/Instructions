# RNN

## 背景



![](Images/1.png)



## 结构



![](Images/2.webp) 

x是一个向量，它表示**输入层**的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示**隐藏层**的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的**权重矩阵**，o也是一个向量，它表示**输出层**的值；V是隐藏层到输出层的**权重矩阵**。那么，现在我们来看看W是什么。**循环神经网络**的**隐藏层**的值s不仅仅取决于当前这次的输入x，还取决于上一次**隐藏层**的值s。**权重矩阵** W就是**隐藏层**上一次的值作为这一次的输入的权重。



![](Images/3.webp)







![](Images/4.png)



可以被概况为：

![](Images/5.png)



## 为什么需要RNN（循环神经网络）

他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理**序列**的信息，即前面的输入和后面的输入是有关系的。

> ***比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；\*** ***当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。\***

***以nlp的一个最简单词性标注任务来说，将我 吃 苹果 三个单词标注词性为 我/nn 吃/v 苹果/nn。\***

那么这个任务的输入就是：

我 吃 苹果 （已经分词好的句子）

这个任务的输出是：

*我/nn 吃/v 苹果/nn(词性标注好的句子)*

对于这个任务来说，我们当然可以直接用普通的神经网络来做，给网络的训练数据格式了就是我-> 我/nn 这样的多个单独的单词->词性标注好的单词。

***但是很明显，一个句子中，前一个单词其实对于当前单词的词性预测是有很大影响的，比如预测苹果的时候，由于前面的吃是一个动词，那么很显然苹果作为名词的概率就会远大于动词的概率，因为动词后面接名词很常见，而动词后面接动词很少见。\***

所以为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就诞生了。





## 反向传播

BPTT算法通过沿着时间展开RNN，将其视为多个时间步长上的前馈神经网络。在每个时间步长，网络的状态会根据当前输入和前一个时间步的状态进行更新。BPTT利用这种时间展开的方式，将RNN转换成一个普通的前馈神经网络，从而可以使用标准的反向传播算法来训练它。

通俗来说，BPTT就像是在处理一个“展开”的RNN，把时间步骤展开成一系列前馈神经网络。然后，利用反向传播算法计算损失函数对网络参数的梯度，以便进行参数更新，使得网络在训练数据上表现更好。



![](Images/6.png)



## RNN的长期依赖问题

**什么是长期依赖？**

​    当前系统的状态，可能依赖很长时间之前系统状态

![](Images/7.png)

**长期记忆失效的原因 —— 权重矩阵连乘**

​    假定循环链接非常简单，去掉激活函数。



![](Images/8.png)

 h0 的系数乘指数级增长，W^t ，若W特征值的幅值如果小于1，那么就是指数级的衰减。

​    则会导致类似于蝴蝶效应的现象，初始条件的很小变化就会导致结果严重的变化。



![](Images/9.png)

**激活函数的选择**

​    RNN中可以用ReLU函数，但不能解决梯度消失、爆炸问题。

​    对矩阵W的初始值敏感，十分容易引发数值问题。

​    梯度的消失和爆炸沿着时间轴的级联导致的。



**为什么CNN不会出现这个问题？**

​    因为CNN中每一层卷积的权重是不相同的，并且初始化时是独立的同分布的，因此可以互相抵消，多层之后一般不会引发数值问题。

​    而RNN是共用相同的权重矩阵W，只有当W取在单位矩阵附近的时候才会有好的效果。





## 解决方案



![](Images/10.png)

TBPTT（Truncated Backpropagation Through Time）算法是一种用于训练循环神经网络（RNN）的反向传播算法，它是对标准的 BPTT 算法的一种改进。TBPTT 算法的主要思想是通过截断时间步来减少训练中的计算开销和内存消耗，从而更有效地训练长序列的数据。

在标准的 BPTT 算法中，需要在整个时间序列上执行完整的反向传播，这会导致梯度消失或梯度爆炸的问题，并且对于长序列数据来说，会占用大量的计算资源和内存。为了解决这个问题，TBPTT 算法采用了一种截断时间步的方法，只在一小段时间窗口内执行反向传播。

具体来说，TBPTT 算法的步骤如下：

1. 将时间序列数据分割成多个固定长度的时间窗口（例如，每个窗口包含 100 个时间步）。
2. 在每个时间窗口内，执行正向传播（前向传播），计算损失函数，并通过该窗口内的数据计算梯度。
3. 将梯度传播回时间窗口的起始点，并更新模型参数。
4. 重复上述步骤，直到处理完整个数据集或达到指定的迭代次数。

通过使用时间窗口来截断时间步，TBPTT 算法可以限制反向传播的范围，从而减少梯度消失或梯度爆炸的问题，并且可以降低计算和内存开销。然而，截断时间步可能会导致模型无法学习到长期依赖关系，因此需要根据具体问题和模型的性能进行合适的时间窗口选择。
