# 交叉熵损失函数



进行二分类或多分类问题时，在众多损失函数中交叉熵损失函数较为常用。 





## 什么是交叉熵损失



交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性 



![](Images/8.avif)

p(x)表示样本的真实分布，q(x)表示模型所预测的分布



交叉熵能够衡量同一个随机变量中的两个不同概率分布的差异程度，在机器学习中就表示为真实概率分布与预测概率分布之间的差异。交叉熵的值越小，模型预测效果就越好。 



交叉熵在分类问题中常常与softmax是标配，softmax将输出的结果进行处理，使其多个分类的预测值和为1，再通过交叉熵来计算损失。



## 交叉熵损失例子

假设你要训练一个模型来识别动物的图片，其中有猫、狗和鸟三类。每张图片都有一个真实的标签，表示图片中的动物是什么。在训练过程中，模型会输出一个概率分布，表示每个类别的可能性，例如 `[0.1, 0.7, 0.2]` 表示模型认为是猫的概率是 0.1，是狗的概率是 0.7，是鸟的概率是 0.2。

交叉熵损失就是用来衡量模型输出的概率分布与真实标签之间的差异。具体来说，对于每个样本，交叉熵损失计算的是模型输出的概率分布与真实标签所表示的概率分布之间的交叉熵，然后对所有样本的交叉熵求平均得到最终的损失值。

换句话说，如果模型输出的概率分布与真实标签相符，那么交叉熵损失就会很小；如果模型输出的概率分布与真实标签相差很大，那么交叉熵损失就会很大。因此，交叉熵损失可以作为一个指标，帮助模型在训练过程中调整参数，使得模型的预测结果更接近真实标签。

总的来说，交叉熵损失在分类任务中是一种常用的损失函数，用于衡量模型预测值与真实标签之间的差异，并帮助模型学习更好的分类策略。



## 以图片分类问题为例，理解交叉熵损失函数 



Fashion-MNIST数据集是一个包含60000衣服，鞋子等图片的数据集，也是实验图像分类算法经常用的数据集。



![](Images/9.avif)



这里，我们就以在这个数据集上的图片分类问题为例，理解交叉熵损失函数。



假设某个场景如下：对于我们设计的用于图片分类的卷积神经网络的训练还没有完成，此时，终止我们的训练，显然，各种层的参数已经保留。从数据集中任选一张图片（类别已经被记录），输入我们的神经网络，结果输出的是一个包含10个数据的一维张量，这10个数据分别对应10种物品的概率。不妨记为



q=[0.1058, 0.1043, 0.0988, 0.1066, 0.0875, 0.0881, 0.1027, 0.1046, 0.1057,  0.0958]

​         

很显然，这个预测结果有点糟糕，不过主要是因为网络没有训练好。同时我们也已知道这个图片的真实类别为4，这时记

p=[0,0,0,0,1,0,0,0,0,0]           

带入交叉熵损失函数，计算如下：

loss= -(0*log(0.1058)+0*log(0.1043)+0*log(0.0988)+0*log(0.1066)+1*log(0.0875)+0*log(0.0881)+0*log(0.1027)+0*log(0.1046)+0*log( 0.1057)+0*log(0.0958))=2.4361

这个结果就是我们的交叉熵损失，当然，我们希望越小越好，这意味着我们的神经网络较为成功。



其实，这个神经网络的训练过程就是对于输入的60000个数据（这里全部作为训练集，没有设置测试集），进行预测，计算损失，更新权重不断使得损失减小，循环往复。最终在训练很多轮后，使得损失足够小，分类的精度足够的高。那么我们可以认为这个神经网络在这个数据集上有较为不错的效果。 







