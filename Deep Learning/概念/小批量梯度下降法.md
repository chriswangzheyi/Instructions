# 小批量梯度下降法

小批量梯度下降法（Mini-batch Gradient Descent）是梯度下降法的一种变体，它在每次迭代中不是使用整个训练集的全部样本计算梯度，也不是仅使用一个样本（随机梯度下降），而是随机选择一个小批量样本来计算梯度。这种方法结合了梯度下降法和随机梯度下降法的优点，通常能够更快地收敛到局部最优解。



批量梯度下降法的步骤如下：

1. 随机打乱训练数据集，以避免模型陷入某种模式。

2. 将训练数据集分成若干个小批量，每个小批量包含一定数量的样本。

3. 在每次迭代中，随机选择一个小批量样本，计算该小批量样本的梯度。

4. 使用该小批量样本的梯度来更新模型参数。

5. 重复步骤3和步骤4，直到达到停止条件（如达到最大迭代次数或损失函数收敛到某个阈值）。

   

相比于梯度下降法和随机梯度下降法，小批量梯度下降法的优势在于：

- 计算效率高：每次迭代只需计算一个小批量样本的梯度，计算量相对较小，同时又避免了随机梯度下降的不稳定性。
- 收敛速度快：通过使用小批量样本的梯度来更新模型参数，能够更快地收敛到局部最优解。
- 内存占用小：不需要一次性加载整个训练数据集，节省了内存占用。

缺点在于：

- 计需要调整的参数变多
- 学习率敏感
- 批量大小通常在32-256之间