# 交叉熵损失函数



进行二分类或多分类问题时，在众多损失函数中交叉熵损失函数较为常用。 





## 什么是交叉熵损失



交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性 



![](Images/8.avif)

p(x)表示样本的真实分布，q(x)表示模型所预测的分布



交叉熵能够衡量同一个随机变量中的两个不同概率分布的差异程度，在机器学习中就表示为真实概率分布与预测概率分布之间的差异。交叉熵的值越小，模型预测效果就越好。 



交叉熵在分类问题中常常与softmax是标配，softmax将输出的结果进行处理，使其多个分类的预测值和为1，再通过交叉熵来计算损失。



## 以图片分类问题为例，理解交叉熵损失函数 



Fashion-MNIST数据集是一个包含60000衣服，鞋子等图片的数据集，也是实验图像分类算法经常用的数据集。



![](Images/9.avif)



这里，我们就以在这个数据集上的图片分类问题为例，理解交叉熵损失函数。



假设某个场景如下：对于我们设计的用于图片分类的卷积神经网络的训练还没有完成，此时，终止我们的训练，显然，各种层的参数已经保留。从数据集中任选一张图片（类别已经被记录），输入我们的神经网络，结果输出的是一个包含10个数据的一维张量，这10个数据分别对应10种物品的概率。不妨记为



q=[0.1058, 0.1043, 0.0988, 0.1066, 0.0875, 0.0881, 0.1027, 0.1046, 0.1057,  0.0958]

​         

很显然，这个预测结果有点糟糕，不过主要是因为网络没有训练好。同时我们也已知道这个图片的真实类别为4，这时记

p=[0,0,0,0,1,0,0,0,0,0]           

带入交叉熵损失函数，计算如下：

loss= -(0*log(0.1058)+0*log(0.1043)+0*log(0.0988)+0*log(0.1066)+1*log(0.0875)+0*log(0.0881)+0*log(0.1027)+0*log(0.1046)+0*log( 0.1057)+0*log(0.0958))=2.4361

这个结果就是我们的交叉熵损失，当然，我们希望越小越好，这意味着我们的神经网络较为成功。



其实，这个神经网络的训练过程就是对于输入的60000个数据（这里全部作为训练集，没有设置测试集），进行预测，计算损失，更新权重不断使得损失减小，循环往复。最终在训练很多轮后，使得损失足够小，分类的精度足够的高。那么我们可以认为这个神经网络在这个数据集上有较为不错的效果。 







