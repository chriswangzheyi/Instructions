## Pipeline


```python
import torch
import torch.optim as optim
import torch.nn as nn

from datasets import load_dataset
from transformers import pipeline

import numpy as np
import matplotlib.pyplot as plt
from tqdm import *
import sys

import warnings
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')

# 判断可用的设备是 CPU 还是 GPU，并将模型移动到对应的计算资源设备上
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```


```python
#文本分类
classifier = pipeline("sentiment-analysis") #创建一个情感分析的管道，它可以用来评估一段文本的情感倾向，通常是判断文本是正面情绪还是负面情绪。

result = classifier("This is a great movie. I enjoyed it a lot!")[0]
print(result)

result = classifier("This movie is so bad, I almost fell asleep.")[0]
print(result)
```


    {'label': 'POSITIVE', 'score': 0.9998773336410522}
    {'label': 'NEGATIVE', 'score': 0.9997857213020325}


## 数据集查看


```python
from datasets import load_dataset

imdb_dataset = load_dataset('imdb')# 加载imdb数据集
print(imdb_dataset['train'][0]) # 查看第一条数据
print(imdb_dataset['train'][-1]) # 查看最后一条数据
```




    {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered "controversial" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\'t have much of a plot.', 'label': 0}
    {'text': 'The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.', 'label': 1}


## 数据处理


```python
#定义数据集
class Dataset(torch.utils.data.Dataset):
    def __init__(self, split):
        self.dataset = load_dataset(path='imdb', split=split)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, i):
        text = self.dataset[i]['text']
        label = self.dataset[i]['label']
        return text, label


train_dataset = Dataset('train')
test_dataset = Dataset('test')
```

```python
print(len(train_dataset), len(test_dataset))
```

    25000 25000


## 词元化


```python
from transformers import AutoTokenizer

#加载Tokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
tokenizer
```




    BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})




```python
# 数据集处理函数
def collate_fn(data):
    sents = [i[0] for i in data]
    labels = [i[1] for i in data]

    #编码 （batch_encode_plus用于将文本批量转换成模型可以理解的格式，即将文本转换成一系列的数字（通常称为令牌或token））
    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,
                                   truncation=True,
                                   padding='max_length',
                                   max_length=500,
                                   return_tensors='pt',
                                   return_length=True)

    #input_ids:编码之后的数字
    #attention_mask:是补零的位置是0,其他位置是1
    input_ids = data['input_ids']
    attention_mask = data['attention_mask']
    token_type_ids = data['token_type_ids']
    labels = torch.LongTensor(labels)

    return input_ids, attention_mask, token_type_ids, labels


#定义数据加载器
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                     batch_size=32,
                                     collate_fn=collate_fn, #用于将多个数据项合并成一个批次
                                     shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                              batch_size=32,
                                              collate_fn=collate_fn,
                                              shuffle=True)

```

## 建立模型


```python
from transformers import BertModel

#加载预训练bert模型
pretrained = BertModel.from_pretrained('bert-base-cased').to(device)

#不训练,不需要计算梯度
for param in pretrained.parameters():
    param.requires_grad_(False)
```

    Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
    - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).



```python
# 定义下游任务模型
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = torch.nn.Linear(768, 2)

    def forward(self, input_ids, attention_mask, token_type_ids):
        with torch.no_grad():
            out = pretrained(input_ids=input_ids,
                       attention_mask=attention_mask,
                       token_type_ids=token_type_ids)
        out = self.fc(out.last_hidden_state[:, 0]) # 最后一层隐藏层作为输入
        out = out.softmax(dim=1)
        return out

model = Model().to(device)
```


```python
# 定义训练器
class Trainer:
    def __init__(self, model, train_loader, valid_loader):
        # 初始化训练数据集和验证数据集的dataloader
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        
        self.device = device
        self.model = model.to(self.device)
        
        # 定义优化器、损失函数和学习率调度器
        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()
        self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.95)
        
        # 记录训练过程中的损失和验证过程中的准确率
        self.train_losses = []
        self.val_accuracy = []
    
    def train(self, num_epochs):
        # tqdm用于显示进度条并评估任务时间开销
        for epoch in tqdm(range(num_epochs), file=sys.stdout):
            # 记录损失值
            total_loss = 0

            # 批量训练
            self.model.train()
            
            for input_ids, attention_mask, token_type_ids, labels in train_loader:
                # 预测、损失函数、反向传播
                self.optimizer.zero_grad()
                outputs = self.model(input_ids=input_ids.to(self.device), attention_mask=attention_mask.to(self.device), token_type_ids=token_type_ids.to(self.device)).to(self.device)
                loss = self.criterion(outputs, labels.to(self.device))
                loss.backward()
                self.optimizer.step()
                total_loss += loss.item()
            
            # 更新优化器的学习率
            self.scheduler.step()
            # 计算验证集的准确率
            accuracy = self.validate()
            
            # 记录训练集损失和验证集准确率
            self.train_losses.append(total_loss)
            self.val_accuracy.append(accuracy)
            
            # 打印中间值
            tqdm.write("Epoch: {0} Loss: {1} Acc: {2}".format(
                epoch, self.train_losses[-1], self.val_accuracy[-1]))
    
    def validate(self):
        # 测试模型，不计算梯度
        self.model.eval()
        
        # 记录总数和预测正确数
        total = 0
        correct = 0
        
        with torch.no_grad():
            for input_ids, attention_mask, token_type_ids, labels in self.valid_loader:
                outputs = self.model(input_ids=input_ids.to(self.device), attention_mask=attention_mask.to(self.device), token_type_ids=token_type_ids.to(self.device)).to(self.device)
                # 记录验证集总数和预测正确数
                total += labels.size(0)
                correct += (outputs.argmax(1) == labels.to(self.device)).sum().item()
        
        # 返回准确率
        accuracy = correct / total
        return accuracy
```

## 模型训练和验证


```python
# 创建一个 Trainer 类的实例
trainer = Trainer(model, train_loader, test_loader)
# 训练模型，迭代 30 个周期
trainer.train(num_epochs = 30)
```

    Epoch: 0 Loss: 434.53685945272446 Acc: 0.7914532650448144
    Epoch: 1 Loss: 402.5158587694168 Acc: 0.8007762483994878
    Epoch: 2 Loss: 393.65125730633736 Acc: 0.801576504481434
    Epoch: 3 Loss: 389.8109784722328 Acc: 0.8057378361075545
    Epoch: 4 Loss: 386.33617067337036 Acc: 0.8108994878361075
    Epoch: 5 Loss: 383.81841921806335 Acc: 0.810699423815621
    Epoch: 6 Loss: 382.1445892751217 Acc: 0.8136603713188221
    Epoch: 7 Loss: 380.45632472634315 Acc: 0.8143806017925737
    Epoch: 8 Loss: 379.21452555060387 Acc: 0.8153409090909091
    Epoch: 9 Loss: 378.1194171011448 Acc: 0.817381562099872
    Epoch: 10 Loss: 377.32714772224426 Acc: 0.8139004481434059
    Epoch: 11 Loss: 376.20741629600525 Acc: 0.8183418693982074
    Epoch: 12 Loss: 375.6312824189663 Acc: 0.8190220870678617
    Epoch: 13 Loss: 374.8075669705868 Acc: 0.8184619078104993
    Epoch: 14 Loss: 374.4073523283005 Acc: 0.819182138284251
    Epoch: 15 Loss: 373.81411695480347 Acc: 0.8206626120358514
    Epoch: 16 Loss: 373.27428355813026 Acc: 0.819822343149808
    Epoch: 17 Loss: 372.8720281124115 Acc: 0.8218229833546735
    Epoch: 18 Loss: 372.5892143249512 Acc: 0.8220630601792573
    Epoch: 19 Loss: 372.2855137884617 Acc: 0.8201824583866837
    Epoch: 20 Loss: 371.9072094857693 Acc: 0.8219030089628682
    Epoch: 21 Loss: 371.6156429052353 Acc: 0.8228233034571063
    Epoch: 22 Loss: 371.4782713353634 Acc: 0.8238636363636364
    Epoch: 23 Loss: 371.2345593869686 Acc: 0.8237836107554417
    Epoch: 24 Loss: 370.84895837306976 Acc: 0.8238236235595391
    Epoch: 25 Loss: 370.6026189625263 Acc: 0.8246638924455826
    Epoch: 26 Loss: 370.38798823952675 Acc: 0.8223031370038413
    Epoch: 27 Loss: 370.2933270037174 Acc: 0.8235835467349552
    Epoch: 28 Loss: 370.0269486308098 Acc: 0.8250240076824584
    Epoch: 29 Loss: 369.9317018389702 Acc: 0.8237435979513444
    100%|██████████| 30/30 [5:12:51<00:00, 625.71s/it]



```python
# 使用Matplotlib绘制损失曲线图
plt.plot(trainer.train_losses, label='loss')
plt.legend()
plt.show()
```


​    
![png](output_17_0.png)
​    



```python
# 使用Matplotlib绘制准确率曲线图
plt.plot(trainer.val_accuracy, label='accuracy')
plt.legend()
plt.show()
```


​    
![png](output_18_0.png)
​    


## 直接finetune


```python
! pip install -U accelerate
! pip install -U transformers

# 导入必要的库
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import evaluate

# 加载数据集
dataset = load_dataset("imdb")

# 加载 BERT 分词器
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# 定义用于对输入文本进行分词的函数
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# 对数据集进行分词处理
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 从数据集中选择一小部分用于训练和测试
small_train_dataset = tokenized_datasets["train"].shuffle(seed=0).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=0).select(range(1000))

# 加载 BERT-base-cased 模型用于序列分类任务
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)

# 加载准确率度量
metric = evaluate.load("accuracy")

# 定义用于计算评估指标的函数
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# 设置训练参数
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10, 
    evaluation_strategy="epoch")

# 创建一个 Trainer 实例
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

# 训练模型
trainer.train()

# 将训练好的模型保存到磁盘上
model.save_pretrained('./results/imdb_model')
```



![](1.png)



```python
# 加载模型
model = AutoModelForSequenceClassification.from_pretrained('./results/imdb_model')

# 创建pipeline
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)

# 测试模型
result = classifier('This is a great movie. I enjoyed it a lot!')
print(result)

# 测试模型
result = classifier('This movie is so bad, I almost fell asleep.')
print(result)
```

    [{'label': 'LABEL_1', 'score': 0.9999207258224487}]
    [{'label': 'LABEL_0', 'score': 0.9998856782913208}]



```python

```
